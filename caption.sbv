0:00:11.120,0:00:17.400
Buongiorno. Buongiorno. Buongiorno a voi due. 
Ciao. Buongiorno agli ascoltatori. Buongiorno alle

0:00:17.400,0:00:22.840
ascoltatrici. Corpo di Antonello. Sei uscito dal 
corpo di Antonello. Perfetto. Hai mantenuto solo i

0:00:22.840,0:00:30.200
capelli più o meno di Antonello che sei l'unico 
dei tre che ne ha seriamente. ancora di meno,

0:00:30.200,0:00:39.000
ma eh e bene, no, devo dire che allora raramente 
ci riascolto ultimamente. Mi sono riascoltato un

0:00:39.000,0:00:45.640
pezzo stamattina perché eh mio figlio di 6 anni 
per farmi piacere mi ha detto che voleva sentire

0:00:45.640,0:00:52.440
in macchina. Non è vero niente, probabilmente, ma 
voleva farmi piacere, quindi lo ringraziamo. Ehm,

0:00:52.440,0:00:57.440
e è stata divertente la puntata. Chi non l'avesse 
sentita può andare, non perché non c'era Alessio,

0:00:57.440,0:01:05.280
ma in generale. Eh, Paolo è stato abbastanza 
cazzone, io anche e quindi avanti così,

0:01:05.280,0:01:11.240
no? Allora, beh, siamo partiti. Fa dire, sei stato 
un'ora e 17, no, un'ora e 10 in macchina. No,

0:01:11.240,0:01:15.920
ho sentito i primi minuti. No, ho sentito i 
primi minuti e mi hanno fatto sorridere. Basta,

0:01:15.920,0:01:21.320
hai scato a 7x, ma soprattutto No, no, solo i 
primi minuti. Ma soprattutto hai schiacciato le

0:01:21.320,0:01:26.800
campanelline, le stelline, quelle cose lì. Io le 
ho già schiacciate le campanelline e le stelline.

0:01:26.800,0:01:34.120
Invitiamo gli altri ascoltatori. Allora, perché 
intanto eh guardando i dati eh abbiamo un sacco

0:01:34.120,0:01:42.400
di eh ascoltatori extra campanelline stelline 
che chiameremo gli ascoltatori occasionali,

0:01:42.400,0:01:52.320
così come i rapporti occasionali. Però eh, però 
però eh sempre per restare sullo stesso livello,

0:01:52.320,0:02:00.720
tanto non ci ascoltano i bambini, a parte mio 
figlio. Ehm eh no, però mettetele ste stelline,

0:02:00.720,0:02:05.960
ste campanelline, ci date una mano se siete lì che 
ascoltate, perché lo vedo che il numero di ore non

0:02:05.960,0:02:12.720
sono compatibili. Non mi non mi vedo non mi vedo 
il nostro ascoltatore normale che ci ascolta due

0:02:12.720,0:02:20.880
3 4 c volte. Sarebbe strano. È vero che forse 
sono strani chi ci ascolta, però eh vabbè.

0:02:21.840,0:02:27.120
Questa era una marchetta venuta quasi simpatica, 
però bravi. Non l'abbiamo preparata, siamo solo

0:02:27.120,0:02:37.440
così scemi. Allora, no, un sacco di cose, ma 
parliamo prima di tutto dei disagi di Paolo.

0:02:37.440,0:02:44.680
Infame. Volevo io rompere il tuo flusso, invece 
tu l'hai rotto a me. Sì, Paolo ha già rotto,

0:02:44.680,0:02:53.120
eh. ammetto di forse avere un problema e per fare 
riferimento a fare anche qualcosa di intelligente,

0:02:53.120,0:02:59.120
ovvero fare riferimento a un popolare articolo 
che è stato ripostato queste ultime settimane

0:02:59.120,0:03:07.440
che parlava da intitolato AI Vampire, i vampiri 
dell'AI, che era a sua volta un riferimento a

0:03:07.440,0:03:14.960
What We Do In the Shadows, per quelli che l'no 
visto, e Colin Robson, il vampiro dell'Energia

0:03:14.960,0:03:18.400
che è un concetto meraviglioso, dovresti 
guardare la prima puntata solo per quella cosa,

0:03:18.400,0:03:26.040
fa riderissimo. Ehm, l'idea è che eh questa ci sta 
rubando un po' l'energia, insomma, ci sta rubando

0:03:26.040,0:03:34.720
un po' l'attenzione, il focus e tutte queste cose 
qua. E è strano perché leggevo di queste cose

0:03:34.720,0:03:42.880
negli articoli che parlavano di San Francisco, in 
particolare con la visione un po' particolarmente

0:03:42.880,0:03:48.360
negativa in cui si diceva che le startup più 
spinte, quelle che già prima lavoravano un

0:03:48.360,0:03:53.600
sacco di ore, adesso gli si vuole fare lavorare 
ancora di più perché gli dice avete tutti questi

0:03:53.600,0:03:58.680
agenti che fanno cose per voi, l'aspettativa è 
altissima e quindi nei contesti più aggressivi,

0:03:58.680,0:04:04.640
diciamo, il l'ambiente è decisamente stressante 
e tossico. A me viene in mente un pochettino Wolf

0:04:04.640,0:04:11.920
of Wall Street con DiCaprio, insomma quello stile 
lì non faccio fatica a immaginarlo. Ehm, però ho

0:04:11.920,0:04:16.760
detto "Vabbè, ok, cioè il problema della serie con 
Valley, il problema di San Francisco, sti cazzi".

0:04:16.760,0:04:27.280
E insomma, ehm, diciamo che ehm quando ho iniziato 
a rendermi conto delle potenzialità applicabili

0:04:27.280,0:04:34.360
alla mia quotidianità di queste tecnologie, ovvero 
riassunto in una frase, letteralmente trasformare

0:04:34.360,0:04:41.720
qualunque idea in una realizzazione da lì a 
breve. Questo è quello che è per qualcuno che sa,

0:04:41.720,0:04:46.960
ha delle aspettative, sa vagamente quello che sta 
facendo. Quasi qualunque, qua quasi qualunque,

0:04:46.960,0:04:52.960
perché l'audio di Linux non puoi risolverlo 
comunque, neanche con l'AI, quello non non

0:04:52.960,0:05:00.400
ci si riesce. Le virtualizzazioni leggere, lo so 
perché ho buttato via parte della mia vita questa

0:05:00.400,0:05:07.560
settimana su quella cosa lì e non la risolvi 
comunque, eh, per cui quasi ogni problema, eh.

0:05:08.320,0:05:15.440
True. Ok, ok, grazie per la correzione, ci sta 
tutta. Ehm, però insomma, m No, realmente, cioè,

0:05:15.440,0:05:20.640
il potenziale è di dire come se avessi tre 
desideri da esprimere per un programmatore,

0:05:20.640,0:05:27.000
in realtà non ne hai tre, ne hai 3 miliardi 
e puoi farne quanti ne vuoi. E cosa qual

0:05:27.000,0:05:31.000
è la risorsa che poi va a mancare a 
questo punto? è banalmente il tempo,

0:05:31.000,0:05:36.320
nel senso che eh tu puoi fare tutte queste 
cose che prima rimandavi ad un weekend lungo,

0:05:36.320,0:05:40.320
a quando la famiglia andava in vacanza e tu 
potevi concentrarti con calma e invece adesso

0:05:40.320,0:05:48.280
tecnicamente puoi farla in qualunque momento 
durante una cosa telegram tanto per dire. Mi

0:05:48.280,0:05:52.960
stavo stavo costruendo, non rompere la mia 
narrativa, eh puoi fare qualunque momento.

0:05:52.960,0:05:58.120
Cosa ti serve? ti serve, beh, probabilmente ti 
serve un abbonamento eh senza limiti di token,

0:05:58.120,0:06:05.360
perché altrimenti a un certo punto il provider ti 
dice "Nice try", ma hai speso tutti sì, mo basta.

0:06:05.360,0:06:12.120
E e quello però il problema l'avevamo più o meno 
risolto grazie al fatto che Stefano lo pagano per

0:06:12.120,0:06:17.400
vendere account di ZI e quindi l'ha fatto comprare 
a me ad Alessio. Io l'ho fatto comprare un'altra

0:06:17.400,0:06:22.840
persona, quindi questo ponte sta funzionando. 
Ehm e io gli ho dato pochi pochi soldi, eh,

0:06:22.840,0:06:29.960
cioè nellre €10 ogni 3 mesi, per cui Ok. No, io io 
gli ho dato l'abbonamento annuale Max Ultra Giga,

0:06:29.960,0:06:37.160
quello in 4K, ehm che comunque ha un prezzo 
contenuto, sì, son €15 al mese alla fine. Sì,

0:06:37.160,0:06:44.120
€250 totali, qualcosa del genere, insomma che è 
accettabile e si sta ripagando tutto. Cioè io sto

0:06:44.120,0:06:51.920
abusando di questa cosa quasi costantemente perché 
fa delle cose poi, ehm, come dicono, giusto per

0:06:51.920,0:07:00.120
non fare dare false aspettative, ogni tanto è giù 
il servizio di ZAI, ehm e loro stessi dicono "Oh,

0:07:00.120,0:07:04.520
noi vi diamo sto servizio, costa poco, però gli 
SL sono quelli che sono, quindi portate pazienza

0:07:04.520,0:07:09.840
se ogni tanto siamo offline." Ed è ragionevole. è 
ragionevole in termini di uno che offre servizi e

0:07:09.840,0:07:15.640
uno che lo paga poco. Diventa difficile ragionarci 
sopra quando sei nel bel mezzo di un ragionamento

0:07:15.640,0:07:20.920
di qualcosa che stavi facendo e finalmente eri 
arrivato al punto e boom non ti risponde più

0:07:20.920,0:07:27.320
l'AI remota e tu dici "Oh mio Dio, è comittata la 
transazione, le informazioni ci sono oppure no?"

0:07:27.320,0:07:35.760
E lì è un po' di sudori freddi e di panico e che 
portano a un eventuale fork. O aspetti o pure fai

0:07:35.760,0:07:41.320
quello che ha fatto Stefano nel cuore della notte 
l'altro giorno. Cosa hai fatto? Cosa ho fatto? No,

0:07:41.320,0:07:49.000
io non dico i miei peccati, però. No, allora no, 
scherzo. Allora, no, GLM, siamo onesti, io l'ho

0:07:49.000,0:07:55.040
sponsorizzato e continuo a sponsorizzarlo perché 
secondo me come modello è notevole. Eh, dopo ci

0:07:55.040,0:08:03.960
arrivo a questo, però eh oggettivamente sì, ogni 
tanto è giù e non è velocissima la risposta. Eh,

0:08:03.960,0:08:10.320
io che prendo in giro Paolo, ma che ultimamente 
lavoro con quattro finestre aperte tutte su Cloud,

0:08:10.320,0:08:17.960
per fare due cose per volta per progetto. Ne ho 
di solito due progetti, due cose per volta. Questa

0:08:17.960,0:08:25.640
lentezza mi dava fastidio e allora sono andato 
a farmi l'abbonamento di Minimax, Minimax M2.5

0:08:25.640,0:08:30.720
5 di cui si dice un gran bene come modello 
eccetera, ma poi su quello arrivo perché

0:08:30.720,0:08:39.080
secondo me è il GLM è meglio, eh, ma che ha un 
abbonamento ha un abbonamento normale e poi ha

0:08:39.080,0:08:48.400
l'abbonamento eh super fast in cui invece loro 
hanno un SL eh di 100 token per secondo. Quando

0:08:48.400,0:08:55.600
l'ho visto non ho potuto resistere. 100 to per 
secondo mi sembrava la la cosa dei miei sogni,

0:08:55.600,0:09:04.600
più o meno, la fibra ottica dei modelli, la fibra 
ottica del degli LLM e l'ho preso ed è bellissimo,

0:09:04.600,0:09:13.080
cioè ad una velocità eh meravigliosa. Secondo me i 
risultati in senso stretto dal punto di vista del

0:09:13.080,0:09:23.720
coding, vabbè, ovviamente sono meglio quelli di 
Clodopus, che dà una mezza pista a tutti. GLM si

0:09:23.720,0:09:31.040
avvicina di più ad Opus, secondo me, di quanto non 
lo faccia a Minimax. È comunque un buon modello

0:09:31.040,0:09:40.560
che paragonerei però ad un Sonet 4.5, però veloce 
in un modo incredibile per fare cose minime,

0:09:40.560,0:09:48.440
cioè minime, anche di buon livello, va bene e 
dà una velocità pazzesca. E qua viene il punto,

0:09:48.440,0:09:57.800
perché tu dici giustamente, no, il eh l'elemento 
tossico del di Silicon Valley, ma il fatto è che

0:09:57.800,0:10:05.680
per come sono fatti i coding agent, quelli da 
riga di comando soprattutto, ne parlavamo con

0:10:05.680,0:10:12.680
Antonello l'ultima puntata di come la nostra 
generazione abbia portato la gamification

0:10:12.680,0:10:18.200
all'interno dell'industria, no? E queste hanno 
portato la gamification all'interno del coding

0:10:18.200,0:10:24.880
perché è c'è proprio il meccanismo di reward, 
no? tu descrivi una cosa e la vedi succedere,

0:10:24.880,0:10:30.520
magari non sempre esattamente, quindi hai voglia 
di fare l'improvement e sei sempre lì a a ciclare

0:10:30.520,0:10:38.720
su questa cosa perché sai che arriverà, mentre 
paradossalmente eh quando scrivi codice di tuo

0:10:38.720,0:10:48.000
Ma gli ascoltatori magari che non scrivono codice, 
magari PM o anche non tecnici, quando scrivi una

0:10:48.000,0:10:55.280
mail ti devi mettere lì pensare o un documento 
mettere lì pensare e quindi il tipo di reward,

0:10:55.280,0:11:01.720
il tipo di soddisfazione che hai è mano che la 
mail si costruisce, ma un processo più lento.

0:11:01.720,0:11:08.080
Qui il reward è scriverne tante di mail per fare 
il paragone con con la mail e quindi sei sempre

0:11:08.080,0:11:16.080
lì che che cicli e diventa davvero difficile 
staccarsi eh anche senza avere la pressione della

0:11:16.080,0:11:21.840
Silicon Valley nei side project che faccio la 
sera, che è poi quello che diceva anche Paolo, no?

0:11:22.360,0:11:28.920
Eh sì, sì, sì, esatto. Ci sarebbe da decidere se 
vuoi citare o spoilerare il riferimento che arriva

0:11:28.920,0:11:39.200
sempre dall'articolo di AI Vampires su ehm come 
questo modello sembra che si avvicini a quello

0:11:39.200,0:11:47.400
del gioco d'azzardo. Sì, sì, sì. Eh, ne parlavo 
in preintervista con una persona che intervisterò

0:11:47.400,0:11:55.840
a breve che me l'ha citato, me l'ha ricordato e 
lui, quindi gli diamo la la paternità di questa

0:11:55.840,0:12:01.320
cosa. Di solito non spoiler le interviste, ma 
ormai siamo qua. Intervisteremo a breve Gabriele

0:12:01.320,0:12:06.400
Venturi. Io l'ho presa larga, eh. Tu potevi potevi 
uscire da questa trappola, invece ci sei cascato

0:12:06.400,0:12:12.320
dentro. No, tu mi hai tendi le trappole, io sono 
bravo a cadere nelle trappole, lo sai. E quindi

0:12:12.320,0:12:18.720
l'hai fatto apposta. Eh, perché dovete sapere 
che Paolo non è d'accordo con me sul fatto che

0:12:18.720,0:12:24.640
non spoiler le interviste. Allora, in tutti i modi 
sta cercando di eh di convincermi del contrario,

0:12:24.640,0:12:31.280
compreso l'inganno. Eh, però io sono facile da 
ingannare, quindi no. Comunque mi ricordava questa

0:12:31.280,0:12:37.440
cosa per tornare, però magari nessun immagino 
che il nostro ascoltatore media sappia chi è

0:12:37.440,0:12:44.480
Gabriele Venturi, ma eh se non lo sapete ascoltate 
ascoltate l'intervista che arriverà, non so dirvi

0:12:44.480,0:12:51.600
quando, ma arriverà. Sono un whist blower, un 
whist blower. mi ricordava proprio questa cosa

0:12:51.600,0:12:56.440
qua di cui poi abbiamo parlato in settimana di 
questa c'è proprio un paper di ricerca su questa

0:12:56.440,0:13:04.560
cosa su quanto assomiglia al gioco d'azzardo, in 
particolare alleo slot machine perché ci sono due

0:13:04.560,0:13:12.800
meccanismi in questa cosa. Il primo è quello 
del schiaccio invio per andare avanti, che è

0:13:12.800,0:13:18.280
il tipico proprio, no, delle slot machine, quando 
schiacci i bottoni per far continuare, quindi ti

0:13:18.280,0:13:27.000
tiene collegato a questa cosa. E la seconda cosa 
è proprio l'indeterminismo intrinseco del dell'AI,

0:13:27.000,0:13:35.360
per cui tu hai un risultato che però a volte ti 
soddisfa, a volte no. e hai sempre la volontà,

0:13:35.360,0:13:42.680
anche quando ti soddisfa, ma non pienamente, di 
migliorarlo, che è un po' come vinco un po' di

0:13:42.680,0:13:48.760
monetine e le rigioco perché spero di vincere di 
più. Il meccanismo con cui viene fatto il paragone

0:13:48.760,0:13:58.160
è questo. Poi io non so se sono stati eh costruiti 
per questo motivo se ho un effetto collaterale. Io

0:13:58.160,0:14:03.440
voglio pensare che sia un effetto collaterale. 
Sicuramente la teoria vorrebbe che è un effetto

0:14:03.440,0:14:08.640
collaterale perché se tu fossi bravo ad usare lo 
strumento vinceresti sempre. In realtà, eh, quando

0:14:08.640,0:14:14.560
stai facendo wipe coding puro su progetti come il 
mio, in cui non padroneggio la tecnologia target,

0:14:14.560,0:14:20.560
metà dei miei prompt sono mi sento fortunato. Io 
non voglio essere autoreferenziale, ma vi dico

0:14:20.560,0:14:27.080
che tutte queste cose sono la lampada di Aladino. 
Voi sapete perché ho chiamato così il mio blog?

0:14:27.080,0:14:32.920
Eh, perché tu descrivi l'idea che hai, 
ma volente o volente ti scappa qualcosa

0:14:32.920,0:14:39.440
e finisce che la lampada ti tira fuori la la 
soluzione che però ha qualche difettuccio perché

0:14:39.440,0:14:46.920
tu non sei stato bravissimo a a descrivere 
il problema e poi devi lavorarci, diciamo,

0:14:46.920,0:14:51.840
un po' come desideri, insomma. Bella, non l'avevo 
mai capito. Eh, sai, forse l'avevi anche proposto

0:14:51.840,0:14:59.280
per scegliere il nome di questo podcast a suo 
tempo, ma non avevo colto. Mi scuso. Ehm, no,

0:14:59.280,0:15:08.800
però eh questa è una notizia che non so se Paolo 
ha già letto, eh, ma che Vai, dimmi. Aspetta, no,

0:15:08.800,0:15:13.920
volevo finire l'escursus della scimmia e della 
tossicità di questa cosa perché si aggancia

0:15:13.920,0:15:21.960
comunque a delle news che vale la pena citare. 
Quindi i m Stefano diceva che lui ha deciso di

0:15:21.960,0:15:26.880
andare in direzione velocità, non aveva tempo di 
aspettare. Io in realtà il tempo di aspettare ce

0:15:26.880,0:15:32.480
l'ho, nel senso che mi aiuta che ogni tanto vada 
lento così posso smettermi di concentrare su

0:15:32.480,0:15:37.160
quella singola attività, staccare la testa dallo 
schermo e anziché vivere la mia vita fare un'altra

0:15:37.160,0:15:44.280
attività sullo schermo e mi aiuta a fare mi forza 
il multitasting, l'attesa praticamente che è una

0:15:44.280,0:15:50.240
proprietà più che un bug per quello che serve a 
me. Qual è lo svantaggio di questa cosa però? Che

0:15:50.240,0:15:56.360
alcuni dei task che lancio di programmazione, in 
particolare su questo progetto in cui di tanto in

0:15:56.360,0:16:00.560
tanto devo lanciare dei benchmark per scoprire se 
troverà mai una soluzione al mio problema oppure

0:16:00.560,0:16:06.200
no per la natura stessa di questi problemi NTP 
complessi, per cui non c'è un algoritmo noto in un

0:16:06.200,0:16:11.360
tempo finito. Quindi devo lanciarlo e boh, vedere 
come va e in particolare vedere se la soluzione

0:16:11.360,0:16:16.720
migliora o se a un certo punto si incastra e non 
va più avanti di così. E sono job lenti. Molte

0:16:16.720,0:16:24.800
volte passavo del tempo lì a fare a guardare, 
un po' come quando avevamo il 56k e scar Sì, sì,

0:16:24.800,0:16:30.400
sì. E e quando avevamo un 56k e passavamo il tempo 
a guardare la barra di completamento che ti diceva

0:16:30.400,0:16:36.200
"Ci mancano 30 secondi, 50.000 anni, 30 secondi, 
quelle cose classiche di Windows." E e niente,

0:16:36.200,0:16:43.560
ed era a tempo un po' perso e quindi eh ho cercato 
di capire cosa potevo fare per migliorare questa

0:16:43.560,0:16:48.680
faccenda. Una delle prime ottimizzazioni, ma non è 
quella di cui andrò a parlare adesso, è quella di

0:16:48.680,0:16:53.440
rimuovere l'umano dal loop quanto più possibile. 
Quindi, anziché dover schiacciare quegli y per

0:16:53.440,0:17:00.800
dire vai avanti, posso leggere questo file, posso 
provare questo, ho cercato di ehm fornire al mio

0:17:00.800,0:17:07.440
agente di coding locale dei sottostrumenti che lui 
poteva utilizzare che erano già permessi da me,

0:17:07.440,0:17:12.000
quindi non doveva chiedere il mio permesso e e 
quello, ok, è stato utile, quindi non ero più

0:17:12.000,0:17:16.600
io il collo di bottiglia, il collo di bottiglia 
però è l'attività stessa, quindi questi eh servizi

0:17:16.600,0:17:24.800
lenti, cosa avevo potuto fare questa direzione qui 
arrivano le storie di Stefano e del suo eh modello

0:17:24.800,0:17:31.800
Open Clow che gira e sta prendendo una laurea su 
internet o qualcosa del genere. E ehm la cosa di

0:17:31.800,0:17:37.920
cui ero più invidioso era l'integrazione con 
Telegram, WhatsApp, non lo so, al punto che mi

0:17:37.920,0:17:45.720
chiedevo "Ma ne vale la pena installare il demonio 
sul computer per fare questa cosa oppure no?" E

0:17:45.720,0:17:50.520
poi ho pensato "Ma no, dai, probabilmente posso 
semplicemente rubare la funzionalità". vado da

0:17:50.520,0:17:54.840
da Gemini o da qualche modello e dico "Senti, devo 
rimplementare la possibilità di controllare il mio

0:17:54.840,0:18:00.360
cloud da remoto". Però visto che siamo nel 2026, 
non saro mica l'unico ad aver avuto questa idea,

0:18:00.360,0:18:05.440
ho guardato Gitab pieno di progetti che fanno 
questa cosa. Lui me ne ha suggeriti due o tre.

0:18:05.440,0:18:12.040
Il uno di questi tre sembrava quello minimale 
che faceva poche cose. L'ho guardato, ehm ho

0:18:12.040,0:18:16.080
detto "Bene, lo installo". Poi mi sono fermato 
un istante, ho detto "Forse non è una buona idea

0:18:16.080,0:18:21.360
comunque laddove non mi fido di OpenClow, fidarmi 
di un altro". random progetto su internet. Quindi

0:18:21.360,0:18:26.440
che cosa ho fatto? Ho chiesto al modello stesso, 
a Cloud stesso, "Senti, stavo per installare sta

0:18:26.440,0:18:29.720
roba, ma forse è meglio che prima mi guardi se 
non fa delle cose losche." Quindi ho chiesto a

0:18:29.720,0:18:36.880
Cloud di dirmi se quel progetto era eh fidato. Lui 
ha fatto una piccola analisi e mi ha detto di sì.

0:18:36.880,0:18:40.320
Siccome non gli voglio credere, gli ho chiesto 
di farne una più approfondita, quindi ho fatto

0:18:40.320,0:18:44.560
un deep research. Lui l'ha fatto per bene, ha 
guardato, mi ha detto "Sì, c'è qualche librerietta

0:18:44.560,0:18:49.520
vecchia, ma normale, niente di strano". E 
soprattutto lo sviluppatore ha una presenza

0:18:49.520,0:18:55.560
online, quindi in teoria è una persona vera. It's 
not a robot a troll, poi fino a un certo punto.

0:18:55.560,0:19:00.280
Ora questa non è una garanzia che non ci siano 
problemi col progetto che ho scelto. Ad ogni modo,

0:19:00.280,0:19:07.120
ho fatto le mie verifiche pigre, diciamo, per 
verificare funzionasse. A questo punto l'ho preso,

0:19:07.120,0:19:11.520
l'ho lanciato e non sono riuscito a farlo 
andare. Perché non sono riuscito a farlo

0:19:11.520,0:19:17.360
andare? perché lui usa era assumeva che io usassi 
Cloud Code, quando io in realtà nel mio setup non

0:19:17.360,0:19:24.320
uso Cloud Code, ma uso un un clone di Cloud Code 
che mi tiene separata la configurazione e quindi

0:19:24.320,0:19:30.000
non andava. Ah, che palle. E quindi che cosa fai? 
Quello che si fa di questi tempi. Chiedi a clock,

0:19:30.000,0:19:34.320
senti clock code, mi fai questa cosa? Perché io 
non uso clock code, uso una variante. Fa certo,

0:19:34.320,0:19:40.040
non c'è problema. Qua me l'ha fixato e funzionava. 
Ho detto wow. A questo punto ho detto cosa faccio?

0:19:40.040,0:19:45.360
Me lo tengo per me? No, non tenermelo per 
me con la COD, per favore, crea un task sul

0:19:45.360,0:19:50.000
progetto originale da cui arrivo dicendogli che 
non funziona in questi setup. lui me l'ha creato.

0:19:50.000,0:19:57.840
Io ho rivisto la definizione del dell'issue perché 
volevo evitare di sbattere in faccia un eventuale

0:19:57.840,0:20:02.680
AI slope allo sviluppatore di questo progetto che 
mi è stato utile. Quindi il minimo che potevo fare

0:20:02.680,0:20:07.440
era verificare a mano di non dirgli cazzate 
e infatti Colo Code non aveva fatto un lavoro

0:20:07.440,0:20:11.760
perfettissimo a spiegare il contesto, quindi ho 
corretto quello, però gli ho aperto il liso. Dopo

0:20:11.760,0:20:17.040
che gli ho aperto liso, gli ho aperto la PR, gli 
ho aperto, ha fatto tutto Cloud. Io gli ho sapevo

0:20:17.040,0:20:21.680
che cosa volevo, lui ha fatto la fatica. Però 
pensateci, sono passato da una funzionalità che

0:20:21.680,0:20:28.200
volevo, l'ho trovata, non funzionava. Il codice me 
l'ha il la gente me l'ha fixata e ha fatto anche

0:20:28.200,0:20:33.720
la parte di contribuirla indietro. La Piara è 
stata emergiata, peraltro, anche relativamente in

0:20:33.720,0:20:38.920
fretta, ma io avrei vissuto anche se non succedeva 
mai con il mio fork. E niente, e quindi io adesso

0:20:38.920,0:20:46.440
ho la possibilità di controllare Cloud tramite 
Telegram e ehm sono contentissimo innanzitutto

0:20:46.440,0:20:53.000
perché mi permette di eh poter mandare avanti 
le cose lente. Ogni tanto guardo, funziona molto

0:20:53.000,0:20:57.920
meglio di quanto avrei mai potuto immaginare la 
UX. Davvero pensavo che sarebbe stato impossibile,

0:20:57.920,0:21:03.800
invece funziona. Eh, ovviamente funziona meglio 
per alcune cose, tipo il mio progetto ha una

0:21:03.800,0:21:09.680
parte di interfaccia web, quella parte ora come 
ora non la sto controllando. Non che non potrei,

0:21:09.680,0:21:16.960
ma m esponendo il il web m a essere accessibile 
da remoto, quindi in realtà non lo sto vedendo,

0:21:16.960,0:21:21.600
quindi funziona meglio per il backend, se volete, 
ma di per sé posso farlo. E quindi io adesso,

0:21:21.600,0:21:26.160
mentre aspetto mia figlia che esce a scuola, o 
vado al bar, oltre che guardarla me e altre cose,

0:21:26.160,0:21:30.360
guardo la chat di Telegram e vedo che c'è il 
bot che ha finito questa roba e gli dico "Senti,

0:21:30.360,0:21:36.920
fai questo, fai quest'altro". E se alcune di 
queste attività voi pensate che richiedono un

0:21:36.920,0:21:41.200
un'interazione attiva, è vero, alcune non tutte 
le attività di di sviluppo con Code puoi fare

0:21:41.200,0:21:47.000
dal telefono perché devi dargli un feedback, devi 
leggere attentamente, ma molte altre sì. Ricerca,

0:21:47.000,0:21:52.840
eh verifica stessa della tua coda, delle attività 
per quali ho iniziato a usare backlog. Stamattina,

0:21:52.840,0:21:58.160
ad esempio, gli ho fatto backgrooming di tutte le 
mie attività, una roba che di solito odio fare,

0:21:58.160,0:22:01.680
l'ho fatta tranquillamente dal telefono mentre l' 
letto, gli ho detto "Senti, prova a controllare

0:22:01.680,0:22:05.760
se abbiamo delle cose che sono marcate come da 
fare, ma in realtà le abbiamo già fatte, ci siamo

0:22:05.760,0:22:12.120
dimenticati". E lui mi ha fatto tutte queste cose, 
ha effettivamente risparmiato ore di stare davanti

0:22:12.120,0:22:17.160
al laptop in maniera tradizionale per fare tutte 
queste attività. E io sono contentissimo di questa

0:22:17.160,0:22:21.080
cosa. Sto spammando tutti i miei amici dicendo 
"Guardate che si può fare sta roba, è facile,

0:22:21.080,0:22:28.480
eh, potrebbe essere l'inizio di un problema ancora 
più consistente di addiction, ma eh la comodità è

0:22:28.480,0:22:34.920
assolutamente lì. E perché vi faccio tutto sto 
pippone?" Perché questa settimana Antropic ha

0:22:34.920,0:22:44.360
ufficialmente rilasciato la stessa funzionalità. 
Eh, via web, però però però via web, non via via

0:22:44.360,0:22:48.560
eh via Sì, loro hanno rilasciato, scusatemi 
se non ha rilasciato la stessa funzionalità,

0:22:48.560,0:22:56.080
ha rilasciato la stessa UX, diciamo, l'app mobile 
per fare eh sessioni long running session di Cloud

0:22:56.080,0:23:02.160
Code. Ehm, la versione fatta in Telegram è più 
casereccia, open source, se volete la versione

0:23:02.160,0:23:08.720
loro è più servizio che paghi, però diciamo che 
hanno validato la UX. Probabilmente questo va a

0:23:08.720,0:23:14.880
pit di Open Clow che è il primo che ci ha fatto 
vedere che poteva essere una buona idea. No, no,

0:23:14.880,0:23:24.320
però allora l'ho guardata, è un po' diverso il 
concetto, nel senso che cioè a me piace di più

0:23:24.320,0:23:31.080
la versione tua, open clos, cioè che si interfacci 
con il tuo Cloud Code eccetera. quella che hanno

0:23:31.080,0:23:40.960
rilasciato in Antropic è molto simile a quello 
che fa Codex di Open AI, quindi è un'istanza web

0:23:40.960,0:23:53.440
di cloud code che si clona il tuo repo e vive 
su un eh su un AVS, su un su un su un cloud da

0:23:53.440,0:24:00.080
qualche parte, ma non è quella del tuo computer, 
che sia VPS o che sia il computer vero, mentre

0:24:00.080,0:24:07.960
invece se quello che fai tu è quella cosa lì. E 
perché a me piace di più? Perché volendo poi dopo

0:24:07.960,0:24:15.400
lì è dove metti il taglio della tua sicurezza, 
gli dai accesso a tante cose sul tuo computer,

0:24:15.400,0:24:23.360
eh, i tuoi file, eccetera eccetera. Infatti non 
escludo di provare a mettere la stessa cosa che

0:24:23.360,0:24:33.240
hai tu e cloud code sul VPS che ho spegnendo Open 
Clow un attimo. Però Antropic ha riconosciuto la

0:24:33.240,0:24:40.240
validità dell'idea di Open Clow decisamente perché 
ha aggiunto l'altra funzionalità ed era la notizia

0:24:40.240,0:24:46.680
che non so se hai ancora letto perché di stanotte 
e credo che potrebbe farti molto piacere sapere

0:24:46.680,0:24:54.200
che esiste. hanno aggiunto la memoria di lungo 
termine su Cloud Code, che è esattamente l'altra

0:24:54.200,0:25:02.160
cosa che ha Open Clow. eh hanno fatto sono fatti 
delle belle pippe, nel senso che hanno fatto una

0:25:02.160,0:25:11.600
cosa seria ed evoluta, hanno messo insieme il 
concetto di Cloud MD, rendendolo però Clud MD è,

0:25:11.600,0:25:19.120
diciamo, la RIDM per gli agenti del progetto, 
l'hanno fatto diventare gerarchico, per cui tu

0:25:19.120,0:25:25.720
puoi avere un CLU MD per ogni componente del tuo 
progetto, per cui ad esempio se hai una parte di

0:25:25.720,0:25:32.520
interfaccia web è una parte di backend, puoi 
avere dei clod MD diversi e o che completano

0:25:32.520,0:25:38.560
il Clode MD di base nello stile di programmazione, 
nel tipo di linguaggio che usi, eccetera eccetera.

0:25:38.560,0:25:46.080
hanno aggiunto eh tutta una serie di rules, come 
le chiamano loro, che assomigliano moltissimo a

0:25:46.080,0:25:54.680
quelle che erano le cursor rules di cursor e 
in più hanno aggiunto il memory MD dove Cloud,

0:25:54.680,0:26:03.600
esattamente come fa OpenClow, eh alla durante la 
sessione capisce quali sono le cose rilevanti e

0:26:03.600,0:26:12.040
se le sintetizza lui nel memory MD, Non ti voglio 
sputanare, ma io sto usando da tre giorni sta roba

0:26:12.040,0:26:19.760
almeno. Eh, l'annuncio su ex di Antropica è di 
stanotte 12:30. Non so che dirti perché Memory

0:26:19.760,0:26:27.000
MD lo sto letteralmente usando da un po'. Rules 
no, Memory MD c'era già. Ok. Ma lo dovevi editare

0:26:27.000,0:26:33.440
tu o gli dovevi dire di memorizzare? Cosa? Slash 
memory? Adesso c'è l'auto memory che vuol dire

0:26:33.440,0:26:39.880
che lui durante la sessione si accorge di che cosa 
stai facendo e si sintetizza le cose fondamentali

0:26:39.880,0:26:44.360
da mettersi nel Memory MD. Ok? Perché questo è 
questo è quello che hanno annunciato stanotte,

0:26:44.360,0:26:51.560
l'auto memory e le rules. No, hai ragione. Le 
rules e memory è una settimana, 10 giorni che

0:26:51.560,0:26:55.680
Infatti le rules le sto usando da questa settimana 
e mi chiedevo ah ma ci sono sempre state,

0:26:55.680,0:27:03.520
non me le ero sempre perse perché sono è dalla 
2158 e dalla dalla 2159 che è quella di stanotte

0:27:03.520,0:27:08.840
c'è l'auto memory. In compenso, il problema 
concreto, ehm, tutta questa memoria il contesto

0:27:08.840,0:27:13.400
te lo mangia. E giusto ieri stavo guardando che 
io quando inizio una nuova attività mi brucio un

0:27:13.400,0:27:19.840
25% di contesto tutte le volte e ho detto sono 
gli MCP anche, quindi sono eh sì, in teoria sì,

0:27:19.840,0:27:24.960
però ho lanciato context per vedere che cosa 
c'era dentro e ti fa lo spaccato. E il mio

0:27:24.960,0:27:30.640
il mio spaccato m adesso vabbè una curiosità m lo 
lo citiamo in podcast, magari qualcun altro ci va

0:27:30.640,0:27:36.320
dentro a guardare e scopre, era eh ne sprecav, 
la maggior parte che sprecavo di contesto non

0:27:36.320,0:27:41.880
erano gli MCP Tools con mia sorpresa, ma erano 
la collezione di Markdown Files che io ho creato,

0:27:41.880,0:27:47.800
in particolare quelli sotto la cartella/cloud 
Docs, l'equivalente delle memorie a lungo termine

0:27:47.800,0:27:53.800
dei poveri prima che che Antropic ci desse questo 
nuovo meccanismo che ho scoperto questa settimana

0:27:53.800,0:28:00.080
a quanto sono sempre lì e io alcune di quelle Sì, 
lui se li legge i Cloud Docs, lui se li rilegge

0:28:00.080,0:28:08.600
tutte le volte, mentre il memory No, il memory e 
il memory l'hanno impostato, sì, in maniera simile

0:28:08.600,0:28:15.240
a alle skill. Hanno un descriptor memoria e si 
legge solo quello che serve quando Sì, è un indice

0:28:15.240,0:28:21.920
praticamente. Sì. Ehm, sì, sì. E quando arriva 
in una situazione in cui si chiede cosa fare,

0:28:21.920,0:28:28.920
dice, "Vediamo un po' eh qual è la se ho qualcosa, 
se l'ho già fatta questa cosa in buona sostanza,

0:28:28.920,0:28:37.680
perché tipo si ricorda eh sessioni di debug, 
cose di questo genere, tiene l'auto memory,

0:28:37.680,0:28:47.920
quindi se trova bug simili o behavior simili, 
eh cerca di di non reinventarsi la ruota.

0:28:47.920,0:28:52.400
di capire cosa ha fatto, che cosa ha funzionato 
ragionamento come ne uscito. Sì. Come cosa

0:28:52.400,0:28:59.680
cosa cos'è che aveva funzionato, perché vabbè 
l'esperienza di chi lo usa è che fate magari gli

0:28:59.680,0:29:04.880
chiedete di fissare una cosa, lui fa un tentativo, 
non va a buon fine, fa un alla fine ci riesce.

0:29:04.880,0:29:09.400
invece di fare questi tre tentativi, sa già 
che il tentativo buono è fatto in quel modo

0:29:09.400,0:29:18.600
ed è il primo che prova, quantomeno poi se non 
va in quella situazione ricomincia a tentare. Ma

0:29:18.600,0:29:24.600
eh e quindi quella lì è un'altra funzionalità 
tipica di Open Clow. Quello che manca, eh,

0:29:24.600,0:29:33.120
ma che è facile da implementare è di svegliarlo, 
eh, di svegliarlo ogni tot da solo con un elenco

0:29:33.120,0:29:39.000
di cose da fare. Tutto sommato con backlog già più 
o meno lo potresti fare. Eh, cioè se tu metti un

0:29:39.000,0:29:48.160
cron ogni 10 minuti e nel promptici fai i prossimi 
due task che hai di back, logo lui lo fa e e hai

0:29:48.160,0:29:53.680
ottenuto più o meno open clop. Poi dopo un altro 
discorso sono le estensioni, gli open tutte le

0:29:53.680,0:30:00.840
skill che ti puoi scaricare perché in realtà dove 
c'è la parte di pericolo di sicurezza soprattutto

0:30:00.840,0:30:08.560
è nell'installarla qualunque e e Open Clow per 
scelta. Eh, loro hanno fatto questa scelta, ha una

0:30:08.560,0:30:17.840
grande facilità di estenderlo per le schile. Ha 
proprio un repository che si chiama Clowub, eh, di

0:30:17.840,0:30:23.680
skill, di cose che puoi installare e lo puoi fare 
lo può fare direttamente lui se glielo chiedi,

0:30:23.680,0:30:31.640
di autoinstallarsi le cose e questa la parte di 
pericolo. Eh, però i componenti fondamentali che

0:30:31.640,0:30:40.720
erano quei tre lì, l'interfaccia, diciamo, remota, 
in qualche modo, la memoria e lo svegliarsi ogni

0:30:40.720,0:30:46.720
tanto, due su tre li hanno riconosciuti 
come ok, lo facciamo anche noi, eh, che

0:30:46.720,0:30:52.480
è in qualche modo una validazione che l'idea non 
era almeno l'idea di base non era una minchiata.

0:30:53.160,0:30:59.680
Sì, nella come vi annunciavo privatamente 
nella mia tod listare i messaggi vocali

0:30:59.680,0:31:04.880
perché ogni tanto mi accorgo che mi perdo un po' 
a scrivere, mi farebbe comodo lanciargli una nota

0:31:04.880,0:31:10.160
breve a voce. Questa cosa il mio setup non ce 
l'ha. Eh, probabilmente farò girare un modello

0:31:10.160,0:31:16.360
locale tipo Whisper o parakit per convertire 
e lo farò. E questo mi ha ricordato, peraltro,

0:31:16.360,0:31:22.120
che eh uno dei tanti problemi personali che 
adesso il Microsoftare ci permette di risolvere

0:31:22.120,0:31:27.080
è che odiando i messaggi vocali di WhatsApp 
mi ha sempre dato noia che li devo ascoltare,

0:31:27.080,0:31:32.680
non posso leggerli e adesso lavorerò a scrivermi 
un convertitore di messaggi vocali in testuali

0:31:32.680,0:31:39.000
completamente privato, deploiato. Probabilmente 
proverò a usare i modelli Gemma di Android per

0:31:39.000,0:31:43.880
farlo girare come app custom Android. Era la 
cosa a cui stavo lavorando stamattina al bar.

0:31:43.880,0:31:51.520
avvisami quando lo fai. Sì. No, 
quello è molto comodo, effettivamente.

0:31:51.520,0:32:01.200
Ehm, adesso io però non vorrei dire m abbiamo un 
un elenco di di come dire di rilasci di modelli

0:32:01.200,0:32:07.200
nuovi che sono usciti in questi queste settimane 
che per quanto faccia un po' l'ISA della spesa

0:32:07.200,0:32:15.440
inizia a essere imbarazzante da come dire 
ignorare. Potremmo magari Sì. fare una Sì.

0:32:15.440,0:32:20.880
Io intanto ricordo a chi chi preferisce leggere, 
che io quell'elenco lì lo faccio in newsletter

0:32:20.880,0:32:28.160
tutte le settimane, se volete, eh, però hai hai 
ragione e partiamo dalla fine, allora, visto che

0:32:28.160,0:32:36.880
tu ci punzecchi e tu ti pigli il la pagliuzza 
più corta. E è uscito Nano Nano Banana 2. Ecco,

0:32:36.880,0:32:45.640
è uscito Nano Banana 2 proprio a brevissimo. Io ho 
fatto qualche prova e m onestamente per il momento

0:32:45.640,0:32:52.680
posso solo dirvi che Sì, eh le immagini che genero 
sono molto belle. Eh funzionava prima, funziona

0:32:52.680,0:33:02.320
adesso il commento di quando è riuscito Gemini 
3. Sì, esatto. Eh vabbè, due due commenti così.

0:33:02.320,0:33:11.320
Uno di impressione molto personale. Eh, ho provato 
a generare qualche immagine di soggetti umani e la

0:33:11.320,0:33:18.760
primissima sensazione così a pelle è Ma questo 
è Nano Banana 2 o una versione nuova di Grock?

0:33:18.760,0:33:27.120
Eh, però vediamo. Eh, non so chi ha usato Grock, 
magari capisce cosa intendo. Ma io non ho capito.

0:33:27.120,0:33:33.240
Ce l'abbiamo nella macchina noi, ma nella 
macchina non genere le immagini, comunque.

0:33:33.240,0:33:42.840
Eh no, vabbè, ma a parte quello m era tanto che 
non uscivano aggiornamenti sulla generazione di

0:33:42.840,0:33:50.880
immagini da parte di Google, onestamente, 
da quando è uscito il Nano Banana Pro e eh

0:33:50.880,0:33:59.520
c'erano stati ultimamente dei alti rilasci 
di modelli anche open weight, quindi bene.

0:34:00.320,0:34:09.600
si inserisce all'interno di del 
rilascio più grosso di Gemini 3.1 Pro.

0:34:09.600,0:34:15.720
Ribadisco, molto fresco, non non l'ho ancora 
guardato bene, eh, però in realtà quello di cui

0:34:15.720,0:34:26.560
volevo parlare io, Stefano, era anche di tutti 
gli altri m Sì, sì. Sì di GLM 5, di Quen 3.5,

0:34:26.560,0:34:38.480
di eh Sonet 4.6, eh GPT 53, Codex Sparks, Minimx 
25, l'hai citato tu prima. Eh, tutto questo per

0:34:38.480,0:34:48.360
dire che eh intanto si nota una una velocità, cioè 
è aumentato il il ritmo dei dei rilasci, se vuoi.

0:34:48.360,0:34:55.920
Eh, tra le le l'ultimo state of the art di ognuno 
dei dei vendor principali è quello successivo,

0:34:55.920,0:35:02.440
la sensazione che il i i come dire i tempi 
tra un rilascio e l'altro si siano accorciati.

0:35:02.440,0:35:15.320
Non so se anche tu hai questa eh questa Sì, 
sì. L'esponenziale rimane. Sì, sì. E poi eh

0:35:15.320,0:35:24.120
c'è tutto un discorso di di benchmarking. Mh 
ci ragionavo giusto l'altra sera. Eh inizia a

0:35:24.120,0:35:31.080
diventare anche difficile capire come eh spiegare 
al all'utente okello che è uscito è meglio di

0:35:31.080,0:35:39.400
quello che c'era prima. Eh, per i modelli open 
weight ho visto che la tendenza è quella di dire

0:35:39.400,0:35:49.520
"Ok, questo modello rispetto allo state of the art 
dei modelli closed source si posiziona più o meno

0:35:49.520,0:35:58.200
qua come dire siamo quasi per dire a livello di 
Opus 45 piuttosto che Opus 46. Eh, invece sugli

0:35:58.200,0:36:06.440
altri il problema è che eh i benchmark non sempre 
sono significativi, non sempre sono o magari sono

0:36:06.440,0:36:11.800
significativi, ma non ti danno davvero l'idea 
di quanto eh sia migliorato un modello rispetto

0:36:11.800,0:36:17.240
ai precedenti. Eh, che scusami Alessio, era un 
po' quello che ci chiedevamo tra di noi queste

0:36:17.240,0:36:23.400
settimane quando era arrivato l'annuncio di Gemini 
3.1 uno che citiamo lui perché è più facile che le

0:36:23.400,0:36:27.840
persone l'abbiano incrociato essendo di Google e 
ci chiedevamo "Ma che cosa fa diverso?" E Stefano

0:36:27.840,0:36:35.920
ci spiegava e per quanto sia adesso glielo faccio 
ripetere, ma per quanto sia interessante il dubbio

0:36:35.920,0:36:42.560
era sempre boh, ok? Cioè mh quanto mi impatta 
direttamente come persona. Stefano, scusa,

0:36:42.560,0:36:52.360
ricordaci cosa fa tre. che era già ottimissimo. 
Allora, tre era già pazzesco e eh sui benchmark è

0:36:52.360,0:37:00.560
migliorato tantissimo, cioè 3.1, Uno, ne cito uno 
perché è quello che mi ricordo, Archeggi 2, che è

0:37:00.560,0:37:05.920
un benchmark relativamente nuovo, tra l'altro 
è un benchmark relativamente nuovo Archeggi

0:37:05.920,0:37:13.920
2 e che dovrebbe testare la capacità EGI del 
modello, cioè capacità di essere meglio del dalla

0:37:13.920,0:37:26.040
media dell'uomo, di tutti gli uomini sulle varie 
attività e ehm 3.0 Zero era stato salutato come

0:37:26.040,0:37:37.480
incredibile perché faceva il 48% 46% eh rispetto 
ad un Chat GPT che faceva 37 per intenderci, no?

0:37:38.200,0:37:47.120
Ecco, 3.1 fa 86, eh, che è quasi il doppio. 
Promosso, promosso, adesso è promosso. tanto che,

0:37:47.120,0:37:55.440
tanto che hanno dovuto fare Arch 3, perché così 
non ha più senso, è a tappo e ed è uscito Ark3

0:37:55.440,0:38:02.960
e se vuoi sulla velocità dei modelli che dicevamo 
prima con Alessio è incredibile anche sono andato

0:38:02.960,0:38:10.640
a vedere questo dato qua per preparare la puntata 
la velocità di rilascio dei dei benchmark anche,

0:38:10.640,0:38:16.240
cioè non ci stan dietro con i benchmark. Beh, 
anche la velocità di rilascio dei benchmark è

0:38:16.240,0:38:22.840
accelerata per forza perché li mandano tappo. 
Che comunque devi pensare a dei test che

0:38:22.840,0:38:31.520
siano sufficientemente challenging, ma non tra 
virgolette fuori dal mondo, perché deve essere,

0:38:31.520,0:38:38.400
come dire, progressiva la la capacità di 
eh di passare, di migliorare, eccetera.

0:38:38.400,0:38:46.680
Non non riesco riesco a fare tutto, eh, cioè c'è 
un E gli altri dati impressionanti che quelli li

0:38:46.680,0:39:00.440
vediamo anche su Opus 4.6 e su Kimi K25, che è uno 
degli altri da citare come rilasci, è la capacità

0:39:00.440,0:39:10.400
di andare multiagente con il sub agent nativo e 
fare compiti estremamente lunghi, cioè Codex è

0:39:10.400,0:39:20.800
arrivato a 28 ore di compito svolto correttamente, 
cioè ti do il prompt diciamo il contesto,

0:39:20.800,0:39:30.240
fai questa cosa e 28 ore dopo è arrivato con il 
risultato corretto senza altra interazione umana.

0:39:31.240,0:39:37.800
Questo qui è l'altro dato eh a cui si si fa molta 
attenzione in questo momento, cioè la capacità di

0:39:37.800,0:39:44.720
svolgere compiti lunghi e complessi e magari di 
parallelizzare, tipo chi mi ha spinto tantissimo

0:39:44.720,0:39:51.000
con il 25 su quella roba qua, hanno avuto un 
miglioramento pazzesco da quel punto di vista

0:39:51.000,0:39:58.040
e anche Minimax, anche se io non l'ho provato su 
Minimax. due, diciamo, light motive che ho visto

0:39:58.040,0:40:07.120
in questo giro di rilasci sono eh uno, la tendenza 
ad allinearsi su un nuovo standard di 1 milione di

0:40:07.120,0:40:15.120
token dimensione della della finestra del contesto 
che eh più o meno tu, non tutti, però vabbè.

0:40:15.120,0:40:27.720
Gemini 3.1 Pro, Quen 3.5, insomma, adesso è 
la la nuova Opus Opus 45, esatto. 46, scusa,

0:40:27.720,0:40:38.960
è il nuovo, diciamo, il nuovo target milione di di 
contesto e poi l'altra cosa è m specializzazioni,

0:40:38.960,0:40:45.040
reinforcement learning, training eccetera, 
specifici per il coding eh quasi tutti,

0:40:45.040,0:40:50.680
eh che vabbè l'abbiamo già detto altre volte, 
ci sta perché è l'ambito all'interno del quale

0:40:50.680,0:40:56.120
si stanno vedendo soprattutto i risultati perché, 
tra virgolette facile, perché è ben verificabile

0:40:56.120,0:41:01.600
eccetera, però mh ed è quello che ha più 
impatto anche. Esatto. anche perché ti serve per

0:41:01.600,0:41:07.200
sviluppare nuovi modelli e quindi di conseguenza 
o nuovi software o nuovi software, cioè quello

0:41:07.200,0:41:15.080
che in questo momento ha più impatto perché 
essendo i modelli confinati nell'ambito virtuale,

0:41:15.080,0:41:20.200
passatemi il termine, nel cloud eccetera, le due 
cose che hanno più cose che hanno più impatto è

0:41:20.200,0:41:27.240
se riesci a migliorare il workflow di lavoro 
di una persona, ma ancora di più se riesci a

0:41:27.240,0:41:38.560
scrivere codice o o progetti effimeri anche 
che vadano in quella direzione. Infatti, ehm,

0:41:38.560,0:41:48.560
c'è un post di Cloud Flir su X di questi giorni, 
eh che adesso non ho sottomano, ma ma l'ho letto,

0:41:48.560,0:41:56.440
eh che riprende una vecchia idea di Hugin Face 
con un progetto che si chiama Small Agent che

0:41:56.440,0:42:04.040
con cui avevo giocato e contribuito un po' tempo 
fa, che è quello invece di generare chiamate API o

0:42:04.040,0:42:11.880
MCP di generare codice e farlo eseguire al volo ai 
modelli. Eh, questa cosa ovviamente è interessante

0:42:11.880,0:42:18.800
perché il codice è più espressivo di una semplice 
pi. Banalmente ci puoi mettere i for e nel codice

0:42:18.800,0:42:25.400
e concatenare più cose, però ci dice anche che 
i modelli stanno diventando abbastanza maturi

0:42:25.400,0:42:33.640
a generare il codice, almeno piccole porzioni di 
codice, in maniera così affidabile che eh molti,

0:42:33.640,0:42:40.520
anche Opus fa questa cosa, m molti stanno 
cominciando a dire "Va bene, l'estensione

0:42:40.520,0:42:47.840
del modello oltre alla reasoning è autoscriversi". 
del codice per risolvere sottoparti del problema

0:42:47.840,0:42:56.880
in maniera deterministica. Eh, e questo potrebbe 
essere un ulteriore salto interessante. Eh,

0:42:56.880,0:43:03.960
invece un'altra cosa che ho notato che iniziano 
a vedersi anche dei tentativi di eh per i modelli

0:43:03.960,0:43:12.520
openweight, chiaramente, di esplicitare e 
di conseguenza tendere a a ridurre eh la

0:43:12.520,0:43:18.560
quanto impatti la quantizzazione sui modelli 
o l'utilizzo di mixture of expert, diciamo, un

0:43:18.560,0:43:24.720
attimo aggressivi, sul risultato finale, sul sulla 
qualità dei risultati ottenuti con l'inferenza.

0:43:24.720,0:43:31.040
e mi riferisco, ad esempio, a Quin 3.5 5 che nel 
suo rilascio in realtà ha rilasciato un gruppo

0:43:31.040,0:43:41.120
di modelli, non solo uno, eh con dimensioni 
varie da 400 billion fino a scendere a 27 con

0:43:41.120,0:43:55.600
mixture of expert diversi che scendono a 17, a 
10, a 3 miliardi di eh di esperti e eh ha fatto

0:43:55.600,0:44:02.040
il i benchmark con tutte queste versioni ha fatto 
vedere quanto perde il modello mano mano che lo

0:44:02.040,0:44:10.000
diciamo lo tagli, lo fai diventare più piccolino. 
Eh, come di nuovo dimostrare che la la frontiera

0:44:10.000,0:44:16.000
è anche nel cercare di eh ottimizzare, diciamo, 
la riduzione delle dimensioni per poter ottenere

0:44:16.000,0:44:23.280
ancora dei risultati accettabili anche con risorse 
più basse. Eh, perché credo che gli open weight

0:44:23.280,0:44:33.320
stiano scegliendo come target attendere mano che 
i computer diventano più potenti, eh l'inferenza

0:44:33.320,0:44:39.400
locale e quindi per loro è interessante andare 
a ridurre le dimensioni, mentre invece gli state

0:44:39.400,0:44:49.240
of the artig al momento sono focalizzati e 
dichiaratamente a raggiungere le giì fa fanno

0:44:50.000,0:44:59.520
Wat? Esatto. Cioè, Sì, sì, assolutamente. Eh, 
lasciami fare una digressione tecnica che che

0:44:59.520,0:45:07.680
ha a che fare anche con questa cosa, ma che magari 
qualche ehm utente si potrebbe chiedere e pensare

0:45:07.680,0:45:17.920
che eh le BigTtech stiano cercando soltanto 
di fare più soldi con questa manovra. Eh,

0:45:17.920,0:45:22.480
in realtà non ci sono delle giustificazioni. E 
qual è la manovra? Scusate, non non ho messo il

0:45:22.480,0:45:30.120
soggetto. Eh, vi sarete accorti che tutti vanno 
verso one million token, ma sopra i 200k cost

0:45:30.120,0:45:37.880
costa i token costano molto di più, cioè hanno 
un pricing fino a che usi il contesto piccolo e

0:45:37.880,0:45:43.680
c'è un pricing diverso se usi il contesto grande 
e uno dice "Vabbè, ok, vuoi farmi pagare di più

0:45:43.680,0:45:49.040
perché voglio fare di più?" Sì, magari una parte 
di verità è questa. Beh, ma costa anche molto di

0:45:49.040,0:45:57.040
più puoi fare l'inferenza con conti No, non solo 
costa di più fare l'inferenza con contesti pieni,

0:45:57.040,0:46:03.680
ma è proprio quello volevo spiegare, cioè eh 
allora il contesto va a finire in una memoria, una

0:46:03.680,0:46:11.520
cash che si chiama KV Cash. Eh ne abbiamo parlato 
nella puntata domande e risposte che abbiamo fatto

0:46:11.520,0:46:18.600
tanto tempo fa. Se volete andate la a pescare. 
Mh, qua dico soltanto in maniera intuitiva,

0:46:18.600,0:46:25.160
cioè il problema è che la quantità di memoria 
utilizzata non scala in modo lineare alla quantità

0:46:25.160,0:46:32.280
di token che metti nel contesto. Non è neanche 
esponenzialeh, una curva un pochino piegata,

0:46:32.280,0:46:43.200
diciamo. Barzotta si dice Barzotta perché il il 
problema del il sostanzialmente la Kevy Cash che

0:46:43.200,0:46:51.040
cosa fa con i contesti lunghi? deve mantenere 
il contesto, appunto, delle singole parole con

0:46:51.040,0:47:00.800
quelle precedenti, ma siccome il il collegamento 
delle parole correnti esplode non soltanto perché

0:47:00.800,0:47:05.360
il contesto indietro è più lungo, ma perché il 
contesto dietro è più ramificato e potresti avere

0:47:05.360,0:47:17.640
più eh ehm più collegamenti con un numero più alto 
di parole precedenti, eh perché magari hai detto

0:47:17.640,0:47:24.920
che ne so ancora, ma che non sai se è ancora o è 
ancora e ancora ha legami con un sacco di roba.

0:47:25.800,0:47:33.360
Eh, questa cosa qua fa scalare la dimensione dalla 
RAM in maniera non lineare, ma più che lineare

0:47:33.360,0:47:38.600
rispetto alla dimensione del contesto. Quindi 
per contesti grandi il costo anche per chi fa

0:47:38.600,0:47:46.400
inferenza aumenta molto e quindi te lo fan pagare. 
chiuso la parentesi tecnica, ma giusto per capire

0:47:46.400,0:47:52.040
anche che c'è una complessità dietro diversa 
da da quella a cui siamo abituati, cioè noi

0:47:52.040,0:47:59.800
siamo abituati, va bene, piglio più RAM, ho RAM 
X, prenderò il doppio e sarà x 2. In realtà non

0:47:59.800,0:48:10.560
serve il doppio della RAM, serve tre volte circa 
la RAM per fare il doppio del contesto. Fine.

0:48:10.560,0:48:20.320
Eh, ok. E dunque io cosa ho provato? Ecco, 
parliamo di cosa di questi modelli nuovi che

0:48:20.320,0:48:30.760
sono usciti. Io ho provato Minimax M2.5 e GLM5. Di 
GLM c'è anche il paper che è super interessante,

0:48:30.760,0:48:34.960
soprattutto nella parte di training 
perché prendono molte delle idee che

0:48:34.960,0:48:41.200
c'erano in Deepsic 32 e le stendono eh sulla 
parte di training. Non mi si addentro qua,

0:48:41.200,0:48:46.960
se avete voglia andatevelo a leggere 
però perché c'è sia il pap, diciamo,

0:48:46.960,0:49:01.560
più divulgativo che si capisce molto bene. 
Ehm, allora i modelli vanno distinti un po' per

0:49:01.560,0:49:13.160
in due modi, no? Le risposte. Una è la la qualità 
della risposta, ma c'è anche un discorso di ehm

0:49:13.160,0:49:21.640
consistenza delle risposte. Allora, spieghiamo 
si intende per consistenza. Spiegalo. No, no,

0:49:21.640,0:49:27.040
spiega tu, spiega tu. Ah, spiego io. No, 
beh, consistenza delle risposte che eh a

0:49:27.040,0:49:34.240
stessa domanda ottengo una risposta che ci 
si avvicini molto, molto banalizzando molto,

0:49:34.240,0:49:41.440
se non la stessa la stessa impossibile perché 
c'è un determinismo, ma eh se faccio due domande

0:49:41.440,0:49:49.560
uguali con lo stesso contesto, mi aspetto che 
le due risposte siano simili o indistinguibili

0:49:49.560,0:49:54.160
nel nella versione ideale. Se vuoi che non è 
sufficiente che il modello ti risponda giusto

0:49:54.160,0:49:59.400
una volta sola, ma deve risponderti sempre la 
stessa cosa. In detto in altri termini, sì,

0:49:59.400,0:50:05.080
certo, non deve rispondere sempre sbagliato, 
senò una consistenza brutta. Mentre invece la

0:50:05.080,0:50:11.480
qualità della risposta, parlando di coding, visto 
che io li ho provati per coding, è che il codice

0:50:11.480,0:50:19.880
generato sia di buona qualità, faccia quello 
che gli è stato chiesto, eccetera eccetera.

0:50:20.720,0:50:30.920
Allora, sulla qualità eh, diciamo che del 
caso migliore sono paragonabili e guardando

0:50:30.920,0:50:39.080
i benchmark si avvicinano molto tutti e due a 
Opus 4.5, quindi la versione precedente di Opus.

0:50:40.000,0:50:47.960
Eh, sulla consistenza GLM è molto molto meglio 
anche guardando dati in giro, ma è anche nella

0:50:47.960,0:50:56.680
sensazione che ne hai nell'utilizzarlo. Poi c'è 
un terzo parametro ed è la velocità e minimax

0:50:56.680,0:51:05.240
è di una velocità spaventosa, cioè è più veloce 
di Sonnet, eh, è velocissimo rispondere e quindi

0:51:05.240,0:51:10.560
la la fase di di reward che ho io dalla mia slot 
machine è velocissima e quindi continuo a cliccare

0:51:10.560,0:51:18.680
come un Stavo pensando eh in questi giorni stavo 
capendo, ho ho scoperto, anzi parliamone live,

0:51:18.680,0:51:23.800
ho scoperto perché me l'ha detto Col Code 
ultimamente le cose le scopo da lui principalmente

0:51:23.800,0:51:31.280
che ehm quando usi Subagens in Cloud Code, lui 
per i subaggents sceglie IQ, quello che è il

0:51:31.280,0:51:36.960
configurato come IQ per fare le attività. sempre 
non lo so, non ho verificato, però lui mi ha detto

0:51:36.960,0:51:41.160
questa cosa, diciamo che indipendentemente se sia 
completamente vero oppure no, era interessante ed

0:51:41.160,0:51:48.080
era plausibile e mi faceva ragionare e e diceva 
"Ok, quindi forse allora non sono sempre così

0:51:48.080,0:51:53.600
contento di demandare i subaggent perché IQO 
ha comunque delle capacità limitate e quindi

0:51:53.600,0:51:57.800
va bene quando lo si manda sul binari, ma quando 
si esce dai binari forse non è una buona idea."

0:51:58.480,0:52:01.960
E allora stavo dicendo che palle, ne parlavo 
con degli amici, mi dicevo, gli dicevo,

0:52:01.960,0:52:05.560
ho scoperto questa cosa, forse smetterò di 
usare un pochettino i subent intimorito da

0:52:05.560,0:52:10.240
questa cosa e qualcuno di loro giustamente mi ha 
detto "Ma non puoi cambiare le carte in tavola,

0:52:10.240,0:52:15.280
non puoi rimappare IQ a quello che vuoi tu?" Eh 
sì, quello è stato stato possibile. Non ci avevo

0:52:15.280,0:52:19.600
pensato io. E quindi adesso che tu, Stefano, 
parlavi di queste cose, stavo dicendo perché

0:52:19.600,0:52:25.960
non provi a giocare con questa cosa? Perché non 
rimappi il tuo IQ a Minimax e vedi se eh ti dà

0:52:25.960,0:52:32.240
delle risposte velocissime per delle cose più, 
diciamo, che non richiedono super intelligenza

0:52:32.240,0:52:40.920
e usi il tuo modellone principale per quelle 
altre, no? Questa è un'idea interessante molto

0:52:40.920,0:52:48.080
e ed è una delle cose che volevo che volevo 
provare. invece è una domanda filosofica, nel

0:52:48.080,0:52:53.080
senso che noi stiamo parlando di velocità, che poi 
vabbè bisognerebbe distinguere tra velocità nel

0:52:53.080,0:53:00.720
processare il il prompt, velocità nel generarti la 
risposta, ma una cosa, cioè ci sono, se vuoi, due

0:53:00.720,0:53:08.200
modi di utilizzare questi coding agent, uno è tra 
virgolette in puro vibe coding, eh chiedo quando

0:53:08.200,0:53:15.280
mi è arrivato il risultato passo allo step dopo 
eccetera, oppure m sto lì e e leggo anche tutto

0:53:15.280,0:53:21.520
il reasoning che il modello sta facendo nel nel 
darmi la risposta, che se vuoi ha anche un aspetto

0:53:21.520,0:53:29.080
mzione, di learning, non so come dire, eh il fatto 
che il modello sia più veloce, che tu magari non

0:53:29.080,0:53:36.640
fai neanche in tempo a leggerti tutto il il suo 
flusso e te lo devi guardare dopo m ha una sua

0:53:36.640,0:53:44.000
rilevanza per voi o cosa? Dipende come lo usi, 
cioè nel senso che io il controllo O lo schiaccio

0:53:44.000,0:53:51.400
raramente. Controll O è per vedere tutta la parte 
di reasoning perché di default ormai Cloud Code ce

0:53:51.400,0:54:01.240
l'ha disabilitata e compressa. Eh, dipende. Plan, 
lo leggo, ma una volta che sono contento dal plan

0:54:01.240,0:54:08.000
che lui faccia il tentativo, no, cazzo, non sono 
riuscito, non passa il test, faccio no, cioè solo

0:54:08.000,0:54:13.800
quando non c'ho niente da fare, voglio gi è come 
guardare Twitch, non so. Io vi dico la verità, io

0:54:13.800,0:54:20.040
lo leggo ed è come guardare Twitch, esattamente, 
ma lo leggo perché non devo schiacciare contrl O.

0:54:20.040,0:54:27.040
Il mio alternativo di Cloud Code che uso eh 
lo tiene aperto in automatico, tant'è che mi

0:54:27.040,0:54:35.840
è successa una cosa curiosa che ci stava nella 
storia precedente. Ehm, ho sono stato bloccato

0:54:35.840,0:54:41.040
dopo avere, scusatemi, sto sto mi sto mangiando le 
parole. Dopo essere riuscito ad aumentare la mia

0:54:41.040,0:54:47.360
produttività mettendomi cloud code su Telegram, 
sono stato bloccato dall'ulteriore vincolo che

0:54:47.360,0:54:54.480
ho ehm ho colpito i i limiti di messaggi di 
Telegram. Generavo talmente troppi messaggi col

0:54:54.480,0:54:59.240
mio cloud code che quindi mi ha cappato Telegram 
stesso. Perché mi ha accappato Telegram stesso?

0:54:59.240,0:55:03.880
probabilmente perché io stavo facendo questa cosa 
su Telegram, ricevevo anche tutti i reasoning

0:55:03.880,0:55:07.800
ed erano sono molto interessanti, soprattutto se 
non sai cosa stai facendo, come nel caso del mio

0:55:07.800,0:55:15.040
progetto. Ehm cioè impari mano che vai, alcune 
cose ovviamente sono lui che cerca un concetto,

0:55:15.040,0:55:19.120
scopre che era nell'altra classe, cioè chi se ne 
frega. Altre volte invece lui si rispiega le cose

0:55:19.120,0:55:23.960
da solo, dice "Allora facciamo così, cosà per 
via di questi motivi". Ed è molto interessante.

0:55:23.960,0:55:29.960
Curiosità ulteriore. Ho beccato leggendo tutti 
questi log di esecuzione che deve esserci o

0:55:29.960,0:55:35.720
qualche errore oppure lui racconta male la storia 
perché ogni tanto non trova dei file e nella

0:55:35.720,0:55:40.840
stessa frase dice il file che dovrei guardare è 
quest'altro che ha esattamente lo stesso nome,

0:55:40.840,0:55:48.480
quindi o è un rendering sbagliato di versioni 
del file che però lui si mostra il il percorso

0:55:48.480,0:55:53.840
e quindi il percorso è corretto, è uguale, ma lui 
intende due punti temporali diversi di quel file

0:55:53.840,0:55:59.440
oppure c'è un bug e me ne sono accorto leggendo 
quello che combina. Un problema con i tool,

0:55:59.440,0:56:03.560
dico problema con i tool potrebbe essere un 
problema con il modello o potrebbe essere solo

0:56:03.560,0:56:08.120
un problema di logging, come vi dicevo, magari 
in realtà lì c'è un ash code, eh i due ashode

0:56:08.120,0:56:13.600
sono diversi, ma il nome del file invece è lo 
stesso. Comunque li leggo e sono interessanti,

0:56:13.600,0:56:18.760
però più in generale Alessio mi hai fatto venire 
in mente che questo problema che tu manifestavi,

0:56:18.760,0:56:25.080
ovvero riusciamo a dare retta a tutto quanto il 
flusso di esecuzione? La risposta è no, ma questo

0:56:25.080,0:56:31.080
problema si manifesta forse più visivamente nei 
progetti di collaborazione, di coding con gli

0:56:31.080,0:56:37.760
open source in particolare in cui è aumentata così 
tanto la produttività per produrre nuovo codice PR

0:56:37.760,0:56:44.000
che adesso il collo di bottiglia è la revisione di 
questo codice. i i tech lead, i project lead seri

0:56:44.000,0:56:48.640
che non vogliono accettare qualunque cosa, non 
hanno letteralmente il tempo per stare a leggere

0:56:48.640,0:56:52.800
tutta la roba che gli arriva e alcuna delle robe 
che gli arriva in alcuni progetti più popolari,

0:56:52.800,0:56:58.800
era famoso l'esempio del tizio di Curl, è 
Patumiera e e loro sprecano del tempo a leggere

0:56:58.800,0:57:05.760
Patumiera di Iyel o di concetti sbagliati, cattive 
idee in generale che adesso il primo che passa gli

0:57:05.760,0:57:10.440
lancia addosso e lascia loro l'incombenza 
di decidere quale rumore, quale segnale.

0:57:13.720,0:57:21.320
Eh, intanto che parlavamo ho verificato qualcosa 
che dicevi giusto per dare l'informazione completa

0:57:21.320,0:57:29.000
e perché mi incuriosiva a me. Allora, per i 
sabigent usa IQ soltanto quando vanno in explore,

0:57:29.000,0:57:37.400
cioè quelli ridonlyent di Explore, ma quando va 
in plan o general purpose lo eredita eredita il

0:57:37.400,0:57:43.640
modello dal dalla sessione madre, quindi se 
sei partito con opus va con Opus. Ok. Ok. Sì,

0:57:43.640,0:57:47.800
è più ragionevole, però appunto era comunque 
affascinante l'idea che ci sia questo livello di

0:57:47.800,0:57:55.120
ottimizzazione dentro Cloud Code che uno volendo 
può preferire, no? Ero già pronto a giocare con le

0:57:55.120,0:58:03.440
variabili d'ambiente, come mi avevi consigliato 
tu, ma la parte di Explore mi interessa poco.

0:58:03.440,0:58:14.400
Ehm, quindi eh, aspetta, usando il mio 
e quindi che tanto piace a Paolo, eh

0:58:14.400,0:58:21.960
no, poi c'è ecco, però la velocità è un tema, 
eh la velocità è un tema, tanto che quelli di

0:58:21.960,0:58:31.480
GPT ci si sono messi pesanti, perché GP3 GPT 
5.3 3 Spark va a livello di velocità di numero

0:58:31.480,0:58:38.440
di token 6 volte più veloce di 5.3 normale 
allo stesso prezzo con meno token, quindi

0:58:38.440,0:58:45.840
a prezzo più alto per token. Ma infatti qui la 
domanda, se vuoi, è ma fino a che punto uno può,

0:58:45.840,0:58:56.640
tra virgolette sacrificare il l'accuratezza per 
avere invece velocità? Eh eh quella è una domanda

0:58:56.640,0:59:08.120
di utilizzo. Allora, nell'utilizzo generico come 
assistente personale secondo me assolutamente sì.

0:59:08.120,0:59:16.600
Nel senso, boh, cioè se lo uso per spostare file, 
organizzarey, farmi le slide, quelle cose lì,

0:59:16.600,0:59:22.080
che Minimax vada come un fulmine, mi interessa di 
più che sia perfettamente accurato, perché tanto

0:59:22.080,0:59:28.960
poi le rivedo sul codice. Tu sai già che il task è 
sufficientemente facile, per cui in ogni caso farà

0:59:28.960,0:59:38.360
Eh, esatto. Sul sul codice complesso, boh, non lo 
so. Tanto che io sto pensando di di smettere di

0:59:38.360,0:59:45.800
pagare tutti i cinesi che pago e pagare soltanto 
un americano, cioè eh Opus e fine. Stavo giusto

0:59:45.800,0:59:52.040
guardando quanti quanti chiamate mi fa fare la 
versione 5x e sono a livello di minimax quasi

0:59:52.040,1:00:00.640
quasi proprio a livello di 20 20 ne ho già 
da 20 ne li do già da Antropic. 40 li do, eh,

1:00:00.640,1:00:07.560
li do a minimax, li metto insieme, sono già 60. 
Con 40 in più mi trovo con Opus 4.6, forse che

1:00:07.560,1:00:12.760
forse dal mese prossimo faccio questa scelta qua, 
così mi avete fatto tutti i conti in tasca. C GPT,

1:00:12.760,1:00:19.600
no, ma quello non posso non darglieli perché mia 
figlia mi ammazza se smetto di pagare chat GPT.

1:00:19.600,1:00:25.240
Quindi, comunque, scusami, prima stavi raccontando 
che per spostare F sul desktop ti va bene un

1:00:25.240,1:00:31.160
modello veloce più che uno bravissimo. Eh, 
mi richiami alla storia, una delle news di

1:00:31.160,1:00:40.240
questa settimana, la K della sicurezza. L'abbiamo 
preparata questa gli ho chiato a posto che c'è un

1:00:40.240,1:00:47.600
un post su Twitter credo c'è un uno screenshot di 
una chat dove non mi ricordo più come si chiama

1:00:47.600,1:00:54.920
eh la persona che è a capo della sicurezza in 
meta ehm gli è scappato di mano il suo peccolo

1:00:54.920,1:01:00.920
nonostante avesse tutte le precauzioni del caso 
definite e gli ha iniziato a cancellare tutta la

1:01:00.920,1:01:06.920
mail che trovava e lei racconta come Come in una 
scena di film d'azione dove corri a cercare di

1:01:06.920,1:01:11.720
smantellare la bomba prima che stia per esplodere, 
è dovuta correre davanti al computer e cercare di

1:01:11.720,1:01:19.280
bloccarlo e e sei in la capa della sicurezza di di 
Facebook che quindi qualcosa ci dovresti capire e

1:01:19.280,1:01:24.120
le persone che lavorano insieme a te ti avrebbero 
dovuto dire esattamente come fare succedere che

1:01:24.120,1:01:30.520
questo non succedesse ed è successo nonostante 
tutto. Ecco, tra l'altro questo spieghiamo eh un

1:01:30.520,1:01:36.480
secondo per gli ascoltatori anche tecnicamente che 
cosa è successo, perché poi sono andato a leggere.

1:01:36.480,1:01:42.080
Allora, premesso che doveva mettere dei gard 
rail più importanti sulla sua mail, eccetera,

1:01:42.080,1:01:50.880
eccetera, ma lei pensava di essere a posto perché 
tra le le Open Clow, come anche Cloud Code,

1:01:50.880,1:01:58.920
gli puoi dare eh una sorta di system prompt 
all'interno del quello che è Cloud MD per cloud

1:01:58.920,1:02:05.960
di cose che deve sempre rispettare e lei le aveva 
detto suggerisci soltanto modifiche sulla mail.

1:02:06.800,1:02:15.480
non prendere mai iniziativa, non fare mai eh 
cambiamenti. Allora, la cosa giusta era dargli

1:02:15.480,1:02:22.520
delle PI che non facessero cambiamenti, fossero 
in sola lettura. Lei si è fidata del dirlo al

1:02:22.520,1:02:28.560
modello e che cosa è successo tecnicamente? 
Questo anche per far capire perché tante

1:02:28.560,1:02:36.760
volte abbiamo insistito anche nella puntata eh 
quando c'era Alex, l'abbiamo spiegato bene. Eh,

1:02:36.760,1:02:45.120
tra l'altro credo che ormai sia il nostro ospite 
più citato, Alex. glielo dirò questa cosa,

1:02:45.120,1:02:52.280
però lo spiegavamo dal fatto che una delle dei 
motivi per usare backlog o sistemi simili è che tu

1:02:52.280,1:02:59.840
fai una sessione, usi tutto il contesto, chiudi, 
riapri o fai clear in modo da partire da sessione

1:02:59.840,1:03:06.760
pulita e di non arrivare mai al comprimere la 
sessione perché quando comprimi potresti perdere

1:03:06.760,1:03:13.280
un po' di qualità. Quello che è successo lì è 
esattamente quello, che la sua casella di mele era

1:03:13.280,1:03:18.200
così grande che quando ho cominciato a leggerla 
tut i messaggi che doveva cancellare erano così

1:03:18.200,1:03:27.040
tanti perché evidentemente non faceva zero in box 
come policy, eh ha ne ha letti così tanti che ha

1:03:27.040,1:03:32.320
riempito il contesto, ha deciso di comprimere, 
l'ha riempito ancora, ha deciso di comprimere,

1:03:32.320,1:03:36.800
l'ha riempito ancora, ha deciso di comprimere 
e nell'ultima compressione, lei dice la terza,

1:03:37.400,1:03:45.480
nella compressione si è persa l'istruzione del non 
prendere non prendere iniziativa e ha deciso che

1:03:45.480,1:03:49.880
stava riempiendo ancora il contesto. Ha detto ma 
via invece di invece di continuare a comprimere

1:03:49.880,1:03:55.560
perché non zappo via tutta sta roba che mi è 
libero il contesto e così è fatto. Ho ho due

1:03:55.560,1:04:02.080
aneddoti su questa cosa. La prima è ovviamente 
questi rischi su scala diversa ce li hai anche

1:04:02.080,1:04:08.600
quando fai Cloud code normale per sviluppare 
cose e gli sviluppatori saggi di Antropic ci

1:04:08.600,1:04:14.640
hanno donato gli hooks per intercettare prima che 
vengano eseguiti i comandi, i comandi stessi. Così

1:04:14.640,1:04:21.640
tu puoi avere dei guard rail eh strong, ovvero 
non con del testo che ogni tanto può perdersi,

1:04:21.640,1:04:26.560
proprio dei passaggi software tradizionale, per 
cui il codice non va avanti finché non viene fatto

1:04:26.560,1:04:32.840
questo ragionamento. E ed è molto importante a 
mio avviso perché mi veniva da commentare prima,

1:04:32.840,1:04:36.480
si vede che quella persona in Facebook non aveva 
mai passato abbastanza tempo a fare vibe coding

1:04:36.480,1:04:41.800
perché ci incapi inevitabilmente che tu gli dici 
di non fare una cosa, lui la fa e a un certo punto

1:04:41.800,1:04:46.840
lo devi proprio bastonare e dire adesso mi sono 
arrabbiato, voglio verificare ogni singola cosa

1:04:46.840,1:04:50.360
che provi a fare, ti impedisco di fare quelle 
che ti ho detto di non fare, che è una lampada

1:04:50.360,1:04:55.640
di Aladino, tipo la lampada di Aladino. Sì, 
ovviamente poi, ehm, cioè funziona questa cosa,

1:04:55.640,1:05:03.320
non ci sono mai contro, è semplicemente un lavoro 
lungo e laborioso perché eh tu ti ricordi tutti i

1:05:03.320,1:05:08.440
99 casi su 100, ma ti dimentichi il centesimo 
e scopri che in realtà erano 500 i casi e

1:05:08.440,1:05:13.400
quindi quel problema in realtà è sempre dietro 
l'angolo, però migliorano le cose ed è una cosa

1:05:13.400,1:05:19.080
su cui faccio molto m molta attenzione per il 
mio sviluppo e per quello del mio team quando

1:05:19.080,1:05:26.240
gli spiego queste cose. La seconda cosa invece che 
me la sono già dimenticata qual era? era che yeah

1:05:26.240,1:05:33.920
sì ehm i contesti e il valore dei contesti e della 
comprensione del contesto. Ho sempre preso sotto

1:05:33.920,1:05:38.560
gamba questa cosa perché ogni tanto avevo più cose 
da fare di quello che il contesto mi permetteva

1:05:38.560,1:05:43.000
e non avevo la sensazione che non si potesse 
fare di meno. Dovevo fare delle cose grosse

1:05:43.000,1:05:46.880
e lui doveva avere informazioni tutte insieme, 
prendere o lasciare. Quindi l'ho sempre preso

1:05:46.880,1:05:52.680
come un male accettabile e tendenzialmente vedevo 
che le performance degradavano un pochettino,

1:05:52.680,1:05:58.200
ma era più una sensazione spannometrica, quindi 
magari era tutto un film nella mia testa o così.

1:05:58.200,1:06:04.920
Questa settimana invece l'ho verificata con 
mano in una maniera quasi buffa. Non so se mi

1:06:04.920,1:06:08.840
è impazzito il terminale, mi ha fatto vedere 
delle cose che non doveva o semplicemente mi

1:06:08.840,1:06:15.640
è capitato di guardarlo tra una compaction e la 
fine della compaction, ma mi ha sputato fuori il

1:06:15.640,1:06:22.160
system prompt che eh compattato successivo ed era 
terribile. Ma non era terribile in cui lui aveva

1:06:22.160,1:06:26.120
selezionato a caso delle frasi, quello potrebbe 
essere accettabile, era proprio sbagliato. c'erano

1:06:26.120,1:06:32.240
ripetizioni di parole, parole mangiate insieme, 
cioè proprio come qualcuno che ha picchiato la

1:06:32.240,1:06:38.480
testa e non non non ragiona più. E questa vederlo 
con i miei occhi mi ha fatto capire quanto non ci

1:06:38.480,1:06:42.320
si possa fare affidamento su quella cosa lì. 
A questo punto quando è successa questa cosa?

1:06:42.320,1:06:48.040
Questa settimana potenzialmente questa settimana 
sì, potenzialmente perché un paio di settimane un

1:06:48.040,1:06:55.360
paio di settimane fa c'era un bug su Cloud Code 
sulla compression e che hanno fissato nel giro di

1:06:55.360,1:07:01.480
qualche ora, ma in quelle ore si è scatenato 
il mondo perché non non andava più nulla.

1:07:02.920,1:07:05.640
Finché se ne parla a livello 
astratto ti dico vabbè ok capisco,

1:07:05.640,1:07:10.760
è meglio se quando l'ho vista ho proprio capito 
che lì io umano non capivo cosa c'era scritto,

1:07:10.760,1:07:16.640
cioè perché ci deve capire lui? Ho proprio 
capito il punto. Non si scherza con quello. No,

1:07:16.640,1:07:24.480
no, infatti eh bisognerebbe sempre cercare di 
lavorare a contesto pulito sulle singole issue,

1:07:24.480,1:07:33.040
portarle in fondo, uscire, rientrare. Ed è il 
motivo per cui gli strumenti che esternalizzano

1:07:33.040,1:07:41.120
questa cosa, che siano PRD, spec, backlog, quello 
che è, perché tutto il ragionamento lo fai prima,

1:07:41.120,1:07:46.960
lo consolidi lì e poi gli dici "Ok, prendi quel 
pezzettino, lui si legge quello che gli serve

1:07:46.960,1:07:56.600
in contesto e e si muove". Ecco. Ehm, cioè 
prima a tanto tempo fa ve l'avevo raccontato,

1:07:56.600,1:08:03.520
non mi ricordo se in podcast o in privato, avevo 
il mio personale flusso in cui facevo la spec e

1:08:03.520,1:08:09.000
poi gli facevo creare un work in progress 
file in modo che lui si intendesse traccia

1:08:09.000,1:08:16.240
di cosa aveva fatto. Eh, strumenti tipo i 
vari, ha già citato Backlog e altri, eh,

1:08:16.240,1:08:22.680
fanno esattamente quella roba lì e ma è veramente 
fondamentale. Tanto che lo dicevamo con Paolo, no?

1:08:23.240,1:08:29.240
ci scrivevamo questa cosa, sia io che te 
lo usiamo anche per task non decoding,

1:08:29.240,1:08:35.040
cioè per prendere appunti delle prossime cose 
da fare sul che che facciamo con l'aiuto di

1:08:35.040,1:08:39.520
Cloud Code, ma non necessariamente di 
coding. Ti dico la verità, adesso che

1:08:39.520,1:08:48.560
c'ho sul telefono ancora di più è un'estensione 
dell'Alexa Siri che non ha mai funzionato bene.

1:08:48.560,1:08:54.480
Per l'altro, scusami, ho fatto una PR a il a 
Backlog MD e me l'hanno provata, era una classe di

1:08:54.480,1:09:00.080
CSS la mia PR, però era un bug effettivo che avevo 
beccato e quindi volevo far presente può contar.

1:09:00.080,1:09:06.000
Ne ho fatte due anch'io. Gli ne ho fatte due 
anch'io proprio questa settimana e penso che

1:09:06.000,1:09:12.480
mi ha detto che le deve guardare, però eh ne ho 
fatte due anch'io perché c'erano due cosette che

1:09:12.480,1:09:18.320
servivano a me personalmente, ma ma che credo 
siano utili al mondo. E sì, contribuospite più

1:09:18.320,1:09:25.040
citato. Sì, no, però contribuite è un bel 
progetto. Eupen, c'ha tante stelle, più di

1:09:25.040,1:09:33.240
1000. Eh, stelle, stelline, campanelli al progetto 
degli altri. No, ma anche al nostro nostro noi le

1:09:33.240,1:09:40.440
stelline, le campanelline vanno messe qua, poi 
dopo ci sono le ci sono le stelle da No, eh lo

1:09:40.440,1:09:47.000
so che altro non lo dice. Anche i punti fragola 
valgono. I punti fragola solo se ce li regalano,

1:09:47.000,1:09:56.440
perché mi piacciono glieli. No, mi piacciono gli 
zaini quell. No, scherzo. Dei piatti della pizza

1:09:56.440,1:10:02.280
di Carrefour, non lo so. Eh, potrebbe essere. 
Beh, ricordiamo che Selunga e Carrefour sono gli

1:10:02.280,1:10:07.320
sponsor di questa puntata. Va bene, credo 
che abbiamo già fatto i pirla abbastanza,

1:10:07.320,1:10:16.400
possiamo cosa dite chiudere. Sì, sì. Va bene. 
Grazie a tutti e tutte di averci ascoltato e

1:10:16.400,1:10:34.440
stelline, campanelline, quelle cose che Paolo 
non vuole che si dicano. E alla prossima. Ciao
