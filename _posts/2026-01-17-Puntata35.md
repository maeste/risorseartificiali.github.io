---
title: "Il caso Grok, ChatGPT Health: perchÃ© lâ€™AI generativa ci costringe a ripensare autenticitÃ , responsabilitÃ  e fiducia #35"
categories:
  - Puntate
tags:

  - AI
  - Deep Fake
layout: single
author_profile: true
---

{% include video id="mwI7ukZ7Kf0" provider="youtube" %}

ðŸ‘‰ [Ascolta su Spotify](https://open.spotify.com/show/16dTKEEtKkIzhr1JJNMmSF?si=900902f2dca8442e)<br/>
ðŸ‘‰ [Guarda su YouTube](https://www.youtube.com/channel/UCYQgzIby7QHkXBonTWk-2Fg)<br/>
ðŸ‘‰ [Segui su LinkedIn](https://www.linkedin.com/company/risorseartificiali)<br/>



## [00:00] Introduction and Context of the Controversy

**Stefano Maestri**
> Ciao a tutti, tutte e bentornati, bentornate. Paolo non saluta perchÃ© Ã¨ maleducato.

**Alessio Soldano**
> Ciao!

**Paolo Antinori**
> Ciao!

**Stefano Maestri**
> Ma c'Ã¨ anche Paolo, se non state guardando il video sappiate che c'Ã¨ anche Paolo. Bene, da dove possiamo partire? Ovviamente dobbiamo partire dalla polemica della settimana, partiamo dalla polemica della settimana. Allora avete visto che probabilmente un po' tutti nel feed hanno visto che c'Ã¨ stata questa polemica delle star di Hollywood contro Grok, l'intelligenza artificiale di Twitter ex Elon Musk, perchÃ© il feed delle immagini di Grok â€” perchÃ© Grok ha un'area che si chiama appunto media se ricordo bene â€” Ã¨ stato invaso da star americane in bikini, che non erano in bikini. PerchÃ© Grok permetteva di spogliare, tra virgolette â€” in realtÃ  non Ã¨ vero, spogliare Ã¨ eccessivo, ma nella cultura americana giÃ  il bikini Ã¨ spogliare â€” di mettere in bikini chiunque a partire da un'immagine vestita e in maniera assolutamente credibile, quindi di fatto generando dei deep fake, se vogliamo dirlo cosÃ¬. E qui c'Ã¨ stata grandissima polemica, levata di scudi. L'Europa sta giÃ  legiferando probabilmente su questa cosa. Ci sono paesi, comunque l'Indonesia ho letto, che ha bloccato l'utilizzo di Grok per questa cosa. Immagino per cultura invece religiosa piÃ¹ stringente. Adesso ragionandoci, perÃ², l'Indonesia credo che sia un paese musulmano.

**Alessio Soldano**
> Assolutamente sÃ¬.

**Stefano Maestri**
> CioÃ¨ dove perÃ² Ã¨ abbastanza evoluto, Ã¨ abbastanza aperto comunque per avere accesso a Twitter e altre cose di questo genere. PerÃ² se prendi una donna di cultura musulmana che Ã¨ coperta per motivi religiosi e la mette in bikini, cioÃ¨ ci sta che loro si siano piÃ¹ irrigiditi di altri secondo me. Detto questo, la reazione di Elon Ã¨ stata quella che a me ha fatto piÃ¹ ridere di tutti, perchÃ© su questa cosa lui ha detto: "Va bene avete ragione", perÃ² non ha messo la parola scusa che non c'Ã¨ nel suo vocabolario credo, ha detto: "Va bene avete ragione, allora lo possono fare soltanto gli utenti paganti". Non ha detto "Non lo fate". Stupendo.

## [02:27] Deepfake Technology and Cultural Reactions

**Alessio Soldano**
> Grandiosa. Esatto.

**Paolo Antinori**
> Chiaramente, chiaramente mi sembra la sua reazione. Io invece volevo commentarvi che Ã¨ un peccato che tutto questo rumore sia avvenuto su Grok, a parte per consentire a Elon di fare questa uscita. PerchÃ© se fosse avvenuto su Google e la gente avesse usato Nano Banana per fare questo mestiere, c'era anche il nome per questa pratica: si sarebbe potuta chiamare Buccia di Banana e sarebbe stato perfetto secondo me. Proprio il nome geniale che neanche i CVI piÃ¹ avanzati potevano permettersi, quindi Ã¨ un'occasione sprecata secondo me.

**Alessio Soldano**
> Beh, ma in realtÃ  credo che sia semplicemente perchÃ© c'Ã¨ l'integrazione tra Grok ed X e quindi era facile poi diffondere tutte le immagini. PerchÃ© probabilmente la gente comunque come lo fa con Grok lo fa anche con Nano Banana e fine.

**Paolo Antinori**
> Ad ogni modo il mio feed di Facebook di alcune mie compagne delle superiori Ã¨ pieno di loro foto in costume che nessuno gli ha chiesto, quindi non sono sicurissimo che sia un reale problema. CioÃ¨ non capisco perchÃ© Ã¨ un problema, perÃ² boh non lo so insomma.

**Alessio Soldano**
> Dice, non sono neanche generate quelle AI.

## [04:50] Legal Implications and Image Authenticity

**Paolo Antinori**
> Ecco, boh, Ã¨ la mia posizione.

**Stefano Maestri**
> No, anche la mia Ã¨ su questo. PerÃ² apre una discussione sui Deep Fake. Ma piÃ¹ che sui Deep Fake ci riflettevo ieri sera, preparando un attimo la puntata. Pensiamo a Gemini. Gemini mette il watermark piÃ¹ o meno complicato per far vedere che l'immagine Ã¨ generata e tutto quanto. In realtÃ  forse almeno sulle immagini personali stiamo ragionando al contrario, perchÃ© la facilitÃ  che si ha di generazione immagini anche con modelli in locale di cui fai fine tuning, di cui fai LoRA eccetera eccetera, e puoi togliere tutte queste cose qui. Forse stiamo ragionando al contrario, forse dovremmo â€” almeno chi ha un'immagine pubblica, non noi tre, anche se siamo su YouTube, ma magari una Liv Tyler della situazione â€” dovrebbe fare il contrario, cioÃ¨ Ã¨ autentico solo quello che firmo io in maniera univoca digitale sull'immagine. I modi per farlo scomodano immediatamente la blockchain nel mio pensiero. Se pensate agli NFT â€” adesso divaghiamo un secondo â€” gli NFT, non-fungible token, dei token che si mettono su una blockchain che sono unici, erano nati per il collezionismo, l'equivalente delle figurine, ma questa cosa qua potrebbe essere usata per le fotografie. Poi indubbiamente il problema rimane, il problema Ã¨ complesso perchÃ© allora la foto del paparazzo che Liv Tyler non firma Ã¨ generata o Ã¨ reale? Si apre tutta un'altra discussione ma quantomeno le foto che io voglio che siano certificate che sono io forse bisognerebbe ragionare al contrario: non un watermark per dire cosa Ã¨ generato ma watermark, o meglio ancora, firma digitale per dire che cos'Ã¨ autentico. Sbaglio?

**Alessio Soldano**
> Ãˆ complesso comunque.

**Paolo Antinori**
> No Ã¨ interessante, appunto non so se... cioÃ¨ o meglio, Ã¨ cosÃ¬ importante sulle foto o no? Posso capire sui documenti: io ho firmato di avere comprato la casa, ho firmato digitalmente il mio documento, Ã¨ inattaccabile. Che la foto della mia comunione sia firmata da me... sti cazzi!

**Stefano Maestri**
> No, la tua no, ma chi lavora con la propria immagine perÃ² sÃ¬. Liv Tyler della situazione, piuttosto che, che ne so, la starlette italiana. Chi lavora con la propria immagine forse boh, non lo so, Ã¨ un mondo che conosco poco.

**Alessio Soldano**
> SÃ¬, ma anche in generale, anche un qualunque personaggio YouTube con tanto seguito eccetera nel momento in cui esce qualcosa di verosimile che perÃ² magari non ha davvero detto lui, puÃ² essere che ti rovini la reputazione eccetera.

**Paolo Antinori**
> Oggi io dico scusate stavo pensando a chi spara un sacco di cazzate davanti a un microfono in un edificio istituzionale di qua e di lÃ  che tanto dice: "No non Ã¨ vero l'ho detto", quindi non lo so lascio un po' il tempo che trova. No invece volevo...

**Stefano Maestri**
> No, certo. No, no, beh, infatti il problema Ã¨ complicato poi da capire cosa voglio che sia autenticato, che cosa no, eccetera. PerÃ² boh.

**Paolo Antinori**
> Volevo aggiungere perÃ² una cosa in questa direzione: un titolo di una news che ho letto ma non ho realmente approfondito era un commento dell'attore Matthew McConaughey che diceva che era infastidito anche lui da questa cosa. Non so se lui lo conoscete ma in realtÃ  la persona Ã¨ piÃ¹ intelligente e piÃ¹ profonda di quanto non diresti guardandolo velocemente. Ascoltando le sue interviste sono rimasto stupito e lui, siccome gli dava fastidio e voleva fare qualcosa di attivo, ha deciso di inseguire una specie di loop legale americano in cui registrava se stesso come un marchio registrato. Essendo lui un marchio registrato, questo attivava tutta un'altra serie di nuove leggi per cui se qualcuno usava la sua immagine violava la legge direttamente. Non so se Ã¨ la fine di tutta questa storia, ma qualcuno ci sta provando.

**Alessio Soldano**
> SÃ¬. Secondo me no, ho letto e ho visto anche del materiale a riguardo io. Il problema Ã¨ che ti vai a impelagare in tutta una serie di differenze tra immagini e video, insomma materiale visivo generato piuttosto che editato con tutti i vari strumenti Photoshop, tutto quello che puoi pensare per l'editing. E a quel punto lÃ¬, dov'Ã¨ la soglia di separazione? Fino a che punto Ã¨ lecito e dopo Ã¨ una rivisitazione o qualcosa di sufficientemente distante dal materiale originario? Se poi consideri che tutto questo Ã¨ lento perchÃ© Ã¨ in qualche modo legato alla burocrazia, alle norme legali eccetera, non funziona. Ãˆ solo un palliativo.

**Paolo Antinori**
> Mi rendo conto, perÃ² tecnicamente Ã¨ la stessa cosa dietro alla Disney e il suo avere sui loro creazioni. Se seguite i news o i media americani di anno in anno, a inizio anno c'Ã¨ l'elenco di materiali vecchi che diventano di pubblico dominio. Qualche anno fa era stato il caso di Topolino in bianco e nero, quello del battello a vapore, per cui John Oliver era andato in televisione apposta per sbeffeggiare la Disney con un pupazzo in bianco e nero di Topolino per far vedere che lui poteva e nessuno poteva dirgli niente. Ancora quest'anno li stavano... quali sono di quest'anno? C'Ã¨ Betty Boop quest'anno, mi ricordo, e qualcos'altro. Pluto forse.

**Stefano Maestri**
> SÃ¬, ma poi il problema Ã¨ complesso anche dal punto di vista che diceva Alessio, perchÃ© ho letto un commento su LinkedIn di una persona italiana che si occupa di queste cose che diceva... adesso non cito la persona perchÃ© non voglio, non avendo davanti quello che ha scritto non voglio citare esattamente le sue parole, ma il concetto che a me Ã¨ piaciuto Ã¨: ok, ma che differenza c'Ã¨ con tutto quello che Ã¨ stato editato fino a ieri su Instagram o altro, tutti i filtri che sono arrivati? CioÃ¨ l'AI Ã¨ solo uno strumento piÃ¹ potente, ma di strumenti per cambiare le fotografie ne avevamo giÃ  tanti e nessuno ha mai detto niente fino ad ora.

## [11:40] Emerging AI Models and Future Directions

**Alessio Soldano**
> E c'Ã¨ anche tutto ciÃ² che Ã¨ in mezzo tra quelle due cose lÃ¬, nel senso che ormai i software di editing hanno l'in-painting generativo da almeno un paio di anni, per cui giÃ  le foto sono state modificate e fino a ieri andava benissimo a tutti, inclusi tutti i vari editor che facevano e fanno tuttora questo di lavoro, e anche alle stesse celebrities perchÃ© magari nel fare il lavoro...

**Stefano Maestri**
> SÃ¬, nel senso che non hanno neanche una ruga.

**Alessio Soldano**
> Esatto, nel fare il lavoro ti sistemi quello che c'Ã¨ da sistemare eccetera. O ti metti in un contesto leggermente migliore di quello in cui hai fatto la foto o cose del genere. Per cui, di nuovo, dov'Ã¨ la linea di separazione?

**Paolo Antinori**
> Se proprio vogliamo, scusate, in passato ho lavorato per un'agenzia di comunicazione e altre robe in cui uno dei servizi che facevano era la fotografia e obiettivamente il nostro fotografo era bravo a fare delle belle foto della gente non bella, se volete la truffa ideologica nasceva giÃ  lÃ¬. Come fa a sembrare bella questa persona che la guarda dal vivo, mamma mia? Quindi, boh, diciamo Ã¨ opinabile.

**Stefano Maestri**
> No, Ã¨ abbastanza opinabile questa parte qua. PerÃ² resta il fatto che siano immagini, siano documenti, siano citazioni... In un mondo che sempre piÃ¹ velocemente genera fake, forse non Ã¨ il fake che deve essere marchiato, ma l'originale, Ã¨ piÃ¹ facile in un certo senso essendo meno. Non lo so, questa qui Ã¨ una riflessione che avevo, se qualcuno magari che ci ascolta ha qualche commento su questa cosa qua, sono curioso, magari ha piÃ¹ esperienza di me nel mondo legale.

**Paolo Antinori**
> Stai con me su questo pensiero perchÃ© non so dove ti porto. Mi hai appena suggerito che una roba interessante su cui sperimentare potrebbe essere un prompt injection visivo nel nostro corpo. Quindi mi tatuo una frase di prompt sulla fronte e ogni volta che qualcuno carica la mia foto io triggero qualcosa. Tra cui: "non usate questa foto per modificare".

**Stefano Maestri**
> Tutto Ã¨ possibile salvo che poi immagino che la prima cosa che fa uno che fa un LoRA, cioÃ¨ un fine tuning di un modello per modificare la tua foto, Ã¨ ignorare quel tatuaggio. PerÃ² vabbÃ¨ tutto Ã¨ possibile. No anche perchÃ© la generazione di immagini e tutto quello che gira attorno l'abbiamo detto nella puntata di chiusura del 2025, che peraltro va molto bene, ancora meglio quella delle previsioni del 2026. Probabilmente i nostri ascoltatori hanno una fiducia spropositata in noi pensando che azzeccheremo qualcosa di quelle previsioni. A parte le battute, perÃ² Ã¨ uno degli ambiti insieme al codice che Ã¨ stato l'uovo di Colombo se vogliamo dal 2025. CioÃ¨ se io penso di nuovo indietro al 2025 le tre cose che hanno cambiato il mondo delle AI sono il reasoning, il coding e le immagini. Se vuoi immagini e video vanno un po' insieme e l'inizio del 2026 non Ã¨ da meno perchÃ© ci sono novitÃ  tutti i giorni. GLM Image qualcuno l'ha visto per esempio?

**Alessio Soldano**
> SÃ¬, sÃ¬. Prima perÃ² lasciami dire una cosa sul discorso di prima. Io credo che una possibile soluzione dal discorso che dicevi â€” distinguere il fake dal reale â€” non sia nel marchiare che cosa Ã¨ fake e cosa Ã¨ reale, ma nella tracciabilitÃ  delle immagini. CioÃ¨ poi decidi tu se accettare un certo tipo di trasformazione e definire che per te quella trasformazione la rende fake. Se perÃ² l'immagine di per sÃ© avesse marchiato da qualche parte la storia di come Ã¨ uscita dalla macchina fotografica ed Ã¨ arrivata a quello che guardi tu, se Ã¨ uscita da una macchina fotografica, a quel punto lÃ¬ tu potresti decidere che cosa fare di quell'immagine. Comunque, GLM Image? SÃ¬.

## [24:49] Exploring Image Generation Techniques

**Alessio Soldano**
> Ãˆ interessante perchÃ© Ã¨ un cambio di paradigma se vuoi. Stiamo parlando di un modello open weight in realtÃ  di ZI, che sono i signori che hanno fatto Z Image, che hanno fatto GLM e che se ne escono con appunto questo modello che definiscono di generazione di immagini autoregressivo discreto. Che cosa significa? Che praticamente abbiamo una prima parte di lavorazione che si basa su un modello come se fosse un LLM â€” in realtÃ  Ã¨ un LLM â€” che fa comprensione del testo piuttosto che dell'immagine, perchÃ© stiamo parlando di un modello sia text-to-image che image-to-image. E ne genera sostanzialmente una rappresentazione in embedding in token discreti, quindi non Ã¨ un qualcosa che poi viene passato alla latent diffusion come in tutti i modelli che siamo abituati ad avere in questi ultimi mesi o anni, ma Ã¨ un qualcosa che rappresenta proprio una descrizione semantica dell'immagine. Come se a partire da un testo oppure da un'immagine noi generassimo un JSON che ci dice qual Ã¨ la composizione dell'immagine, quali sono i soggetti rappresentati, come interagiscono tra loro, tutte queste informazioni. E poi, una volta che abbiamo questa rappresentazione strutturale della semantica dell'immagine, da questo stato intermedio si fa la vera propria generazione dell'immagine. Quindi sono due passaggi, tant'Ã¨ che leggevo che anche il training Ã¨ fatto proprio in due step. PerchÃ© fanno questo? Per avere una fedeltÃ  migliore nella generazione del testo e in tutte quelle scene complesse tipo infografiche piuttosto che diagrammi, queste cose un po' piÃ¹ difficili dove i modelli a cui siamo abituati ultimamente fanno fatica.

**Stefano Maestri**
> Ãˆ interessante, assolutamente. Si potrebbe cercare di capire meglio quello che hai spiegato prima, poi magari facciamo domani una puntata di approfondimento come abbiamo fatto mesi fa sui modelli e potremmo vedere la differenza tra questo GLM Image e per dire Flux e Quen, che sono gli altri modelli open weight a cui siamo abituati a vedere la gente riferirsi per la generazione di immagini.

**Paolo Antinori**
> Io ho trovato molto interessante la tua spiegazione, e anche chiara, perchÃ© se la normale manipolazione di immagine mi sembra magia nera â€” ovvero non riesco proprio a capacitarmi che togli il rumore dall'immagine e viene fuori l'immagine, cioÃ¨ follia, simile al lavoro che fa lo scultore con un blocco di marmo â€” rimane comunque follia.

**Alessio Soldano**
> Ãˆ interessante questa tua cosa. Ãˆ come dire che si passa dalla generazione come una scultura a... partiamo da dei blocchi Lego e con questi facciamo l'immagine se vuoi.

**Paolo Antinori**
> Infatti mi sono ritrovato a pensare che la spiegazione di oggi che hai fatto, quella sul discorso semantico, Ã¨ assolutamente piÃ¹ ragionevole. SarÃ  che il mio cervello ragiona piÃ¹ facilmente in termini di parole e di testo, dico: ok, certo, sarebbe sempre dovuto essere cosÃ¬, invece no, non lo era. Ci stiamo arrivando solo adesso ed Ã¨ affascinante la scoperta di queste cose.

**Alessio Soldano**
> In realtÃ  c'Ã¨ un periodo in cui era cosÃ¬. Adesso dovrei approfondire di piÃ¹ perÃ² se capisco bene i modelli DALL-E e i primi modelli di generazione di immagini di ChatGPT funzionavano in questo modo. Poi sono stati abbandonati e si Ã¨ passati alle cose a cui siamo abituati ora semplicemente perchÃ© la qualitÃ  delle immagini che generavano non era sufficiente e soprattutto non erano stabili, c'era tantissima variabilitÃ  in quello che veniva generato a partire da testo simile da cui Stable Diffusion eccetera. PerchÃ© ci torniamo adesso? Mi verrebbe da dire perchÃ© gli LLM sono migliori e perchÃ© con tutto quello che abbiamo capito nel frattempo possiamo fare un lavoro migliore di quello che faceva DALL-E.

**Paolo Antinori**
> Questo dettaglio che hai dato, ovvero di storicamente in che ordine sono andate queste scoperte e queste esplorazioni, mi ricorda tantissimo il dualismo tra AI simbolica e AI non simbolica. Abbiamo iniziato con l'AI simbolica, non andavamo da nessuna parte, siamo andati in quella non simbolica che ha dei grossi risultati, poi perÃ² ci Ã¨ venuto in mente che forse ci fa comodo accoppiarla di nuovo con parte di quella simbolica per fargli i guardrail e tutta quell'altra serie di storie che la tengono centrata.

**Stefano Maestri**
> E non solo, sapete che io tengo d'occhio costantemente arXiv per capire dove va la ricerca. C'Ã¨ un trend nella ricerca in questo momento di provare a usare i modelli LLM con dei metalinguaggi da loro inventati, piÃ¹ efficienti dell'inglese per la comunicazione e soprattutto per i token di pensiero, che se vuoi Ã¨ un linguaggio di programmazione in qualche modo perchÃ© diventano dei linguaggi piÃ¹ formali evidentemente della lingua inglese che hanno meno ambiguitÃ  e che danno risultati nella parte di thinking piuttosto importanti. Ovviamente tutte queste ricerche quando arrivano sono fatte con modelli un po' vecchi ormai rispetto a quello che abbiamo oggi perchÃ© il tempo della ricerca non Ã¨ il tempo dell'industria. PerÃ² in sÃ© la ricerca Ã¨ interessante. L'altro trend di investimento Ã¨ l'explainability di cui tra l'altro parlo con un ospite in intervista â€” spoiler di chi Ã¨ ma ascoltatela mercoledÃ¬ prossimo. PerÃ² l'altro trend Ã¨ l'explainability e ovviamente queste due cose vanno in contrasto perchÃ© se li fai ragionare in inglese puoi leggere il loro ragionamento, se li fai ragionare in aramaico antico fai un po' piÃ¹ fatica. GiÃ  fai fatica leggendo in inglese.

**Paolo Antinori**
> Ci sta questo che dice ed Ã¨ affascinante e mi ricorda due cose. Quel vecchio demo di qualche tempo fa in cui avevano fatto vedere due device controllati da agenti fisici tipo due Alexa che scoprivano essere due agenti e quando si rendevano conto di essere due agenti iniziavano a mandarsi suoni come il modem. Dicevano: beh, tanto siamo due robot ed Ã¨ piÃ¹ efficiente cosÃ¬, e quello era affascinante. E poi un altro articolo in cui qualcuno faceva un'analisi di quali siano i linguaggi naturali piÃ¹ friendly per gli LLM in termini di risparmio di token per via di come si costruiscono quei linguaggi, e la ricerca diceva che il polacco Ã¨ la lingua piÃ¹ friendly per gli LLM. Non so perchÃ©, lo trovo comunque buffo che ci sia questo discorso di statistica di quale funziona meglio a livello di economia.

**Stefano Maestri**
> Io intanto vi faccio vedere tornando all'argomento di prima che cosa ha fatto GLM che Ã¨ disponibile in questi giorni su Hugging Face. Gli ho chiesto di creare un'immagine di un "young boy playing soccer with a red ball on snow" e questo Ã¨ il risultato. A me colpisce abbastanza l'effetto della neve qui che Ã¨ molto realistico secondo me. La posizione della gamba destra Ã¨ un po' innaturale perÃ² vabbÃ¨... ci sono i pantaloni che ingannano un po'. PerÃ² la prima impressione Ã¨ notevole.

**Alessio Soldano**
> SÃ¬ sembra che ci sia una scarpa sinistra sul piede destro. Allora io non ho provato ancora perÃ² quello su cui dovrebbe dare il meglio Ã¨ prompt in cui tu chiedi proprio cose specifiche del tipo come Ã¨ composta l'immagine, come interagiscono le cose eccetera. Bisogna mettersi lÃ¬ un attimo con calma a studiare.

**Stefano Maestri**
> Bisogna mettersi lÃ¬ e fare non quel prompt dal cavolo che ho fatto io. Vediamo se c'Ã¨ un prompt da eseguire...

**Alessio Soldano**
> Gli altri modelli magari fanno casino, per capirci.

**Paolo Antinori**
> Mi avete dato un'idea ragazzi che magari puÃ² diventare anche un progetto. Stai praticamente suggerendo che potrebbe essere utile avere una libreria di supporto al prompting simile a quello che Ã¨ SuperClaude apposta per le immagini realistiche o a cartoni.

**Alessio Soldano**
> Ma in realtÃ  ce n'Ã¨ di progetti cosÃ¬ di enhancing di prompt per immagini, anche perchÃ© non Ã¨ solo enhancing ma Ã¨ anche styling. Per cui a partire da una descrizione generica tu la puoi arricchire da un punto di vista di testo, perchÃ© poi le nuances del testo fanno tutta la differenza del mondo e a seconda dei termini che usi riesci a trasferire una sensazione, un mood differente nell'immagine: toni dei colori, queste cose qua che ti portano a immagini completamente differenti. Io ho un paio di meta-prompt che tu dai a ChatGPT e dici: va bene ho questo prompt oppure ho questa immagine, rendimela piÃ¹ fotorealistica con tutta una serie di descrizioni di cosa vuol dire per te fotorealistico, e riscrivimi il prompt. E poi quel prompt lo riprovi a generare. Oppure: rendimi l'immagine come se fosse scattata da una macchina fotografica a pellicola 35 mm con imperfezioni, graffi o roba di questo genere.

**Paolo Antinori**
> Figo, non sapevo che ci fosse, ha completamente senso. PerÃ² hai spinto il mio cervello nell'altro collegamento logico: c'Ã¨ spazio per gli MCP server nella generazione delle immagini? E se sÃ¬, per fare che cosa? Per scriptare Photoshop o non lo so per cosa si potrebbero usare.

**Alessio Soldano**
> Sia per queste cose dei prompt, sia per offrire migliorie specifiche di immagini, alla fine avere giÃ  delle ricette fatte esposte da un server.

**Paolo Antinori**
> Quindi perÃ² per la parte di prompting del protocollo di MCP non per la parte di operazioni.

**Alessio Soldano**
> Ma anche per la parte di operazioni, cioÃ¨ l'MCP server che ripulisce l'immagine perchÃ© diventi adatta a farci i passaporti per dire. O che ti ridimensiona le immagini perchÃ© siano postabili su Twitter piuttosto che su Instagram.

**Paolo Antinori**
> O quello che ti mette in bikini.

**Alessio Soldano**
> O quello che ti mette in bikini. PerÃ² tornando al discorso delle immagini postabili su una certa piattaforma, uno dice "vabbÃ¨ le riquadro devono essere quadrate", ma non Ã¨ sempre solo una questione di cropping. Con l'image-to-image puoi dirgli "tieni piÃ¹ o meno la stessa foto perÃ² deve essere quadrata" e allora magari scopri che se Ã¨ un soggetto umano cambia leggermente la posizione per essere in modo tale che ci stia, magari accovacciato invece che in piedi.

**Paolo Antinori**
> Grazie, Ã¨ affascinante, ha senso quello che mi racconti.

**Stefano Maestri**
> Intanto io ho preso uno dei loro prompt di esempio e noto che il testo non Ã¨ perfetto come sostengono loro. Poi hanno voluto fare gli sboroni, hanno messo la parola "Raspberry" che Ã¨ scritta correttamente, che era una delle cose difficili per molti LLM. Comunque sÃ¬, le immagini sono molto belle. Ãˆ piÃ¹ affascinante forse del singolo dettaglio la spiegazione che ne hai fatto prima. Io e Paolo siamo al livello per cui se prendo questo o prendo un Nano Banana dico: boh le immagini sono belle, il mio livello di gusto non Ã¨ abbastanza raffinato per apprezzare la differenza.

**Alessio Soldano**
> SÃ¬, ma di nuovo qui il focus non Ã¨ il gusto o l'estetica, a loro interessa piÃ¹ la semantica dell'immagine. Infatti nel lancio dicono che hanno una qualitÃ  visiva comparabile agli approcci di diffusion attuali, non si sbilanciano piÃ¹ di tanto. Z Image sicuramente fa meglio, perÃ² il controllo sulla composizione e sul significato dovrebbe essere molto migliore. Ricordiamoci che parliamo di un modello open weight, quindi il punto di riferimento Ã¨ Quen o Flux 2. Quando vai a vedere Nano Banana o Grok, chissÃ  cosa fanno loro.

**Stefano Maestri**
> L'impressione Ã¨ che in Nano Banana Pro ci sia tanto preprocessing e post processing abbondante. Abbiamo visto che Quen Ã¨ uscito con quello che fa le immagini a layer multipli, ma io non metterei la mano sul fuoco che Nano Banana sotto non faccia questa cosa e che il testo lo generi con un modello in un layer che poi sovrappone tutto il resto.

**Paolo Antinori**
> Stiamo aspettando che il nostro amico Karpathy si annoi e faccia il suo progettino su GitHub dove fa parlare tra di loro 10 modelli visivi.

**Stefano Maestri**
> Si puÃ² darsi. Il progettino era talmente semplice, potremmo metterci pure noi. Io con le immagini no, sono veramente una capra.

**Alessio Soldano**
> E parlando di immagini e video open weight di questi giorni c'Ã¨ un'altra news interessante che Ã¨ LTX 2, che Ã¨ un modello di generazione video. Ad oggi Ã¨ il miglior modello open weight per generazione di video con audio: mette assieme quello che fa meglio l'audio, quello che fa meglio il video, quello che ti fa fare l'editing. Sostanzialmente fa video di qualitÃ  come One 2.2 ma Ã¨ tipo 18 volte piÃ¹ veloce. Siamo arrivati a clip da 20 secondi che sono piuttosto lunghe. Ti semplifica la vita non poco. E poi uno dei problemi Ã¨ che il video tende ad andare alla deriva verso la fine della clip, si perde consistenza. Se puoi fare video da 20 secondi, magari ti fermi a 15 e hai consistenza migliore di quella che avresti avuto prima con un modello da 10 secondi.

**Paolo Antinori**
> Mi hai dato in mente un soggetto per un film di fantascienza: un'astronave visita i pianeti alla Star Trek e ne visita uno in cui vedono un sacco di comunicazioni, parlano con gli abitanti, ma in realtÃ  non c'Ã¨ niente, Ã¨ soltanto un mega LLM e gli umani sono morti e lui genera video e parla al presidente e tu solo alla fine scopri che era tutto virtuale.

## [37:02] AI in Healthcare: Opportunities and Ethical Concerns

**Stefano Maestri**
> Molto bello. Parliamo di un trend che va al di lÃ  della notizia in sÃ©. OpenAI Ã¨ uscita con ChatGPT Health per la salute: un agente specializzato in cui c'Ã¨ un modello e tutto quello che ci gira attorno per supporto sia agli utenti che ai medici per problemi relativi alla salute, in cui si possono caricare cartelle cliniche per ottenere consigli medici oppure per supportare il medico che deve fare la diagnosi. PerchÃ© dico che Ã¨ un trend? PerchÃ© Anthropic sta lavorando a Claude Salute e Google ha rilasciato Gemma fine-tuned proprio per la salute. Ãˆ uno dei business individuati. GiÃ  quello di ChatGPT non Ã¨ disponibile in Europa per via della legislazione piÃ¹ stringente sulla privacy, ma i legali di OpenAI ci stanno lavorando.

**Alessio Soldano**
> Secondo me se davvero il problema Ã¨ solo la privacy lo risolvono perchÃ© basta avere una serie di rassicurazioni sul trattamento dei dati e secondo me lo aggirano.

**Stefano Maestri**
> SÃ¬ Ã¨ solo che la privacy nel medical in Europa Ã¨ piÃ¹ stringente della privacy normale. Ma loro sono assolutamente fiduciosi. Avete un'opinione sull'utilizzo dei modelli LLM in aiuto al medico o per consigli privati di automedicazione?

**Paolo Antinori**
> Io sono assolutamente a favore, al punto che se mi chiedete come mai i dati medici hanno una privacy piÃ¹ sensibile di altri non so dare un'informazione ragionevole. Che ve ne frega di sapere qual Ã¨ la mia pressione sanguigna o quanto peso? Sul discorso di non concedere di usarli ai professionisti perchÃ© rischiano di sbagliare... allora gli togliamo anche internet e i libri e gli lasciamo la penna. Potrebbero sbagliare comunque.

**Stefano Maestri**
> E poi potrebbero sbagliare comunque... anzi...

**Alessio Soldano**
> Io penso che ce ne sarebbe tanto bisogno perchÃ© i medici sono oberati di lavoro. Ogni aiuto in quella direzione Ã¨ utile. Penso che il problema di base sia lo stesso della guida autonoma: il discorso della responsabilitÃ  nel caso poi qualcosa non vada come dovrebbe. Un medico piÃ¹ AI fa meglio del medico da solo. Ma se c'Ã¨ un errore di chi Ã¨ la colpa? Ãˆ colpa di chi ha scritto il software, di chi non ha vigilato? Bisogna trovare dei paletti.

## [46:15] User Responsibility in AI Healthcare Decisions

**Paolo Antinori**
> Io sono flessibile anche su quel discorso etico lÃ¬. Se il chirurgo sbaglia Ã¨ colpa sua. Ma se non c'era il chirurgo tu eri capace di usare il bisturi? PerchÃ© incolpi quel povero Cristo che non si Ã¨ tirato indietro? Un conto se c'Ã¨ ovvia negligenza, ma se doveva lanciare testa o croce, povero Cristo, non Ã¨ colpa sua.

**Stefano Maestri**
> Leggevo la newsletter di Matteo Roversi che diceva che "agency" (responsabilitÃ ) Ã¨ la parola piÃ¹ usata in Silicon Valley. Il problema Ã¨ dove metti la responsabilitÃ . Se lo vedi come tool, Ã¨ chiaro che la responsabilitÃ  Ã¨ tua. Ãˆ nel momento in cui lo antropomorfizzi troppo che non Ã¨ piÃ¹ un tool ma un'entitÃ  terza. In quanto sistema stocastico Ã¨ un tool utilizzato da qualcuno che si prende la responsabilitÃ . Diverso Ã¨ nel momento in cui ChatGPT Health Ã¨ usato dall'utente finale. Non conosco nessuno oggi che abbia fatto causa a Google perchÃ© si Ã¨ automedicato e si Ã¨ fatto del male. PerchÃ© dovremmo farlo con l'AI? Alla fine la responsabilitÃ  Ã¨ tua che prendi la decisione di prendere la medicina.

## [47:38] The Risks of Anthropomorphizing AI

**Alessio Soldano**
> Quindi tu mi stai dicendo che questa cosa se ne esce con un opportuno prompt engineering per far sÃ¬ che la versione Health sia ancora piÃ¹ cauta nel dirti: guarda ti consiglio di fare cosÃ¬ perÃ² tieni conto che potrei commettere errori, parla comunque con un medico.

**Stefano Maestri**
> Immagino di sÃ¬ e immagino anche che nelle spunte che metterai ci sia scritto tutto questo. Capisco il rischio perchÃ© chi ci parla ha la sensazione di qualcosa di piÃ¹ di Google perchÃ© parla la tua lingua, genera la risposta. Quindi capisco il rischio di antropomorfizzarla, perÃ² alla fine la responsabilitÃ  Ã¨ di chi fa l'azione.

## [50:02] The Future of AI in Personal Health

**Stefano Maestri**
> Se volete fare un fast forward a 10-20 anni, quando ci sarÃ  la robot badante a mettere la medicina sul tavolo, allora i confini saranno sempre piÃ¹ sfumati. Sul medico io non vedo il problema: lÃ¬ c'Ã¨ il professionista che decide di usare uno strumento. Quando Ã¨ l'utente finale ad usarlo i confini sono sfumati.

**Alessio Soldano**
> Al tempo stesso, magari tu prima leggevi i bugiardini o chiedevi al tuo amico, cercavi su internet e decidevi di prendere tre pastiglie e poi stavi male.

**Stefano Maestri**
> Esatto, nessuno ha fatto causa a Google perchÃ© ha cercato "mi fa male il piede" e ha preso il primo link che magari era una cagata.

**Paolo Antinori**
> Il primo ChatGPT in cui ci consultiamo per la salute Ã¨ nostra madre. Tua madre ti dice di prendere una pastiglia e tu lo fai.

**Stefano Maestri**
> PerÃ² lÃ¬ qualcuno si prende la responsabilitÃ . Nel momento in cui lo utilizzi, la responsabilitÃ  Ã¨ dell'utente finale. PiÃ¹ i tool sono avanzati, piÃ¹ sembrano umani nella conversazione.

**Alessio Soldano**
> Io invece pensavo alla possibilitÃ  di avere qualcosa disposto in ogni caso ad ascoltare i tuoi dubbi, mentre l'umano magari ha fretta o non ti dÃ  retta.

**Stefano Maestri**
> Sfondi una porta aperta, l'ho giÃ  usato per farmi domande sulla mia salute. Il problema Ã¨ di chi c'Ã¨ dall'altra parte, non Ã¨ soltanto dello strumento.

**Alessio Soldano**
> Recentemente parlavo con un'amica ingegnere informatico che l'ha usato tanto per una situazione critica di un familiare e poi chiedeva conferma a mia moglie che ha un background sanitario. Le conferme sono arrivate perchÃ© era corretto quello che aveva capito. PuÃ² essere che l'utente meno esperto si faccia prendere la mano.

**Stefano Maestri**
> Ãˆ che non tutti sono ingegneri informatici. Sicuramente un tema divisivo.

**Alessio Soldano**
> L'AI Health dovrÃ  preparare l'utente al fatto che potrebbe ricevere informazioni che una persona umana non gli avrebbe dato per non spaventarlo. C'Ã¨ quella parte di intelligenza emotiva che il medico ha e che magari nell'AI potrebbe non esserci.

## [52:49] Debating AI Memory Systems

**Stefano Maestri**
> Argomenti forti e divisivi. Passo ad avere un'opinione forte per il prossimo argomento: da qualche settimana la mia headline su LinkedIn Ã¨ "I have no problem with strong opinions" (grazie ad Andrea Saltarello). Alessio ci ha messo in scaletta dei progetti sulla memoria degli agenti di coding: Simplemem e Claudemem. Ho parlato a lungo di memoria come "the next big thing". Questi progetti fanno la memoria con un MCP server, un database relazionale e Chroma DB per gli embedding per matchare e cercare la memoria. Ma se guardo le skill di Claude (agentskills.io) Ã¨ tutto molto piÃ¹ semplice: un file Markdown in una directory. C'Ã¨ un tool che legge le skill e le mette in una piccola area di memoria non indicizzata. Quando Claude fa qualcosa si interroga se una skill Ã¨ rilevante e carica il Markdown nel contesto. Niente di esotico. Chi Ã¨ vicino a quello sviluppo sta semplificando tantissimo. Chi ci si approccia con piglio ingegneristico tende a progettare database e dipendenze. Ma forse i modelli sono piÃ¹ intelligenti di cosÃ¬: stiamo usando un bazooka per sparare a una mosca.

## [01:04:58] The Complexity of AI Memory and Learning

**Paolo Antinori**
> Non sono cosÃ¬ drastico. Ci sono due aspetti: la scalabilitÃ  (non so se mangia il contesto) e l'auto-aggiornamento. Le skill Markdown sono statiche finchÃ© non le modifichi tu. Invece un sistema di memoria esterno presuppone che possa non essere statico, aiutando a mantenerli aggiornati. Se io non sono attivo a dire alla skill "aggiungi questo", lei non lo fa. Se la skill sbaglia tre volte di seguito a mettere la password, non gli viene in mente di segnarselo. Questa parte rimane all'umano o a un terzo attore indipendente che controlla le sessioni e si accorge quando la skill deve venire migliorata. La soluzione non Ã¨ ancora ovvia.

**Stefano Maestri**
> L'idea di Anthropic Ã¨ un marketplace delle skill come se fossero pacchetti da tenere in locale. C'Ã¨ chi ha fatto del sistema multi-agente la ragione di vita (Manus) e chi invece dice che l'agente Ã¨ quella roba minima capace di interagire con l'ambiente (Claude Code) a cui aggiungi skill o MCP server. Noi ingegneri cerchiamo di continuare a pensare come facevamo prima, ma gli LLM sono piÃ¹ intelligenti di quello che ci aspettavamo. Dovremmo concentrarci piÃ¹ sul ragionare sul contesto che sull'architettura. Bisogna chiedersi cosa deve memorizzare l'agente: dati o nuove skill? Esperienza o continuous learning?

**Paolo Antinori**
> Espandere la memoria Ã¨ un passo verso la superintelligence. Continuare a migliorare le proprie skill Ã¨ un passo verso la general intelligence. Un essere umano impara dalla propria esperienza. Se fossi in grado di chiedere al sistema cosa ho mangiato in viaggio di nozze perchÃ© puÃ² essere utile per la salute e io non me lo ricordo, quella Ã¨ una superintelligence.

**Stefano Maestri**
> Sono d'accordo. Ma per un agente di coding ricordarmi com'era il codice prima? Ho Git per quello. Io sono convinto che serva la memoria, ma non so se serva una memoria semantica o procedurale. Mi verrebbe da dire procedurale: imparo a fare cose invece di ricordarmi i token che sono passati.

**Alessio Soldano**
> BisognerÃ  fare delle prove. Lo guarderÃ² bene anch'io.

**Stefano Maestri**
> Bene, siamo arrivati a target. Paolo non lo vediamo piÃ¹, fa anche brutto avere questa fetta nera. Ringraziamo tutti quelli che sono arrivati fin qui. Mettete stelline, campanelline e like. Metteteci dei commenti, abbiamo espresso tante opinioni forti e divisive e ci piacerebbe avere feedback per tornarci la prossima volta. Ciao a tutti!

**Alessio Soldano**
> Ciao ciao, alla prossima.