---
title: "L'AI scala davvero? E poi coding creativo, Gemini ovunque e ChatGPT introduce la pubblicit√† #36"
categories:
  - Puntate
tags:

  - AI
  - Pubblicit√†
  - Vibe coding
layout: single
author_profile: true
---

{% include video id="7Sc5PLAV-nw" provider="youtube" %}

üëâ [Ascolta su Spotify](https://open.spotify.com/show/16dTKEEtKkIzhr1JJNMmSF?si=900902f2dca8442e)<br/>
üëâ [Guarda su YouTube](https://www.youtube.com/channel/UCYQgzIby7QHkXBonTWk-2Fg)<br/>
üëâ [Segui su LinkedIn](https://www.linkedin.com/company/risorseartificiali)<br/>



## **Introduzione e Saluti**

**Stefano Maestri**

Buongiorno a tutti. Ci vedete con dei sorrisi ebeti tutti quanti, se state guardando, perch√© abbiamo appena... No, Paolo, no, Paolo √® serio, eh, perch√© abbiamo appena finito di ridere nella preparazione della puntata e voi non saprete mai bene di che cosa, ma questo fa parte del gioco. E comunque questo √® il secondo tentativo di registrazione dell'inizio della puntata, perch√© prima sono scoppiato a ridere che mi √® venuto in mente quello di cui stavamo parlando.

Per√≤ veniamo a voi ascoltatori. Intanto grazie, grazie che ci ascoltate e anche sempre pi√π numerosi, devo dire; siamo molto contenti delle ultime puntate. Diciamo un paio di cose ai nuovi ascoltatori, visto che pare che ne abbiamo parecchi. Allora, magari andare a recuperare puntate vecchie sarebbe cosa buona e giusta da parte vostra, ma magari quelle sulle news possono sembrare poco rilevanti perch√© qua le cose cambiano tutti i giorni praticamente. Per√≤ facciamo anche delle interviste ogni 15 giorni: quelle invecchiano poco perch√© sono la visione della persona intervistata intorno al mondo dell'AI. L'ultima uscita mercoled√¨, tra l'altro, molto verticale, molto tecnica sui modelli. Se siete interessati a questa cosa, la persona invitata che √® Emanuele Fabiani √® super super preparato e ne ha discusso un po' con me, per cui se avete voglia andate a sentirvelo, ma ci sono anche tutte le altre che noi ci sentiamo di consigliare. Il prezzo da pagare sono stelline, campanelline e dirlo ad amici e parenti.

**Alessio Soldano**

Dovremmo fare e lasciare dei commenti, dico dei commenti con le vostre curiosit√†, anche sul perch√© ridevamo.

**Paolo Antinori**

Magari siete coraggiosi a chiedere a internet di lasciarvi dei commenti. Ragazzi, non si vede che non passate mai da YouTube voi. Eh, comunque dovremmo fare una puntata montaggio, sai? Tipo Friends, che ogni tanto ti fanno vedere gli spezzoni delle parti migliori. Devi trovare un modello che lo faccia per noi perch√© non c'ho cazzo di farlo io.

**Stefano Maestri**

Ma io l'avevo proposta come puntata di Natale, mi avete detto se ero scemo o cosa e invece improvvisamente abbiamo cambiato idea perch√© abbiamo pi√π ascoltatori, quindi cominciamo a tirarcela. √à quella la verit√†, no?

**Paolo Antinori**

Io sono stato preso in giro dalla famiglia al tavolo del Natale. I miei cugini universitari mi hanno assolutamente preso in giro chiedendomi se ero un influencer e quelle cose l√¨. Quindi spero che voi cugini universitari ci ascoltiate, per√≤ hanno messo stelline, campanelline, senn√≤ non possono prendermi in giro.

**Stefano Maestri**

√à una domanda interessante.

**Paolo Antinori**

I miei cugini universitari mi hanno raccontato che LinkedIn ha dei giochi e io neanche lo sapevo. Quindi vedi tu che livello...

**Stefano Maestri**

A me lo pubblicizza LinkedIn perch√© molti dei nostri colleghi sembra che passino il tempo a fare questi giochi, perch√© mi dice "Ah, ma sai che tot tuoi colleghi hanno fatto questo gioco?". Io penso "E vabb√®, non c'√® niente da fare?". Eh vabb√®, comunque...

**Alessio Soldano**

Io lo scopro adesso, invece. Boomer\!

## **Riflessioni sull'Intelligenza Artificiale**

**Stefano Maestri**

Allora, no, partiamo da una cosa cos√¨ veloce che ho sentito anche in un podcast stamattina mentre passeggiavo. La capacit√† delle AI che comunque continua a pi√π che raddoppiare: l'articolo che ho qua davanti dice 7 mesi, il podcast diceva addirittura quattro. Quindi legge di Moore, se vogliamo... si dice, no, che la legge di Moore era per i processori, capacit√† di calcolo dei processori. In realt√† dipende come la guardi. Quella dei 4 mesi che citavano, l'ho letta anch'io, √® raddoppiare ogni 4 mesi la potenza dell'AI rispetto a certi benchmark versus costo. Quindi a parit√† di costo ogni 4 mesi ho una potenza doppia, che fa un po' il paio con quella cosa che dicevamo, no? Che dieci volte meno costano i token. Se guardiamo fino a gennaio 2025, guardando i costi sia di OpenAI che di Anthropic, circa 10 volte meno il prezzo del token a parit√† di potenza sui modelli che c'erano gi√†, diciamo.

**Alessio Soldano**

Scusa Stefano, chiariamo che cosa intendi quando dici la potenza in questo caso? Cos'√® che migliora?

**Stefano Maestri**

Il miglioramento di risposta sui cinque benchmark principali. Ok? Il miglioramento sui cinque benchmark principali costa, a parit√† del raggiungimento di quei livelli di benchmark, costa la met√† ogni 4 mesi, quindi 10 volte meno circa in un anno. Che vuol dire? Se proietto questi numeri... e questa valutazione... cambiano i benchmark in realt√† perch√© gli LLM hanno benchmark diversi, ma nell'articolo che ho letto un paio di settimane fa pi√π o meno √® confermata fin dal 2012 guardando i sistemi di deep learning, ovviamente non gli LLM che non c'erano ancora. Che √® interessante per√≤, perch√© per 10 all'anno vuol dire per 1000 in 3 anni.

**Alessio Soldano**

E queste migliorie sono considerando sia la parte algoritmica che la parte, diciamo, di hardware che migliora?

**Stefano Maestri**

S√¨, s√¨, s√¨. La risposta finale, s√¨. Quindi c'√® il miglioramento dell'hardware che pi√π o meno leggevo che, se √® vero che sulle CPU la legge di Moore non vale pi√π (cio√® il miglioramento ogni 18 mesi, doppio della performance a parit√† di prezzo), in realt√† √® rimasta valida, anzi √® leggermente meglio intorno ai 14 mesi sulle GPU. Quindi la legge di Moore sull'hardware pi√π o meno resta costante e questa cosa qua, la scalabilit√†, che poi √® parte integrante della legge di scalabilit√† di Amodei, va in questa direzione ed √® comunque... significa che i miglioramenti al momento sono ancora super significativi.

## **Scalabilit√† e Investimenti nell'AI**

**Paolo Antinori**

Io ho sentito dei commenti interessanti su questi temi, su come parte del successo e dell'accelerazione dell'AI sia attribuibile proprio a questo aspetto, ovvero √® facile andare da un executive e spiegargli: "Pi√π soldi ci metti, pi√π veloce ci puoi andare perch√© hai comprato pi√π cose, pi√π macchine". Cio√® la abilit√† di scalare √®, se non lineare, quantomeno facilmente prevedibile. Quindi le persone capiscono che pi√π investono pi√π ottengono, ed √® il motivo per cui si √® accelerato. C'√® la speculazione che questa curva si abbasser√† e a quel punto potrebbero cambiare le regole del gioco, diciamo.

E poi di contro qualcuno osservava come l'apporto e il motivo per cui sono interessanti le esplorazioni open source nell'ambito delle AI, in particolare quelle asiatiche, √® proprio perch√© sembrerebbe che di l√† ci sia stata meno disponibilit√† di poter bruciare soldi in questa maniera e quindi bisognava essere un po' pi√π intelligenti a decidere come far ottenere delle performance senza metterci dentro i soldi. Si sono concentrati pi√π a spaccare il bit, ad avere delle idee intelligenti, a unire livelli, fare tutte le cose interessanti, l'innovazione che ci hanno portato. Quindi entrambi i lati della medaglia, secondo me, sono stati utili a contribuire a questa accelerazione finale.

## **Pre-Training e Consumi Energetici**

**Stefano Maestri**

S√¨, s√¨, s√¨. E poi un'altra cosa limitrofa che leggevo in settimana che fa il paio con quell'intervista di Karpathy che abbiamo citato qualche settimana fa, l'intervista che √® fatta da Dwarkesh Podcast, che appunto √® pi√π un'intervista sulla sua visione dal mondo anche della biologia eccetera eccetera. Per√≤ lui diceva una cosa... loro, anzi, perch√© in realt√† √® Dwarkesh che lo porta a questo ragionamento. Loro facevano un ragionamento interessante su come il pre-training (quindi lo ricordiamo un attimo agli ascoltatori: la parte in cui si prende un volume enorme di dati, lo si d√† a un modello vergine in qualche modo e questo impara i pattern), la fase di pre-training assomigli di pi√π in qualche modo a una compressione esagerata dell'evoluzione. Ok?

Quindi il modello arriva con il pre-training e sa fare delle cose ben pi√π che di base, sa fare delle cose che... anche se poi Karpathy dice che il paragone con il mondo animale √® impossibile eccetera eccetera, ma teniamolo un attimo per spiegazione. Arriva con questo pre-training e sa gi√† fare delle cose. Poi c'√® tutta la parte di reinforcement learning agentica, di si guarda avanti verso la memoria procedurale piuttosto che memoria storica per migliorare durante il tempo, che √® un po' l'apprendimento dell'uomo, no?

E per√≤ leggevo questa roba qui dei dati dei consumi invece dell'AI. C'era un articolo provocatorio che diceva: "Ma noi facciamo molto di pi√π con 20 Watt", stimato che il cervello umano consumi una ventina di watt. Eh, per√≤ ni, nel senso perch√© se guardiamo quanto costa un'inferenza in termini di Watt a singolo utente... a singolo utente, perch√© noi stiamo ragionando di un cervello centralizzato che serve mi pare milioni di utenti, dagli ultimi dati di OpenAI un miliardo forse di utenti attivi. S√¨, un miliardo di utenti attivi, non mi ricordo quanti contemporaneamente di media, e quindi dovremmo dividere per il numero di utenti. Sull'inferenza non siamo lontani da quel numero l√¨. Come wattora costa tantissimo la parte di training, per√≤ se la vediamo come l'evoluzione, quanto √® costata in questi milioni di anni tutti i cervelli che sono evoluti per arrivare dove siamo oggi? Non lo so, √® volutamente un po' provocatoria. Forse qualcuno dei nostri amici che si occupa di pi√π di sostenibilit√† legata all'informatica... gli stanno venendo i capelli bianchi solo per questa mia affermazione.

**Alessio Soldano**

A me comunque fa abbastanza strano che qualcuno abbia, come dire, investito del tempo nel fare questo parallelismo, come a giustificare il fatto che stiamo spendendo...

**Stefano Maestri**

No, il parallelismo lo sto facendo io adesso, eh, se ti faccio strano. Vabb√®, mi spiace. No, il parallelismo lo sto facendo adesso sull'evoluzione mettendo insieme i pezzi di quello che diceva Karpathy che dice "ma il pre-training in realt√† √® pi√π simile all'evoluzione" e leggevo questo articolo del consumo del cervello umano che diceva "eh ma consuma pochissimo rispetto a quello che consumano i modelli". E poi in realt√† nell'articolo stesso si citava il consumo dell'inferenza che in realt√† non √® cos√¨ alto. E son tornato invece al pre-training che √® effettivamente estremamente costoso da un punto di vista energetico, spaventosamente costoso da un punto di vista energetico. Per√≤ anche il nostro di pre-training √® stato estremamente costoso da un punto di vista energetico e non solo energetico. Forse cos√¨ partivo per la tangente sperando di triggerare qualche hater che a me ha gi√† triggerato.

## **Innovazioni nei Modelli AI**

**Stefano Maestri**

No, modelli, modelli, modelli, modelli cinesi. Dico io una cosa che ci riporta un attimo ai modelli immagini, ai modelli sulle immagini, cos√¨ poi ci sono un po' di novit√† che Alessio ci racconta. Abbiamo parlato di GLM Image la volta scorsa; non abbiamo detto una cosa abbastanza significativa che era un fortissimo rumor allora. Adesso √® stato confermato direttamente da Zhipu AI, cio√® l'azienda che sta dietro GLM, che √® il primo modello di cui hanno fatto il training completamente su uno stack Huawei e non Nvidia. Che si riaggancia anche a quello che diceva Paolo, no? Che hanno dovuto usare un po' pi√π di intelligenza avendo chip meno performanti.

In questi giorni poi c'√® tutta una polemica anche di Amodei che dice che dare gli Nvidia in Cina √® come dare i missili nucleari alla Corea del Nord, che √® un'affermazione un tantino forte. Per√≤ lui l'ha gi√† detta tante volte questa cosa: il fatto di tenere tutto negli Stati Uniti il pi√π possibile. Poi io non condivido il protezionismo in generale...

**Alessio Soldano**

Neanch'io, anche perch√© a quel punto per√≤ interessante se facessimo anche il duale, il contrario. Cio√® pensa quanta tecnologia cinese o gi√π di l√¨ usiamo. Immaginiamo di fare senza...

**Paolo Antinori**

Degli ESP32 stai parlando? Io s√¨, non ce la farei senza gli ESP32.

**Stefano Maestri**

No, √® vero. Ho tutta la parte robotica in arrivo. No, per√≤ √® interessante questa cosa qua: cio√® primo training grosso fatto su hardware completamente cinese e tra l'altro Huawei, cio√® il mostro a due teste visto dall'America che ha persino tagliato Android su Huawei.

## **Hardware e Comoditizzazione**

**Paolo Antinori**

Pi√π che quello, io la leggo come una sorta di storia di comoditizzazione dell'hardware per fare training. Nel senso cos√¨ come un tempo... cio√® storia dell'informatica: un tempo il computer te lo vendeva soltanto IBM o chi li costruiva e quindi controllavano il monopolio, poi pian pianino ci sono stati i cloni ed √® diventato pi√π alla portata di tutti. E Google stessa, neanche negli ultimi anni, oramai saranno stati 30 anni fa, ha rivoluzionato dicendo: "Noi abbiamo... ci siamo resi conto che √® pi√π conveniente avere degli hard disk che sappiamo che se ne rompono 100 al giorno e cambiarli quando si rompono, che non investire negli hard disk ad alta resistenza che non si rompono".

Mi sembra un passaggio in questa direzione. Nel senso: quest'anno fanno la roba con Huawei, tra qualche anno qualcuno ci dir√† che ha comprato una vagonata di iPhone o di Mac Mini e ci hanno fatto un cluster di training. Come quando qualche tempo fa qualcuno faceva sta roba con la PlayStation 5, vi ricordate? O la quattro. C'era il ban e qualcuno aveva scoperto che poteva usare quella per le GPU che c'erano dentro per farci fare altre cose. Cio√® √® l'ingenuity, la parola che mi fa sempre sorridere perch√© in italiano suggerisce qualcosa di diverso, ma in realt√† √® l'inventiva. L'inventiva √®... c'√® un modo di dire italiano che dice "la mamma della capacit√† di inventare"...

**Stefano Maestri**

No, certo.

**Paolo Antinori**

S√¨, qualcosa del genere.

**Alessio Soldano**

Arrangiarsi con quello che si ha, mettiamola cos√¨.

**Stefano Maestri**

S√¨, s√¨, l'arte di arrangiarsi. Ecco, per√≤ quella roba l√¨ mi triggera invece un'altra cosa a me di cui non abbiamo forse mai parlato e che ovviamente non abbiamo in scaletta, ma mi √® venuto in mente adesso.

## **Distribuzione del Calcolo e Progetti Futuri**

**Stefano Maestri**

All'inizio del 2025 c'era stato un grandissimo esperimento a livello mondiale di training distribuito, cio√® un po' come il... cos'era? Il Seti@home, non so se lo ricordate. Siamo abbastanza vecchi tutti per ricordarci il Seti@home.

**Paolo Antinori**

Poi c'era quello successivo delle proteine, che era forse pi√π attuale.

**Stefano Maestri**

S√¨, spieghiamo a chi ha qualche anno meno di noi che cos'era. Era praticamente un progettino che ti scaricavi e ti assegnavano un chunk di informazione, un pezzo di informazione su cui fare dei calcoli molto complicati. L'obiettivo di Seti@home era vedere se c'erano segnali radio che potessero essere alieni o comunque significativi venuti dallo spazio.

**Paolo Antinori**

Praticamente Bitcoin, ma non ci guadagnavate niente.

**Alessio Soldano**

Esatto.

**Stefano Maestri**

Esatto. Poi Bitcoin in qualche modo √® vero, tutta la parte del fare attraverso i nodi... o tenerli proprio su, non soltanto fare mining, hai ragione. Avevano fatto un tentativo di distribuzione dell'hardware, l'avevano chiamato World Model, che poi adesso vuol dire un'altra cosa questo "World", tutt'altra cosa. E c'√® anche un modello trainato cos√¨ che si attestava pi√π o meno come un Meta piccolo della versione 3, un Llama piccolo, l'equivalente di un Llama 3 tipo a 20 billion, una cosa cos√¨. Per√≤ non ha preso questa cosa qui, cos√¨ come non hanno preso altri progetti di fine-tuning estremamente semplificati di quel periodo. Non so se non c'√® volont√† o se √® proprio una tecnologia che non si presta a questa cosa. Non ho un'opinione, non ho approfondito per saperlo, per√≤ lo segnalavo solo cos√¨ per dire che quel tentativo l√¨ di distribuire il calcolo e farlo diventare davvero una commodity al momento non √® andato benissimo. Poi le cose cambiano e chiss√†.

**Alessio Soldano**

C'√® sicuramente una componente algoritmica importante, cio√® capire quanto e come il problema pu√≤ essere spezzettato, perch√© non √® detto che si possa.

**Stefano Maestri**

S√¨, no, pensandoci √® un macello, perch√© comunque tutti i nodi sono tra loro connessi, fortemente connessi. Quindi quello che tu fai a casa tua influenza tantissimo il layer successivo e cos√¨ via. √à un problema complicato da spezzettare, anche se in realt√† gi√† si va in parallelo su GPU multiple, non √® che il calcolo viene fatto single thread, ovviamente.

**Alessio Soldano**

√à da capire poi anche il fatto di parallelizzare, come dire, rifasarsi tra un nodo e l'altro quanto ti costa.

**Stefano Maestri**

S√¨, probabilmente c'√® quella componente l√¨.

**Paolo Antinori**

Comunque, visto che hai citato Seti@home, non so se qualcuno di voi guarda Pluribus su Apple TV, serie di fantascienza. Le premesse sono praticamente quelle di Seti@home come inizia, quindi se siete curiosi per il vintage vi incoraggio a guardare quantomeno la prima puntata.

**Stefano Maestri**

Ok. Non c'ho Apple TV, ma ci attrezzeremo.

## **Ritorno di Flux e Black Forest Lab**

**Stefano Maestri**

Quindi vorrei sapere di Flux.

**Alessio Soldano**

Allora, mi verrebbe da dire che Black Forest Lab is back. Probabilmente vi ricordate qualche un mese fa, una cosa cos√¨ prima di Natale, c'era stato l'annuncio di Flux 2 e nel giro di un paio di giorni √® uscito Z-Image Turbo che praticamente ha spazzato via Flux 2 e la community se n'√® praticamente dimenticata. L'ha spazzato via perch√© faceva immagini uguali, anzi un po' pi√π belle, in una frazione del tempo che impiegava Flux 2\. Evidentemente quelli di Black Forest Lab hanno capito che l√¨ andava l'evoluzione e se ne sono usciti qualche giorno fa con una nuova versione di Flux 2 che si chiama "Kleine". Per chi sa il tedesco, chiaramente "Kleine" √® piccolo, che sono pensati proprio per fare concorrenza a questi di Z-Image e anche a Qwen-Edit 2512 che √® uscito nel frattempo, che era un'altra evoluzione dei modelli di Qwen di fine anno scorso che migliorava molto la qualit√†.

Quindi abbiamo questa nuova famiglia di modelli che sono sostanzialmente dei distillati a partire da Flux 2, quello che era stato rilasciato l'altro mese, ma pensati per essere pi√π veloci, pi√π piccoli e soprattutto per spingere sul fotorealismo. Famiglia di modelli tutti open weight, tutti con capacit√† di editing, cosa che Z-Image Turbo non ha e quindi diciamo √® un vantaggio competitivo, che sostanzialmente hanno performance in termini di velocit√† migliori o pari a quelli di Z-Image a seconda di quale si sceglie e qualit√† a livello di Qwen-Edit 2512 che era gi√† leggermente sopra a Z-Image Turbo.

Sono... cio√® ci sono le versioni da 4 billion e da 9 billion, quindi comunque relativamente piccoli, che si differenziano per la licenza del modello. La versione 9 billion ha una licenza proprietaria di Flux per cui le immagini non sono utilizzabili per fini commerciali, mentre la 4 billion √® Apache 2.0. La versione 9 billion richiede una ventina di giga di VRAM per eseguire e il 4 billion invece 8, che significa "piccoli" perch√© per chi non si ricordasse Qwen-Edit richiedeva una sessantina di gigabyte di VRAM per eseguirlo. Sono distillati perch√© sono ottimizzati per generare immagini in quattro step, il che fa s√¨ che su hardware anche non esagerato (quindi diciamo top di gamma della versione consumer delle Nvidia RTX) si ottengano immagini in 1 secondo, 2 secondi, una cosa di questo genere.

Ho citato Nvidia perch√© cos√¨ come quando √® uscito Flux 2, la prima versione dell'altro mese, anche per questi "Kleine" Black Forest Lab ha lavorato con Nvidia per fare delle versioni quantizzate FP8 e NVFP4 che sostanzialmente richiedono met√† della memoria e sono da 1.5 a 3 volte pi√π veloci. Questo perch√© di nuovo uno degli obiettivi √® far s√¨ che funzionino con le GPU consumer, anche quelle mid-level.

**Paolo Antinori**

Tanto stiamo... anche perch√© √® diventato impossibile comprare della memoria in questi giorni, no? Tra l'altro, quindi anche i prezzi delle schede video saliranno, se non sono gi√† saliti. Quindi mi tengo... beh, scusami, lasciami commentare questa che √® nata come una battuta, ma √® in realt√† una news del periodo. Praticamente il costo della RAM agli utenti finali, a noi che vogliamo comprare il computer, √® aumentato tantissimo perch√© la maggior parte della RAM mondiale esistente l√† fuori se la stanno portando via Elon e gli amici suoi per costruirsi i data center. Quindi non c'√® pi√π RAM comprabile, tipo il 75% l'hanno comprato loro. Che effetti ha avuto questa cosa? Ha avuto gli effetti che nel mondo dei telefoni, ad esempio, si stanno rivedendo dei telefoni che ti propongono 2 GB di RAM, cosa che non si vedeva pi√π da 10 anni perch√© non sanno pi√π dove prenderla. Ma la parte pi√π significativa √® la storia di Samsung. Samsung... i produttori, la divisione che produce telefoni sta comprando RAM non Samsung, perch√© a Samsung che √® un produttore di RAM conviene maggiormente venderla agli altri che non venderla a quelli che comprano i telefoni, e quindi Samsung fa prima a comprarla fuori. Questa roba √® follia dal mio punto di vista.

**Alessio Soldano**

Eh, quindi s√¨, e perch√© il mercato della telefonia √® ormai con margini bassissimi, quindi non riescono a guadagnare.

**Paolo Antinori**

Guarda, mi fa sentire meglio che io ho un telefono con 12 GB di RAM senza nessun buon motivo, devo dire la verit√†, c'era quello e l'ho preso e forse ho fatto una buona scelta nel futuro pi√π di quanto potevo immaginare. Comunque, visto che ho tolto seriet√† a quello che dicevi, fammi commentare con una informazione semiseria. Io il tedesco non lo conosco e quando tu mi hai detto che "Kleine" significa piccolo, ho detto "Ah". E io non posso non confessarti che quando io leggevo "Klein", ho sempre pensato al concetto della bottiglia di Klein. La bottiglia di Klein √® una di quelle superfici costruite in maniera tale per cui l'interno diventa l'esterno. Adesso stai condividendo queste immagini 3D, in realt√† le hanno realizzate... ci sono fuori delle... eccola l√†, ci sono delle bottiglie per cui la superficie interna diventa l'esterna. Perch√© io ho pensato questo? Ho pensato: "Boh, con tutte le allucinazioni e le robe mind-bending che fanno i modelli visuali era un omaggio a sta roba". Ti giuro che l'ho pensato, non sto facendo la battuta, e invece imparo che "Kleine" significa semplicemente piccolo.

## **Evoluzione e Distillazione dei Modelli**

**Alessio Soldano**

Ok. Cosa stavo dicendo... S√¨, che si tratta di modelli open weight questi di Flux 2 Kleine. Sono disponibili su GitHub, su Hugging Face, eccetera. Tra l'altro su Hugging Face potete provarli gratuitamente in questo periodo. Mettiamo il link in descrizione se ci ricordiamo. Certo. S√¨, s√¨. Poi cercateli l√¨.

E nella pagina di GitHub ho notato due cose interessanti. Allora, la prima √® che nel rilasciare questi modelli hanno anche rilasciato tutta una serie di script per provarli e fin qui normale, in Python. E tra le varie cose che fanno questi script c'√® il prompt upsampling, che √® quella cosa che citavi tu Paolo forse l'ultima volta o la volta prima, non mi ricordo. Cio√® l'idea di far migliorare il prompt a un modello LLM prima di passarlo alla generazione dell'immagine facendo text-to-image. E nella descrizione sostanzialmente questi di Black Forest sostengono che il modello √® stato pensato, addestrato, eccetera proprio per funzionare bene con un upsampler davanti. Sapevatelo, come si suol dire. E hanno rilasciato lo script per fare questa cosa.

E sempre in tema di trasparenza, le immagini generate da Flux 2 Kleine hanno il watermark, un po' come fa Google con SynthID. In questo caso, sempre continuando nell'ottica della trasparenza eccetera, l'algoritmo che usano per il watermark √® pubblico e c'√® il link al progetto GitHub che fa questa cosa, anche qui interessante. E questo √® quanto. Le versioni, tra l'altro, oltre alle versioni che vi dicevo, ci sono anche le versioni "base" che non sono distillate per dare il massimo in quattro step: sono pi√π lente, ma si prestano per fare training di LoRA o per fare altre versioni distillate, un po' come quando con Flux 1 avevano fatto la versione "Schnell". Mi aspetto che possa succedere lo stesso a partire da queste versioni base. E tra l'altro tutta questa cosa che √® successa in questi giorni con Flux 2 Kleine, se vogliamo, spiega che il flop da un punto di vista mediatico di Flux 2 √® stato il prezzo da pagare per poter ottenere questi modelli. Adesso, cio√® Flux 2 era la base da cui partire e adesso stanno distillando, producendo nuove versioni specifiche per, in questo caso, il fotorealismo e la velocit√†.

**Stefano Maestri**

No, beh, interessante che poi il trend della distillazione... perch√© una delle cose che cerchiamo di fare sia in podcast che in newsletter √® quello di sempre tenere a mente i trend prima ancora che in s√© la cosa del momento, no? Perch√© le notizie del momento sono notizie del momento, ma la cosa che invece cerchiamo di fare √® individuare il trend per capire in che direzione stiamo andando. La distillazione a partire da modelli grandi √® un trend che abbiamo visto prima sugli LLM, adesso stiamo vedendo sulle Stable Diffusion e ho letto in giro che si comincia a fare anche sui modelli video. Questa cosa qua di specializzare i modelli video a partire da un modello di base... e sugli LLM i cinesi, per tornare a quello che dicevamo prima, sono stati i primi a introdurre distillazioni importanti (DeepSeek a inizio del 2025\) e stanno continuando ad andare in questa direzione. Cio√® specializzare i modelli, che allora √® leggermente diverso da quello che si chiama fine-tuning. Fine-tuning √® proprio specializzare il modello a fare certe cose. Qui se volete √® un fine-tuning, ma a livello di reinforcement learning, reasoning successivo, che d√† risultati ottimi, spesso ancora migliori di un fine-tuning normale.

Per√≤ questo qui √® una tendenza: vedremo credo nell'arco di quest'anno sempre di pi√π, anche nei modelli LLM, rilasciare modelli pi√π piccoli che sono dei distillati dei modelli grandi. Banalmente quelli open di OpenAI sono fatti per distillazione anche quelli. Perch√© questa cosa? Perch√© comunque quello che dicevamo prima, ridurre la dimensione dei modelli se fanno il compito specifico pu√≤ essere comunque un vantaggio, soprattutto quando progetti architetture agentiche di grandi dimensioni dove magari di modelli in gioco ne hai pi√π di uno.

**Alessio Soldano**

Anche per andare sull'edge.

**Stefano Maestri**

Certo, certo. Eh poi sai, quello non √® lontano o diverso da quello che ho appena detto io, perch√© uno dei trend che io mi immagino vedremo, non so dirti se 2026 o 2027, √® la distribuzione reale dei sistemi agentici. La cosa su cui sto lavorando va in quella direzione l√¨. E per distribuzione reale intendo che magari hai un sistema multi-agente in cui uno degli agenti sta sull'edge, sul tuo telefonino, sulla tua lavatrice, e poi comunica con una serie di altri agenti che fanno altre cose. Per√≤ sempre un agente che ha una qualche forma di intelligenza, ma per avere qualche forma di intelligenza sull'edge c'√® bisogno di modelli pi√π piccoli. Questo √® un trend possibile.

Tra l'altro sui sistemi multi-agentici ho postato di recente i miei complimenti al libro di Antonio Gulli che ho riletto in cartaceo.

**Paolo Antinori**

L'hai gi√† riletto? Io non ho ancora finito di leggerlo la prima volta. Mi fai sentire una schiappa.

**Stefano Maestri**

Allora, lo dicevo nel post. Devo dire che per me √® stata una lettura anche abbastanza veloce nel senso che sono concetti che... vabb√®, insomma, facciamo pi√π o meno quelle cose l√¨ e tutta la parte agentica facciamo pi√π o meno quelle cose l√¨, per cui ho fatto una lettura piuttosto rapida. E poi io l'avevo letto abbastanza nelle parti fondamentali anche in digitale quando era andato in pre-release, quindi √® stata una lettura veloce. Devo dire che comunque il libro vale assolutamente la pena di essere preso anche cartaceo. Io non sono un super fan dei libri cartacei, ma i manuali, specialmente dei pattern, poter andare avanti e indietro e guardare quelle quattro righe che ti servono in quel momento in cui lo stai implementando... penso che sia impagabile. In pi√π lui d√† i diritti in beneficenza, quindi insomma mi sono sentito di comprarlo.

Eh, per√≤ il libro √® molto bello, eh, per√≤ apre per me una discussione sul: "Ma ha senso un libro oggi?". Io me lo sono chiesto. Tutto si muove cos√¨ veloce, no? Ha senso oggi mettersi a scrivere dei libri? Tutti stanno scrivendo libri sulle AI con dubbi risultati. Noi no perch√© siamo controcorrente, noi non lo stiamo scrivendo, anche se... ma forse vorremmo, chi lo sa. Vorremmo, non vorremmo, non lo so, non me lo sono mai chiesto, per√≤ tutti li stanno scrivendo e qualcosa non ha del tutto senso per me. Perch√© scrivere oggi delle cose che si cristallizzano per un mese? Boh. Quello l√¨ sui pattern ne ha tantissimo di senso, perch√© fermarsi un attimo e dire: "Ok, siamo arrivati qui, per fare queste cose si fa cos√¨" e poi da una persona che ha una visione pi√π larga e quindi dice "s√¨, si fa cos√¨, ma chiss√† magari domani questa roba la fa gi√† il modello, la ristrutturi cos√¨". Comunque √® fondamentale fermarsi, per farsi venire anche nuove idee a chi li pensa questi sistemi. Fermarsi un attimo e vedere quello che c'√® √® fondamentale. Cos√¨ come io trovo fondamentale usare i sistemi di coding ad agente, anche se non vuoi scrivere codice ma vuoi fare altro, per vedere come funzionano gli agenti, perch√© al momento sono gli agenti di coding quelli che funzionano meglio.

## **Coding Assistito e Creativit√†**

**Stefano Maestri**

Ne parlavamo offline con Paolo che stiamo usando parecchio per fare un po' di tutto. Almeno io, non so te Paolo, ma io i vari CLI li sto usando per fare un po' di tutto sul mio PC, non soltanto scrivere codice: dallo scrivere testo al mettermi a posto i file, a fare queste cose qua.

**Paolo Antinori**

Guarda, io in realt√† no, non ancora. Ma non √® detto che non inizi perch√© dei casi d'uso ce li ho ovviamente anche io di disordine di file o di disordine di tab. Per√≤ sono per ora ancora legato a due utilizzi principali: la scrittura di codice, cosa che peraltro non facevo quasi da una vita. Quindi in realt√† mi ha... appunto, come l'avete raccontato voi in altri momenti, ci sono arrivato solo adesso e mi ha fatto riscoprire un po' questa passione della creativit√†, di far succedere cose in molto meno tempo e con molto meno sforzo.

L'altra cosa invece che ho gi√† citato in passato, ma per me √® ancora una rivelazione, √® usarli come... mi viene in mente una parola in inglese e mi sento scemo a dirla, tipo "chaperon", come si dice in italiano... come qualcuno che ti aiuta, come sostegno, ecco questa potrebbe essere una parola, per fare invece interazioni con sistemi che sarebbero gi√† miei. Ad esempio io lo uso parecchio per organizzare e lavorare con il mio server casalingo in cui gli faccio fare ottimizzazione piuttosto che scrivermi degli script di Ansible, farmeli correggere, ma anche solo chiedergli: "Senti, come mai ogni tanto mi si frizza cos√¨ frequentemente?". E lui mi spiega perch√© la RAM sta swappando, la sta comprimendo... insomma, robe un po' nerd avanzate.

Oppure l'altra cosa, sempre molto nerd mi rendo conto, ma √® come configurare o gestire il mio router. Nel senso: il mio router √® un router che monta un sistema operativo open source, funziona, fa un sacco di cose, tutti quanti dicono che √® il meglio che c'√®, ma √® difficile, cio√® obiettivamente io non lo capisco fino in fondo. Il modello mi aiuta a capirlo, mi aiuta a gestire questa cosa. Poi faccio la stessa cosa con Home Assistant, il software per la smart home e queste cose qua, e mi stanno aiutando a usare meglio il software. Per dirvi... non lo so quale possa essere un software pi√π consumer a cui le persone possono relazionarsi maggiormente, magari Excel o Photoshop. √à come se io lo usassi per usare questi tool. Questi sono fortemente visuali e i miei meno, per√≤ il mio interesse √® andare in quella direzione e quello √® un uso che sto facendo davvero tanto.

**Stefano Maestri**

Beh, per√≤ scrivere una macro o una formula di Excel con un modello mi serve sicuro, eh, nel senso che non ne sono in grado.

**Paolo Antinori**

Leggevo stamattina nella mia LinkedIn-sfera italiana di qualcuno che segnalava l'equivalente open di Cursor, Open Workspace o qualcosa del genere. Sono curioso, magari mi lancer√≤ anche in quella direzione per vedere se riesco a utilizzare il mio recentemente comprato abbonamento GLM, perch√© ho usato il codice di Stefano che assolutamente voleva farcelo usare.

**Alessio Soldano**

Codice sconto\!

**Paolo Antinori**

E quindi in realt√† adesso ho questo credito di uso che mi fa dire: "Sto pagando questo servizio, devo usarlo per qualunque cosa" e probabilmente lo user√≤ per qualunque cosa.

**Stefano Maestri**

S√¨. E poi √® interessante il discorso anche del coding. √à interessante perch√©, come dicevi tu, non lo facevi da una vita e anche io mi sono rimesso a scrivere tanto codice. Ho letto un articolo che magari citer√≤ meglio con i miei pensieri personali anche nella newsletter che esce dopo questo podcast, di Salvatore Sanfilippo, antirez per chi lo conosce con il nome di GitHub, che √® uno degli sviluppatori italiani pi√π famosi e pi√π eclettici e ha anche scritto libri di narrativa e tutto. Lui ha fatto Redis, √® uno sviluppatore open source che ha sempre opinioni forti su tante cose. Chi l'ha visto su YouTube sa che Salvatore ha opinioni forti un po' su tutto, un po' come me forse.

E lui dice questa cosa qua: c'√® questo articolo in cui sostanzialmente dice "mi sono messo anch'io ad usare l'AI assistant coding e funziona. Non cascate nella trappola dell'anti-hype, cio√® di dire per forza che √® soltanto una moda, perch√© se lo provate √® mind-blowing. Cio√® io ho fatto pi√π cose di quelle che farei normalmente, e non vuol dire che perdi professionalit√†". Lui dice una cosa nell'articolo che io ho trovato interessante e dice: "Ma in fondo, quando voi programmavate da ragazzini fino alle 2 di notte, il punto era lo scrivere il codice o costruire qualcosa?". Building, scrive in inglese lui. Ed effettivamente forse la domanda √® proprio quella, cio√® il punto √® costruire.

Andando su altri articoli che ho letto in questi giorni sull'argomento, in fondo abbiamo fatto sempre pi√π software man mano che abbiamo fatto pi√π astrazione di questo software. √à vero, ne parlavo in intervista con Mario Fusco, che √® un'astrazione diversa perch√© √® indeterministica questa eccetera eccetera, per√≤ resta il fatto che pi√π astraiamo pi√π scriviamo software. Sia come comunit√† che come individui, perch√© alla fine la soddisfazione √® vedere il risultato, non √® vedere la singola riga di codice, almeno per la maggior parte delle persone. Poi per carit√†, c'√® anche chi a cui piace spostare le variabili da un registro all'altro, per√≤ diciamo che √® proprio una nicchia anche all'interno dei programmatori, questa, o no?

## **Riflessioni sul Gioco e la Creativit√†**

**Alessio Soldano**

No, a me hai fatto venire in mente che non mi ricordo da chi ho letto questa cosa, ma recentemente leggevo un commento di qualcuno che scriveva: "√à anche possibile che tra un po' chi scriver√† codice lo far√† perch√© gli piace proprio l'atto dello scrivere il codice, perch√© trova piacere nel fare quella cosa l√¨, pi√π che per il risultato finale". Perch√© se il punto √® il risultato finale, allora tanto vale che l'effort sia speso a un livello un pochino pi√π alto, ad esempio nel prompting, nel pensare all'architettura, eccetera.

**Paolo Antinori**

Alessio, il mondo √® strano. Auri, uno dei nostri amici e colleghi, ogni anno partecipa a un contest sul Commodore 64 di chi riesce a fare il videogioco pi√π interessante con un hard limit di 64K o 128K di codice. Persone trovano divertente imporsi questo limite e vedere come riuscivano a superarlo. Quindi ci sar√† sempre qualcuno che vorr√† fare l'attivit√† fine a se stessa.

**Alessio Soldano**

Ma infatti il mio commento non era dispregiativo verso questo tipo di attivit√†. Era per dire che il senso a quel punto √® che ti deve dare soddisfazione proprio stare al gioco di scrivere il codice secondo determinate regole, e fine. Cio√® √® un po' come chi fa il cubo di Rubik o...

**Stefano Maestri**

Esatto, stavo arrivando proprio l√¨. Pensavo alle parole crociate o al sudoku, che non ti danno un risultato tangibile nel senso di esterno alle regole che ti sei dato. La soddisfazione √® nel riuscire a farlo, √® una sfida con te stesso. Per√≤ entri nel mondo della sfida con te stesso, non nel mondo dell'industria o della produzione di cose nuove. Invece uno che scolpisce il legno per farne una statuina piuttosto che un violino lo fa per vedere il risultato finale, non per come muove la lima; non √® il muovere la lima la soddisfazione, ma √® il risultato finale.

**Paolo Antinori**

Mi piace molto questa osservazione di Sanfilippo sul fatto che l'obiettivo, anche quando eravamo al buio dello schermo catodico, era quello di creare pi√π che non di capire come farlo. Perch√© s√¨, probabilmente era quello. Il side effect √® che abbiamo scoperto che in realt√† trovavamo divertente anche la parte pi√π nerd di capire come funzionava. Motivo per cui abbiamo degli strani amici che sono esperti di performance: a loro interessa pi√π sapere come fare andare delle cose veloci di quante cose abbiano effettivamente da dover fare andare veloci. Loro vogliono sapere di poterlo fare, non hanno un obiettivo. Quindi √® un pochettino tutti e due. E il mio parallelo analogico √® il Lego. Il Lego √® spesso citato come il gioco preferito di molti. Potresti dire che l'obiettivo del Lego era ottenere l'astronave o la casa. S√¨, per√≤ in realt√† il divertente era montarselo ed √® stato con il Lego che abbiamo imparato che i muri non devi mettere i mattoni tutti della stessa maniera in verticale, ma li devi fare incrociati perch√© reggano meglio. C'√® anche questa parte ludica ma educativa che √® la parte interessante.

**Stefano Maestri**

No, no, ma in realt√† esistono entrambi gli aspetti. Hai citato il Lego ed √® significativo. Ne parlavo ieri con Stefano Gatti e abbiamo fatto due chiacchiere. Parlavamo proprio di Lego e di come i miei due figli e i suoi due figli allo stesso modo giochino al Lego in maniera diversa. C'√® chi ama quello che hai detto tu, cio√® prendere mille pezzi sparsi e costruire qualcosa, e chi come mio figlio: gli regali una scatola di Lego e non tira su la testa per ore fino a quando non √® andato una pagina per volta del manuale e ha costruito esattamente quello che vede nella fotografia sulla scatola, perch√© lui vuole quella cosa l√¨. Mi ha gi√† detto che vuole una vetrinetta per esporre tutti i suoi Lego finiti perch√© non li distrugger√† mai. Che √® l'opposto di come giocavo io.

**Paolo Antinori**

Io ringrazio mia madre che quando faceva le pulizie li spazzava via con noncuranza e quindi finivano sul pavimento e il gioco ricominciava. Magari lo faceva per me per ispirare la mia creativit√†. Il fatto di perdere le istruzioni contribuiva ulteriormente a non poter ricostruire la cosa esattamente com'era.

## **Il Vibe Coding e le Opinioni di Linus Torvalds**

**Stefano Maestri**

Cambiamo argomento.

**Paolo Antinori**

Scusami, volevo citare un'altra cosa. Abbiamo parlato di Sanfilippo, ma l'uscita che forse non abbiamo citato √® quella di Linus Torvalds. In questi giorni ha stupito tutti quanti dicendo: "Sapete che vi dico? Che forse il vibe coding non √® cos√¨ terribile". Perch√© la leggenda narra che nel periodo delle feste ha detto "vabb√®, posso non lavorare, cosa faccio? Scrivo del codice", perch√© questo √® quello che fanno i programmatori col tempo libero. E lui, se non mi ricordo male, si √® dato a Rust, di cui ammette di non conoscere tanto, e ha commentato come sia stato possibile per lui realizzare cose con l'aiuto dell'AI. Ha fatto un'affermazione forte: "Non √® una cosa professionale, ma √® molto divertente". Ora questa √® una posizione da Linus, che ti dice che secondo la sua visione non √® ancora utilizzabile. Magari ha anche ragione, perch√© anche noi stessi ogni tanto diciamo che non lasceremmo tutto in mano all'AI. Di sicuro concordiamo sul fatto che sia divertente. Ed √® interessante come il primo spiraglio di apertura venga da una persona cos√¨ opinionata, il re degli opinionati, ed √® riuscito ad ammettere questa cosa pi√π che mai su Rust, che √® un argomento controverso nel kernel Linux.

**Stefano Maestri**

S√¨, s√¨, s√¨. Beh, allora non possiamo non citare Kent Beck, che √® considerato dai programmatori Enterprise come un grande idolo, inventore dell'Extreme Programming e del Test Driven Development. Lui ha una newsletter su Substack in cui ormai scrive solo di vibe coding come un ragazzino entusiasta (notate che Kent Beck √® vicino ai 70). Dice: "Stanotte non sono riuscito ad andare a letto perch√© dovevo dare un altro prompt". Lo capisco perfettamente. Lui ne dice molto bene, dice che √® tornato a essere il "builder entusiasta" che era quando aveva vent'anni. Quindi il panorama √® vario. Ci sono anche degli assoluti detrattori che oggi non abbiamo citato, per√≤ c'√® un movimento di programmatori influencer che sta andando in questa direzione perch√© oggettivamente quello che √® successo in meno di un anno √® assolutamente notevole.

**Alessio Soldano**

S√¨, e poi il fatto che uno use case specifico non sia ancora completamente delegabile all'AI non significa che tutto il resto non possa trarre estremo giovamento. Sei pi√π efficiente perch√© ti liberi di tutta una serie di altre cose da fare. Se puoi scrivere i test in un decimo del tempo, quei nove decimi li puoi usare per scrivere il codice della business logic.

**Paolo Antinori**

A tal proposito mi sto trovando sempre pi√π spesso a beneficiare della possibilit√† di fare multitasking con il vibe coding. In parte perch√© guadagni tempo, e il tempo guadagnato lo uso per fare un'altra sessione di programmazione su un altro progetto in parallelo. Cos√¨ mentre aspetto che questa finisce, vado avanti con l'altra.

**Stefano Maestri**

O sullo stesso? Se l'hai organizzato bene, pure sullo stesso.

**Paolo Antinori**

Su questo non ci sono ancora arrivato. Ne parlavo con alcuni collaboratori questa settimana: non siamo ancora arrivati al punto di sfruttare bene Git Worktree per lavorare sullo stesso progetto in parallelo. Tendenzialmente il nostro cervello lavora meglio in parallelo puro su progetti diversi.

**Stefano Maestri**

Abbiamo avuto ospite ad aprile Alex di Backlog MD e una delle cose che abilita l'uso estensivo di Backlog MD √® proprio l'uso dei Git Worktree per lavorare in parallelo sullo stesso progetto su task diversi, e funziona molto bene.

## **Le Innovazioni di Apple e Gemini**

**Stefano Maestri**

Allora, 50 e passa minuti e non abbiamo ancora toccato gli argomenti principali della scaletta, ma facciamolo ora. Li enuncio tutti e tre. I tre argomenti principali sono Apple da un punto di vista business, Apple che sceglie Gemini come motore di Siri (pare che non gli cambino nome a Siri). Mettono Gemini dentro Siri, attenzione: Gemini e non OpenAI Chat GPT, che era stato a lungo chiacchierato come il sistema che avrebbe risollevato Apple Intelligence. Invece scelgono Gemini dicendo che per l'applicazione Enterprise √® meglio, mettendo ancora pi√π in difficolt√† i progetti di Chat GPT.

In pi√π ci aggiungiamo che anche loro stanno facendo il loro dispositivo. I rumors lo danno come un bottone da mettere qua con telecamere e microfono, o collegato alle loro cuffie, quindi uso della voce e delle telecamere, un po' come gli occhiali ma non indossabile. Pare che loro non vogliano andare all'indossabile, anche se io insisto che la nuova interfaccia tutta in trasparenza diceva "occhiali, occhiali, occhiali". Vedremo.

L'altra cosa legata a questo √® che se Gemini finisce su Apple, vuol dire che finisce su tutti i telefonini: Android da una parte e iOS dall'altra. Puoi installarti l'app di Chat GPT, ma mio padre prender√† quello che trova pre-installato, che sar√† comunque Gemini. Rimane giusto Llama dentro a WhatsApp e alle altre applicazioni di Meta. Tra l'altro Meta AI Labs ha presentato internamente i nuovi due modelli Avocado e Mango. Vedremo cosa esce.

E l'altra cosa invece √® che Google lancia Personal Intelligence, un'integrazione molto pi√π profonda con tutto Google Workspace. Tanto da andare a vedere i nostri dati, se lo autorizziamo, per rispondere a qualunque domanda senza specificargli "cerca nella mia mail". Lui utilizza mail, documenti e immagini per darmi la risposta. "Ho bisogno di comprare qualcosa per mia moglie perch√© compie gli anni domani": lui potrebbe rispondere che in una foto di quattro settimane fa ha visto che stava guardando la vetrina di un certo negozio. Questo √® il livello di integrazione che promettono.

## **L'Intelligenza Artificiale e la Privacy**

**Stefano Maestri**

Affascinante, anche un po' distopico. Non lo so, ho sempre opinioni forti su questo. Super affascinante da un punto di vista tecnico, ma da un punto di vista privacy... quanto diamo in mano a questi sistemi?

**Alessio Soldano**

Anche perch√© stiamo parlando di Google che ha tantissime informazioni sulla popolazione mondiale occidentale. Se ci aggiungi che finisce in tutti i telefoni compresi gli iOS...

**Stefano Maestri**

Oppure se aggiungi il fatto che stanno per lanciare (si √® gi√† visto qualcosa da Xiaomi e Samsung) gli occhiali che raccoglieranno informazioni video di tutto quello che vedi. Tante, tante informazioni.

**Paolo Antinori**

Ti devio dalla scaletta un istante per commentare il link di Shane Legg di Google. Ha fatto un post apparentemente serio in cui dice: siccome la General AI Intelligence √® dietro l'angolo, sto cercando qualcuno interessato a fare ricerca su quali saranno gli impatti dell'economia post arrivo dell'AGI. √à significativo perch√© lo ha postato qualcuno che lavora in Google e probabilmente vede cose dietro porte chiuse che noi non vediamo. Sta letteralmente assumendo per ragionare su cosa avviene dopo, perch√© il domani c'√® gi√† apparentemente.

**Stefano Maestri**

Io ho sentito la sua intervista al podcast di DeepMind. √à il direttore del laboratorio AGI di DeepMind, uno che ha visibilit√† su cose che possiamo solo immaginare. Gi√† nel 2012 parlava di modelli di linguaggio avanzati. Per cui se questo dice "prepariamoci che l'AGI √® qui", l'AGI √® qui. E lo dice anche nell'intervista. Quando gli chiedono quando ci sar√† l'AGI, lui risponde: "Dipende cosa intendi. Se intendi un solo modello che faccia tutto, 2-3 anni. Se intendi svariati modelli che nei loro verticali fanno gi√† meglio dell'uomo, allora siamo gi√† l√¨".

## **Pubblicit√† e Chatbot: Un Nuovo Paradigma**

**Stefano Maestri**

Terza notizia invece: Chat GPT ha lanciato in tutto il mondo "Chat GPT Go", 8 dollari per avere un abbonamento. Si colloca a met√† tra il free e quello da 20 dollari. Ma la notizia pi√π significativa √® che contiene la pubblicit√†. Dicono che le risposte non saranno influenzate dalla pubblicit√† e che sar√† sempre evidente cosa √® annuncio e cosa risposta. Immediatamente dopo, Google esce dicendo che Gemini la pubblicit√† non l'avr√† mai.

**Alessio Soldano**

Finch√© non cambiano idea...

**Stefano Maestri**

Per√≤ la loro risposta √® stata quella l√¨: "Voi avete bisogno della pubblicit√†? Bravi, siete degli straccioni. Noi facciamo a meno, tanto siamo gi√† su tutti i telefoni del mondo". Queste cose a livello business sono impattanti. La pubblicit√† nei chatbot sposta il modo di fare pubblicit√†, perch√© diventa estremamente pertinente avendo informazioni precise su cosa cerca l'utente.

**Paolo Antinori**

Ma buttandola su un discorso di leggi, essendo noi europei, non c'√® qualche legge che obbliga un messaggio commerciale a manifestarsi in quanto tale? Dovrebbero ben segnalare che √® pubblicit√†, aprendo la porta a quelli come me che filtrano via tutto.

**Stefano Maestri**

Sai che non lo so? Penso di s√¨.

**Alessio Soldano**

Il fatto che sia ben identificabile non significa necessariamente che sia rimovibile.

**Paolo Antinori**

Sul testuale che renderizza nel browser mi viene da dire che √® facilmente rimovibile. Pi√π difficile togliere quella di YouTube.

**Stefano Maestri**

No, per√≤ sai cosa? Ben identificabile e ben dichiarata. Nelle serie TV italiane, se il tizio beve un chinotto di una certa marca, √® scritto nei titoli di coda che quella marca √® sponsor della puntata. Per√≤ dentro al contenuto c'√® la pubblicit√†, e puoi nasconderla bene bene. Se ci riescono in TV, ci riescono anche altrove. Credo basti notificare all'utilizzatore che il servizio contiene advertising.

## **Riflessioni Finali e Conclusioni**

**Stefano Maestri**

Comunque super significativi questi movimenti business. Li abbiamo tenuti in fondo cos√¨ siete arrivati fino alla fine della puntata. Se siete arrivati fin qua e non avete ancora messo le stelline, siete delle brutte persone. Dovete metterle assolutamente.

**Alessio Soldano**

Cos√¨ che noi le vedremo e vi metteremo i link in descrizione.

**Stefano Maestri**

Esatto. Se ce lo dimentichiamo, mettete un commento per chiederlo. Ricordiamo che adesso abbiamo anche tutte le trascrizioni sul sito di Risorse Artificiali, quindi se volete prendere un pezzetto di testo per usarlo contro di noi e denunciarci, lo potete fare facilmente.

**Alessio Soldano**

O farvi fare i riassunti dalle AI degli episodi.

**Stefano Maestri**

Ecco, questo non fatelo proprio: almeno ascoltatelo\! Per√≤ mi triggeri su questo.

**Paolo Antinori**

Scusami Stefano, tu non stavi lavorando a creare il modello vocale sulla tua voce da rilasciare open?

**Stefano Maestri**

No, non mi risulta d'averlo fatto, per√≤ posso farlo. Non me ne frega niente della mia voce. In realt√† mi triggera quello che ha detto Alessio perch√© io non sopporto i podcast con voci generate. Il riassunto di qualcosa che √® in video magari lo faccio dopo averlo ascoltato, ma sento l'esigenza di un contenuto pi√π sfaccettato. Nel podcast ha senso mantenere una fruizione diversa, per√≤ questo √® il mio personale punto di vista.

**Paolo Antinori**

Possiamo dire le parolacce nel podcast, in scritto non viene bene.

**Stefano Maestri**

Possiamo dirle, infatti. Va bene, senza parolacce salutiamo. Grazie a voi e grazie a tutti quelli che ci mettono stelline, campanelline e ci seguono. Alla prossima. Ciao\!

**Alessio Soldano**

Ciao\!

**Paolo Antinori**

Ciao ciao\!
