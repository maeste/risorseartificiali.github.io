---
title: "Lâ€™AI tra GPT 5.2, benchmark, API, occhiali smart, cucina robotica e futuro della guida autonoma #30"
categories:
  - Puntate
tags:

  - AI
layout: single
author_profile: true
---

{% include video id="NmnTOJ7Ldi0" provider="youtube" %}

ðŸ‘‰ [Ascolta su Spotify](https://open.spotify.com/show/16dTKEEtKkIzhr1JJNMmSF?si=900902f2dca8442e)<br/>
ðŸ‘‰ [Guarda su YouTube](https://www.youtube.com/channel/UCYQgzIby7QHkXBonTWk-2Fg)<br/>
ðŸ‘‰ [Segui su LinkedIn](https://www.linkedin.com/company/risorseartificiali)<br/>


# Trascrizione: Puntata 30

## [00:00] GPT 5.2, Open Router e Teaser

**Stefano Maestri**
> Beh, non possiamo che partire dalla fine. Partiamo da ieri sera. GPT 5.2, no?

**Alessio Soldano**
> GPT 5.2, sÃ¬, che Ã¨ come Capodanno, eh.

**Stefano Maestri**
> E comunque prima di fare uscire il nuovo modello, sapendo che lo pubblicizzerÃ² con questo benchmark, sistemo il modello affinchÃ© vada bene con questo benchmark e dal giro dopo lo fanno tutti...

**Paolo Antinori**
> ...e quindi niente e quindi la fine della mia storia Ã¨ che sono passato a Open Router alla fine.

**Stefano Maestri**
> SÃ¬, ci sono dei modelli Llama perchÃ© faceva la battuta su Llama 4...

**Paolo Antinori**
> ...perÃ² c'Ã¨ un perchÃ©, quello Ã¨ il regalo di Natale per Zuckerberg, sai che a lei gli piace stavo questo.

**Stefano Maestri**
> Invece un uso concreto da Gen Z delle AI, quando dico che magari i giovani che ci sono nati o che ci stanno crescendo hanno pensieri un po' laterali rispetto all'uso che ne facciamo noi.
> Google ha annunciato che nel 2026 lanciano gli occhiali anche loro. Hanno detto: "Non li facciamo noi, noi facciamo la parte software e il modello che siamo capaci a farli. L'hardware lo fa Samsung basato su Android XR, la nuova versione di Android Augmented Reality e gli occhiali li fa Warby Parker".

**Paolo Antinori**
> Spegniamo prima, non spegniamo prima, chi lo sa?

**Stefano Maestri**
> E mia moglie mi fa: "Prova a chiedere a Gemini che cosa ne pensa".

**Alessio Soldano**
> Attualmente fanno tipo 450.000 viaggi per settimana. VabbÃ¨, ricordiamolo, Waymo sono macchine a guida autonoma.

**Stefano Maestri**
> L'ultima cosa che abbiamo in scaletta, che sono un paio di ricerche, tre in realtÃ , molto verticali sul mondo dei modelli.
> Adesso negli ultimi 3 minuti mi sono perso anche l'altro nerd che ascoltava e ci sono solo io che ascolto, quindi saluto me stesso. Il me stesso.

## [02:28] Intro e Analisi GPT 5.2

**Stefano Maestri**
> Buongiorno. Buongiorno a tutte e tutti e eccoci di nuovo con la puntata del sabato. Ciao.

**Paolo Antinori**
> Ciao Paolo. Ciao Alessio. Ciao. Ciao, scusa, mi sono confuso sul sabato. Ok, certo. Una puntata del sabato.

**Stefano Maestri**
> Quindi da dove partiamo? Non possiamo che partire dalla fine, partiamo da ieri sera. GPT 5.2, no? GPT 5.2, sÃ¬, che Ã¨ come Capodanno... non so, Ã¨ l'inizio del red code o code red, come cavolo l'ha chiamato Altman.
> Sapete che sostanzialmente ha fatto un comunicato interno di cui Ã¨ subito arrivato il leak, probabilmente rivisto da lui prima che facessero il leak, in cui ha detto "Attenzione, attenzione che Gemini ci sta mettendo il pepe al culo."
> Adesso sintetizzando fortemente quello che c'era scritto lÃ¬, e quindi diamoci una mossa, facciamo qualcosa di rilevante, di impattante, di incredibile, di bellissimo, come soltanto OpenAI sa fare.
> Allora, e hanno rilasciato GPT 5.2. Ãˆ questa cosa bellissima come solo OpenAI sa fare? Boh, nel senso che Ã¨ uscita ieri e non ho neanche capito se a me l'hanno giÃ  messa sull'account perchÃ© non si capisce piÃ¹ che modello usi, quindi immagino di sÃ¬, perÃ² non c'Ã¨ piÃ¹ scritto esplicitamente.

**Alessio Soldano**
> Forse sÃ¬.

**Stefano Maestri**
> Benchmark. Benchmark importanti, indubbiamente benchmark importanti, con tutto che ne abbiamo parlato con Alberto qualche puntata fa di benchmark affidabili, non affidabili, bla bla bla, perÃ² i benchmark sono importanti. In particolare uno che Ã¨ quello che loro hanno cominciato a mettere da Chat GPT 5, che fanno... non ho capito se anche altri lo fanno, ma credo di sÃ¬, perÃ² Ã¨ quello che si chiama GDP-Eval.
> Che cos'Ã¨ sto benchmark? Questo benchmark Ã¨ interessante perchÃ© invece di avere puzzle, diciamo puzzle di matematica, puzzle di programmazione, cose cosÃ¬, misura o cerca di misurare cose che abbiano un impatto reale di business. Quindi capacitÃ  di scrivere mail lunghe, riassunti, fare grafici, oppure delle sintesi di business, analizzare gli andamenti di borsa. C'Ã¨ dentro lÃ¬ c'Ã¨ dentro un po' di questa roba qua.

**Alessio Soldano**
> SÃ¬, mi sembra di aver letto che sono tipo una quarantina di use case reali nel mondo finance ed economico.

**Stefano Maestri**
> Esatto. Ãˆ tutto finance ed economia. Non c'Ã¨ altro perchÃ© loro dicono c'era giÃ  tanto altro sul coding, c'era giÃ  tanto altro su matematica e testo, ci voleva qualcosa di diverso e quindi hanno inserito questo benchmark qua che Ã¨ appunto nel mondo finance e come dicono loro... che stravince, insomma.

**Alessio Soldano**
> Sbaraglia la concorrenza.

**Stefano Maestri**
> SÃ¬, sÃ¬, che ti tradurrei sigaretta e braccio fuori, insomma, sorpasso cosÃ¬. Nel senso che rispetto a GPT 5.1 che faceva un po' meno del 39%, questo fa quasi il 71%.
> Allora faccio lo scettico, non voglio... lo dico prima cosÃ¬ non faccio un attimo lo scettico. Ãˆ un trend questo qua perÃ² di modelli nuovi che escono, che sbaragliano i benchmark nuovi che abbiamo giÃ  visto succedere tante volte.
> CioÃ¨, arriva il benchmark nuovo di Zecca, il primo modello che lo prova fa un 35-40%, quelli dopo super. E poi dopo invece ci si stabilizza un po' di piÃ¹.
> Io non voglio fare il malizioso e dire che i dati dei benchmark sono stati usati nel training, perchÃ© magari no, perÃ² sicuramente se io fossi uno dei ricercatori che fa il training del modello e vedo questo benchmark nuovo, quantomeno lo guardo cosa c'Ã¨ dentro e mi faccio ispirare o no?

**Alessio Soldano**
> SÃ¬. Poi comunque prima di fare uscire il nuovo modello, sapendo che lo pubblicizzerÃ² con questo benchmark, sistemo il modello affinchÃ© vada bene con questo benchmark e dal giro dopo lo fanno tutti e quindi ci si allinea.

**Stefano Maestri**
> Allora, loro spergiurano di non fare questa cosa qua che dici tu, che era la cosa che ha detto "io non voglio fare il malizioso", perÃ² almeno l'hanno visto prima, cioÃ¨ non Ã¨ un dato completamente nuovo, almeno per il ricercatore. PerchÃ© ricordiamo tutti che dato che entra, dato che esce e se quel tipo di modellazione di dati non era mai venuta in mente a nessuno prima, forse non Ã¨ stato insegnato cosÃ¬.
> Che tra l'altro apre tutta una serie di considerazioni se volete sui Transformer, perchÃ© stiamo dicendo che sono dei pappagalli bravissimi, mascherati, ma bravissimi, perÃ² pur sempre dei pappagalli. Non Ã¨ una novitÃ  per gli addetti ai lavori, perÃ² Ã¨ anche un po' per sfatare un attimino il mito del, se vuoi, dell'intelligenza vera e propria.
> In realtÃ  replicano comportamenti che gli sono stati insegnati. Poi nell'uso reale, se i comportamenti che ci sono stati insegnati sono tanti e tali che ti permettono di avere un improvement sul tuo lavoro, anche chi se ne frega. PerÃ² d'altra parte non Ã¨... d'altra parte va sempre tenuta presente questa cosa qua. CioÃ¨ dipende che cosa gli viene insegnato, no?

**Alessio Soldano**
> Appunto, anche senza entrare in malizia, Ã¨ come dire che questo nuovo modello che Ã¨ uscito Ã¨ esperto, se vuoi, in quest'ambito qui, perchÃ© si Ã¨ deciso di spingere di piÃ¹ su questa parte della conoscenza, questa parte delle capacitÃ  di analizzare questo tipo di informazioni e i prossimi probabilmente faranno la stessa cosa.

**Stefano Maestri**
> SÃ¬, sÃ¬. Beh, se la vogliamo vedere positiva Ã¨ che non scende negli altri benchmark.

**Alessio Soldano**
> No, infatti. Ma alla fine Ã¨ un po' come se pensiamo al mondo del software, come dire: vabbÃ¨, abbiamo questi test che sono i benchmark, che tutti fanno girare prima di rilasciare il loro software, che sarebbe il modello, e per non far brutta figura si accertano di aver sistemato tutto il software per benino, affinchÃ© i test diano dei risultati almeno comparabili a quelli del giro prima.
> Nuovo benchmark, nuovi test. Loro molto probabilmente li hanno fatti girare prima di uscire e gli altri no, perchÃ© non sapevano ancora che c'era questo benchmark. La prossima volta, prima di uscire, anche loro faranno le loro belle verifiche.

**Stefano Maestri**
> SÃ¬, sÃ¬. E indubbiamente qui la difficoltÃ , lo spieghiamo un attimo per gli ascoltatori, Ã¨ che essendo nell'indeterminismo il rischio Ã¨ che la coperta sia un po' corta, no? CioÃ¨ tu tiri per migliorare un aspetto del modello e cala dall'altra parte. La sfida Ã¨ quella di riuscire a strecciare questa coperta, di riuscire a coprire tutti i casi e apparentemente solo guardando i benchmark sono riusciti a fare questa cosa.
> Poi prime impressioni lette su X: Ã¨ bello, figo, funziona bene, dÃ  dei risultati molto migliori di 5.1. Uno Ã¨ mortalmente lento, questo letto su X. Ãˆ vero che bisogna vedere perchÃ© magari sono in rollout e non era ancora deployato su tutte le GPU che hanno a disposizione perchÃ© lo stanno facendo gradualmente, quindi i primi giorni sono sempre da prendere un po' con le pinze sulle performance. PerÃ² qualcuno dice non ci Ã¨ dato sapere se perchÃ© cosÃ¬ grande, nuovo eccetera o perchÃ© non...
> Qualcuno dice: Ã¨ estremamente figo perÃ² vale la pena usarlo soltanto per cose che richiedono veramente un ragionamento lungo e profondo, che Ã¨ lo stesso feedback che se leggete hanno dato quelli che stanno provando Gemini nella versione Deep Think. PerchÃ© oltre a Gemini 3, Gemini 3 Ã¨ uscito per gli utenti super paganti, quelli da 250 dollari al mese.

**Alessio Soldano**
> SÃ¬, anche in versione Deep.

**Stefano Maestri**
> Che Ã¨ la stessa versione che Ã¨ stata usata... cioÃ¨ no, la stessa Ã¨ la modalitÃ  che era stata messa su Gemini 2.5 ufficialmente, ma forse era giÃ  il 3, per vincere le Olimpiadi di matematica. Ãˆ quella versione capace di pensare in maniera piÃ¹ profonda. E anche lÃ¬ dicono bello, figo, fa delle cose incredibili, ancor piÃ¹ di Gemini 3, ma Ã¨ molto piÃ¹ lento e ci sta Deep Think, cioÃ¨ penso a lungo. E quindi questo Ã¨ un po' il discorso.

**Paolo Antinori**
> PerchÃ© un po' la storia di appena era uscito Chat GPT 4, no? Che era lento e ti suggerivano loro di stare attento di usarlo per quello che serviva e non per qualunque cavolata.

## [13:36] State of Enterprise AI e Costi Video (Sora/Veo)

**Stefano Maestri**
> Eh sÃ¬, perÃ² questa roba qui dei 5.2, e cioÃ¨ in particolare quel benchmark lÃ¬ che abbiamo nominato, va a braccetto con tutta una tendenza che si rileva in queste settimane nel mondo OpenAI, al di lÃ  di code red, code lÃ¬ come si cavolo l'ha chiamato Altman.
> C'Ã¨ scritto anche un articolo nel loro blog che si chiama "State of Enterprise AI", che sono 5 minuti di lettura che consiglio di farsi ed Ã¨ il loro spaccato su come il mondo enterprise sta adottando l'intelligenza artificiale, nella fattispecie Chat GPT perchÃ© i dati che hanno loro evidentemente sono quelli.
> Allora, al di lÃ  che l'articolo in sÃ© da leggere o almeno scorrersi, poi ripeto, sono 5 minuti di lettura, si puÃ² anche leggere tutto, Ã¨ interessante che loro esprimano un focus sull'Enterprise perchÃ© hanno spinto tanto sul consumer. Ultimamente stanno dicendo attenzione perÃ² perchÃ© comincia a essere il momento di guardare a delle revenue perchÃ© alla fine di questo parliamo, anche dal mondo enterprise, e quindi c'Ã¨ piÃ¹ attenzione a quello.
> E avevamo giÃ  visto piÃ¹ attenzione nella parte agenti, codex e tutte queste cose. Hanno inserito le chat di gruppo che anche quelle alla fine possono essere un giochino divertente tra amici, ma sono soprattutto per il mondo enterprise.
> Se tanto mi dÃ  tanto, se arrivano a pubblicarlo vuol dire che un'analisi interna Ã¨ stata fatta. Cominciano ad esprimere un interesse forte per il mondo enterprise che sembrava un pochino piÃ¹ appannaggio di Anthropic e Google. Loro stanno dicendo attenzione che anche a noi interessa, cioÃ¨ non Ã¨ che vogliamo lasciare sul piatto questa cosa qua, benchÃ© abbiano sempre detto fino ad oggi "a noi interessa di piÃ¹ il mondo consumer".
> In realtÃ  avevano pensato ad un social media, avevano fatto questa cosa qua di Sora in versione quasi social media. A proposito, avete visto che Ã¨ sparito, non ne parla nessuno?

**Alessio Soldano**
> SÃ¬, non ne parla nessuno.

**Stefano Maestri**
> Sora come Ã¨ stato per Sora 1. Se vi ricordate Sora 1, il modello piÃ¹ incredibile. Ãˆ uscito Veo. Accidenti Ã¨ uscito Veo. Niente, Sora, ciao.
> Invece Sora 2... allora, i leak dicono una cosa diversa. I leak dicono che 20 secondi di video generato da Sora 2 costa $5.80 a OpenAI e l'avevano rilasciato in maniera social, tutti fate i video eccetera eccetera. Poi si sono accorti che nella prima settimana sono andati quei 5 billion e han detto "Aspetta un attimo che rallentiamo quell'attimino perchÃ© diventa difficilmente sostenibile questo oggetto qua". Che ci sta, eh. E mi chiedo, non so se qualcuno di voi ha dei dati, va bene che Google ne ha da buttare, ma Veo e Nano Banana eccetera che costi avranno? PerchÃ© immagino non irrisori.

**Alessio Soldano**
> Allora, io su Veo non lo so, se non se provi a usare Veo per dire... io uso Open Art, un servizio che fa tipo... cioÃ¨ tu compri una subscription di un tot di euro al mese e poi puoi usare vari modelli con dei crediti. E la generazione di filmati con Veo Ã¨ costosissima, cioÃ¨ tipo 30-40 volte quello che ti costa usare i modelli cinesi. PerÃ² vabbÃ¨, non lo uso praticamente mai.
> Sulle immagini leggevo giusto credo ieri sera o stamattina, non ricordo, che quando Ã¨ uscito qualche settimana fa Nano Banana Pro, in realtÃ  il costo delle API per generare immagini con Nano Banana Pro Ã¨ tipo quattro volte Nano Banana precedente. Quindi da 6-7 centesimi, no di meno, ad immagine con il vecchio Nano Banana a 13, 14, 15 con il nuovo. Che comunque 4x non Ã¨ poco.

**Stefano Maestri**
> SÃ¬, sÃ¬, ma infatti anche per dire giusto ma a livello di quanto si fanno pagare loro...

**Alessio Soldano**
> ...e che immagino che per loro alla fine voglio dire energia, che tendo a vedere come proporzionale a quanto costa loro. Non credo che abbiano margini completamente differenti.

**Stefano Maestri**
> Non piccolissimi i margini, secondo me, perÃ² come su tutto il cloud. PerÃ² vabbÃ¨, questa non ho insights, Ã¨ un'opinione. PerÃ² Ã¨ un discorso anche di sostenibilitÃ  alla fine, non so.

## [18:57] API Google: Limiti e "Ban"

**Paolo Antinori**
> Guarda, io contribuisco a questa conversazione non parlando dei modelli per immagini, ma di modelli base. Vi raccontavo che ho litigato con Google nel weekend perchÃ© stavo facendo il calendario dell'avvento degli agenti e a un certo punto ho smesso di funzionare perchÃ© avevo usato troppo API. Gli ho detto, "VabbÃ¨, ok, non ho idea di quale sia il limite, devo avere sforato un pochettino." Quindi ho portato pazienza, ho aspettato qualche giorno per vedere se si ristabiliva, non si Ã¨ ristabilito niente.
> Allora mi sono stufato, ho detto "Cia, fammi guardare su internet cosa si dice." Anzi no, scusate, ho fatto quello che fanno tutti. Ho creato un secondo account, ho detto "Beh, creo un altro account e mi faccio dare un'altra API". Creo un nuovo account, faccio andare altri API e mi blocca lo stesso.
> Dico, "Eh, sono diventati cosÃ¬ intelligenti che hanno capito chi sono io e quello che faccio?", che Ã¨ fattibilissimo.

**Alessio Soldano**
> Ma del resto Ã¨ Google, sa tutto di te, no?

**Paolo Antinori**
> Del resto Ã¨ Google, sa anche... SÃ¬. E quindi ho detto "Magari sono arrivati a quello, ma mi sembra strano, cioÃ¨ queste cose si possono fare ma nessuno quasi le fa". Allora ho cercato su internet, anzi ho chiesto a Gemini stesso, ho detto "Oh, com'Ã¨ che prima usavo e adesso cosa ti ho fatto? PerchÃ© non me la dai piÃ¹ praticamente?"
> E lui mi ha risposto e c'Ã¨ un po' di internet che si sta lamentando, in particolare in Europa, perchÃ© Google ha cambiato la quota del free tier, quindi della parte gratuita, e l'ha abbassata.
> Ho detto, vabbÃ¨, perÃ² l'ha abbassata, ok, cioÃ¨ due o tre chiamate al giorno me le farai fare, no? Quando crei adesso... vi invito anche voi a farlo perchÃ© magari davvero bloccano solo Paolo e non gli altri. Quando create una nuova chiave, lui vi fa copiare il comando di prova in Curl per dire copialo, lancialo nella shell e vedi che ti funziona. Se lanci quello, istantaneamente ti esce un 429 in cui ti dÃ  i rate limit e ti dice quota zero.
> Quindi non Ã¨ che l'ho esaurita. Poi c'ha un messaggio confuso che ti dice tra 35 secondi ti si libera, ma non Ã¨ vero, perchÃ© poi Ã¨ hardcodato quel quota zero nell'altra parte del messaggio. E niente, hanno tagliato completamente la quota che Ã¨ stato interpretato come un segno del loro stesso successo, non potevano piÃ¹ permettersi di regalare a destra e a sinistra l'accesso all'API e quindi niente... e quindi la fine della mia storia Ã¨ che sono passato a Open Router alla fine.

## [21:25] Open Router e DisponibilitÃ  Modelli

**Stefano Maestri**
> Ecco, sÃ¬, che lo nominavamo l'altra volta, Open Router, lo ridiciamo per gli ascoltatori. Ãˆ un servizio che vi permette di utilizzare diversi modelli attraverso l'API, sia modelli gratuiti che modelli a pagamento con qualche cosa particolare che se ci mettete almeno un po' di soldini vi danno piÃ¹ accesso a quelli liberi anche piÃ¹ crediti a quelli liberi.
> PerÃ² Ã¨ un servizio interessante per chi sta sviluppando cose con le API per provare senza dover fare l'abbonamento o mettere crediti su tutti i provider di modelli che voglio provare, ma provare diversi provider attraverso Open Router.

**Alessio Soldano**
> Questo Ã¨ un pattern che c'Ã¨ in vari ambiti. Ad esempio, io prima citavo Open Art, se vuoi Ã¨ piÃ¹ o meno lo stesso ragionamento per le immagini. La cosa carina Ã¨ che tu cosÃ¬ facendo ti becchi, diciamo, gli aggiornamenti dei modelli, cioÃ¨ esce un nuovo modello, questi dopo un po' lo attivano e quindi hai giÃ  le cose nuove che puoi provare e vedere come vanno, senza dover pensare a "no, vabbÃ¨, mi registro quest'altro" piuttosto che se posso farlo andare in locale, preparo il sistema per usarlo in locale cosÃ¬.

**Stefano Maestri**
> E tra l'altro questo ci collega ad una cosa che io ho scritto in newsletter settimana scorsa. Ho messo il link al loro articolo, diciamo, di fine anno, che Ã¨ un'analisi lunga, sono 40 minuti almeno a leggerla se la leggete tutta per bene. Ãˆ una lunga analisi di come le API vengono utilizzate attraverso il loro servizio, ma avendo un numero elevato adesso di clienti ed avendo diversi modelli hanno un punto di vista privilegiato su questa cosa, quindi hanno molti dati e li rendono pubblici questi dati.
> Ed Ã¨ interessante da leggere per chi fosse interessato a questa cosa qua. Adesso non ha senso andare nei dettagli qui nel podcast perchÃ© vabbÃ¨, son grafici, numeri, perÃ² se avete voglia di leggerlo trovate facilmente l'articolo, poi magari lo metto anche in descrizione e c'Ã¨ comunque nella mia newsletter di settimana scorsa.

**Paolo Antinori**
> Un altro motivo per cui quello che hai appena detto Ã¨ interessante Ã¨ anche perchÃ© la lista dei modelli nel catalogo di Open Router Ã¨ bella lunga, ce ne saranno, non so se 50, 100, cioÃ¨ quando inizi a leggerla ti stufi a un certo punto e dici "vabbÃ¨, ok, dammene uno perchÃ© non ho voglia di leggerlo."

**Stefano Maestri**
> Fai te. SÃ¬, sÃ¬, sÃ¬, sÃ¬. Ci sono tutti gli state of the art, ci sono la maggior parte di quelli open. Per dire, c'Ã¨ Mistral, tra gli europei, ci sono i vari DeepSeek, GLM, Yi tra i cinesi, e poi che altro? Poi ci sono i Llama, sempre non c'Ã¨ il quattro di Llama, non si vede quello, perÃ² non piace, chissÃ .
> E poi ci sono tutti gli state of the art nelle varie versioni disponibili ancora in API, cioÃ¨ tipo di GPT, c'Ã¨ ancora il 4o visto. Per chi volesse provare cose piÃ¹ vecchie ha senso? Non ha senso oggi, ma non lo so questo...

**Paolo Antinori**
> Scusami, a tal proposito, nel mio rant precedente contro... ovviamente irragionevole perchÃ© mi lamentavo che Google non mi dava piÃ¹ una cosa gratis, non era neanche cosÃ¬ generosa la loro offerta, perchÃ© loro l'accesso che hanno tolto era quello a Gemini 2, non il 3 attuale. Quindi hanno deciso che l'hanno dismesso.

**Stefano Maestri**
> Ma non Ã¨ che l'hanno dismesso? Non Ã¨ che semplicemente l'hanno dismesso nella versione free puoi accedere solo a 2.5, da 2.5 in avanti? PerchÃ© poco tempo fa l'hanno fatto con 1.5, lo so perchÃ© mi si sono rotti tutti i test di A2A.

**Paolo Antinori**
> Allora, se cosÃ¬ Ã¨, nessuno si Ã¨ ricordato di cambiare il Curl che ti genera l'endpoint, perchÃ© l'endpoint dice il due. Io ho provato a tweakarlo, a passare da due a tre, l'ho fatto di sicuro, 2.5 non sono sicuro di aver provato, che poi comunque potrei non aver indovinato perchÃ© c'hanno dei suffissi e quindi potrei non aver saputo il segreto, perÃ² niente suggerisce quello e Reddit non suggeriva che fosse quello il punto, quindi se cosÃ¬ Ã¨ non ci sono capitato.

**Stefano Maestri**
> No, no, era perchÃ© mi Ã¨ successa questa cosa qui che a un certo punto non andavano piÃ¹ nessuno dei test a 2A perchÃ© non rispondeva piÃ¹ il modello e perchÃ© la libreria che usavamo usava 1.5 che era un modello molto vecchio.

**Alessio Soldano**
> Quindi dirti rate limit zero era molto piÃ¹ semplice per loro che spiegarti che l'avevano tolto, no?

**Stefano Maestri**
> In realtÃ  probabilmente l'errore che beccavo io era piÃ¹ sensato. Parlante di rate limit zero. Adesso non me lo ricordo, Ã¨ passato un po' di tempo.

**Paolo Antinori**
> A onore di Google c'Ã¨ il link che ha scritto "vai a leggerti rate limiting" perÃ² Ã¨ poi una pagina da documentazione tipica di Google in cui non si capisce comunque una mazza e devi dire "sÃ¬, ma quindi cioÃ¨ me li dai o non me li dai?". Non so se giocano sull'ambiguitÃ  o danno per scontato che si capisca. Non mi Ã¨ chiaro.

## [27:04] Leak su Meta e Wearable (Pebble)

**Stefano Maestri**
> PuÃ² essere, puÃ² essere che giochino un po' sull'ambiguitÃ . Ma abbiamo parlato come prima dei modelli Llama perchÃ© faceva la battuta sul Llama 4, perÃ² c'Ã¨ una notizia che non ho messo neanche in scaletta perchÃ© l'ho letta ieri sera, Ã¨ questa che dice sostanzialmente che ci sono leak, ma credibile a questo punto, perchÃ© altrimenti non si capisce perchÃ© tutto l'investimento di Meta dei mesi scorsi...
> SÃ¬, che vi ricordate, ha preso un sacco di ricercatori, un sacco di gente prendendola ad altre societÃ , strapagandoli, facendo un'acquisizione importante, non mi viene il nome dell'azienda, ma sostanzialmente per prendere il loro CEO di questa azienda e farlo diventare il capo di tutta la parte AI.
> E comunque il leak per venire i giorni nostri Ã¨ che loro stiano lavorando forte su un modello ma closed, cioÃ¨ che abbiano abbandonato la filosofia Open che stava dietro a Llama, Llama Stack e tutto per tornare Closed Source e cercare di andare a fare revenue su quella cosa lÃ¬.
> Che se da un lato Ã¨ ragionevole perchÃ© vogliono fare revenue, dall'altro deve essere proprio buono buono sto modello perchÃ© da un lato si sono fatti cattiva fama con l'uscita di Llama 4, dall'altro non sono riconosciuti per davvero, almeno nel mondo enterprise, come una big tech come le altre, ma piÃ¹ come quella dei social media o comunque della parte consumer.
> PiÃ¹ buttano via anche il fatto di essere relativamente open, andare sul modello proprietario si prendono un rischio... da un lato comprensibile, dall'altro Ã¨ un bel rischio che si prendono.
> PerÃ² il leak dice che non Ã¨ che non sono spariti e non Ã¨ che sono con la coda tra le gambe a piangere se stessi su Llama 4, semplicemente hanno fatto altre scelte. Quali siano queste altre scelte Ã¨ difficile da capire perchÃ© il livello di ricercatori che si sono portati in casa potrebbe pure essere qualcosa oltre i Transformer. PerchÃ© han preso gente di primissimissimo livello, magari meno in vista, magari non quelli che vanno a fare le interviste perchÃ© hanno fondato OpenAI 5 anni fa.

**Alessio Soldano**
> SÃ¬, che perÃ² sono ricercatori sul pezzo, diciamo.

**Stefano Maestri**
> Ma gente veramente di primo livello, quindi boh, che cosa stiano facendo sono molto curioso. Se Ã¨ quello che stan facendo, prima o poi dovranno scoprire le carte e quantomeno vendere qualcosa, perchÃ© altrimenti con tutti gli investimenti che han fatto, per quanto abbiano soldi pressochÃ© infiniti, e prima o poi finiscono anche quelli pressochÃ© infiniti, perchÃ© tra l'altro hanno comprato un'altra azienda anche questa settimana. Aspetta, com'Ã¨ che si chiama? Beh, vediamo questa ce l'abbiamo da qualche parte negli appunti. "Acquire Limitless" che fa Augmented Reality.

**Paolo Antinori**
> Eh, perchÃ© quello Ã¨ il regalo di Natale per Zuckerberg, sai che a lei gli piace quella cosa lÃ¬, quindi ogni tanto gliene regalano una.

**Stefano Maestri**
> SÃ¬. Ah, no. Non fa augmented reality. Registra conversazioni che perÃ² sta dentro a tutto quel trend di registrare la voce costantemente per poi farne riassunti.
> Io sono tentatissimo qui, ho la pagina aperta, di ordinare un Pebble Index che voi non saprete che cos'Ã¨, ma Ã¨ un coso fighissimo, un anello che vi faccio vedere la diapositiva.

**Alessio Soldano**
> Stavo giusto dicendo, ma questa roba quindi gli serve per gli wearable delle registrazioni?

**Stefano Maestri**
> Eh, io la interpreto cosÃ¬. Io ho trovato questo oggetto bellissimo, queste cose qua, insomma, dipende. Guardate che oggetto bello. Questo Ã¨ un anello. Ãˆ un anello che costa pochissimo che non ha... devi ricaricarlo perchÃ© Ã¨ un dispositivo abbastanza stupido. Sostanzialmente un microfono che si collega al tuo cellulare e promette... Ã¨ una startup, una di quelle cose che si ordinano, che non l'hanno ancora prodotto e quindi probabilmente non mi arriverÃ  mai.
> Ma tu schiacci quel bottone lÃ¬ e registri la tua voce, poi c'hai un modello locale al telefono, quindi non condividi con niente, che ti fa il riassunto di tutto quello che hai registrato durante la giornata, che se volete Ã¨ un'applicazione piccola ma che ha un suo perchÃ©. Tutto sommato poi in Europa probabilmente se l'ordino mi arrestano. Il giorno dopo nel momento in cui io l'ho ordinato in Europa mi arrestano perchÃ© violo la privacy di tutti quelli intorno a me. Viene qua il garante della privacy con i mitra e mi portano via, non mi vedete piÃ¹.
> PerÃ² boh, in teoria Ã¨ ordinabile. Ho solo tentato di farmi il regalo di Natale.

**Alessio Soldano**
> Il giorno dopo lo mettono sugli smartwatch, cioÃ¨ non Ã¨ che sia cosÃ¬ tanto differente il gesto...

**Stefano Maestri**
> No, no, perÃ² piÃ¹ discreto, boh, non so.

**Alessio Soldano**
> SÃ¬, non Ã¨ diverso il gesto hai ragione, guarda, sÃ¬, hai ragione.

**Paolo Antinori**
> Ma scusami, si chiama Pebble? Sai per caso se Ã¨ di quella stessa startup che in passato aveva fatto il Pebble che era uno smartwatch con lo schermo e-ink? PerchÃ© se sono loro hanno deliberato loro, sono loro. E allora allora lo porteranno a casa il prodotto, cioÃ¨ hanno un track record che convince che succederÃ . Poi loro erano falliti o erano stati comprati, non mi... forse erano stati comprati e chi li aveva comprati aveva discontinuato il prodotto, perÃ² ho degli amici che ce l'hanno quell'orologio lÃ¬. Era una nerderia carina.

**Stefano Maestri**
> Infatti dicono dopo 9 anni in stasi siamo tornati, proprio Ã¨ la line.

**Paolo Antinori**
> Beh, allora se volevi essere sicuro se lo vedrai, probabilmente lo vedrai, perÃ² come diceva Alessio, anche secondo me da lÃ¬ poi arriva dappertutto, arriva sull'orologio, sull'automobile, sugli occhiali, prossimo giro...

**Stefano Maestri**
> SÃ¬, sÃ¬. No, no, ma infatti finisce sicuramente sugli occhiali e la cosa affascinante Ã¨ che tu non lo debba ricaricare, nel senso che ha dentro una sua batteria che dicono durare per anni.

**Paolo Antinori**
> Eh beh, sÃ¬. Eh sÃ¬, sÃ¬, puÃ² darsi anche che usi quelle altre tecnologie che si usano nella domotica, per cui l'energia cinetica della tua pressione del pulsante gli dÃ  quel tanto che basta per attivare.

**Stefano Maestri**
> SÃ¬. E poi ce l'hai addosso anche magari basta un piccolo accelerometro che carica, perÃ² era affascinante questa cosa che non lo devi caricare.

**Alessio Soldano**
> Io pensavo al meccanismo degli orologi. SÃ¬, meccanici, cioÃ¨ agli automatici meccanici.

**Stefano Maestri**
> SÃ¬, sÃ¬, Ã¨ una cosa del genere, di sicuro. Boh, mi aveva affascinato questa idea qui del Pebble. Poi mi era venuta in mente un'altra cosa da dire mentre parlavo di questa roba qua, ma me la sono persa.

**Paolo Antinori**
> Eh, comunque per rimarcare qualcosa di detto del passato, invece la seconda parte di questa invenzione, ovvero il modello locale che ti prende del transcript, questo vi ricordo che Ã¨ uno dei casi d'uso base del progetto di AI Edge Gallery di Google, quello che si puÃ² scaricare in cui tira giÃ¹ un modello da 8 GB o qualcosa del genere e ti gira locale sul telefono. Io ce l'ho, mi gira, non lo tengo acceso, non lo uso praticamente mai, ma si puÃ² fare.

**Stefano Maestri**
> Ma e non ti ho mai chiesto, ma il consumo della batteria di quell'oggetto lÃ¬?

**Paolo Antinori**
> Non lo so, perchÃ© io lo tengo acceso il tempo che ci gli chiedo due cose e poi dico vabbÃ¨, ma perchÃ© devo fare sta cosa? CioÃ¨, ha senso se sei in aereo o se sei sottoterra. Quindi non ho trovato un uso.
> Ero tentato, a dire la veritÃ , di capire se potevo usarlo come building block per crearmi la mia automazione per convertire i maledettissimi messaggi vocali di WhatsApp in un messaggio testuale che WhatsApp ha la feature lato server, ma non per l'italiano e quindi non posso avere la traduzione automatica da vocale a scritto.
> Quindi ho detto, "Magari me la riesco a scrivere con l'aiuto di Vibe Coding o con qualche cosa cosÃ¬, ma sarebbe probabilmente da mettere in piedi un'app dall'inizio alla fine, con il modello che sia sempre attiva e a quel punto sÃ¬, la batteria potrebbe avere un impatto."

## [36:08] Gen Z: Studiare con l'AI

**Stefano Maestri**
> Guarda, su questo invece un uso concreto da Gen Z delle AI, quando dico che magari i giovani che ci sono nati o che ci stanno crescendo hanno pensieri un po' laterali rispetto all'uso che ne facciamo noi, l'utilizzo che ne fa di piÃ¹ mia figlia e su e alcuni suoi amici, compagni di classe contaminati un po' da le mie manie.
> Eh, allora lei usa chat GPT piÃ¹ di me, molto piÃ¹ di me, nel senso che ha accesso anche a tutti gli altri Gemini, Claude e quant'altro. Claude non ci va d'accordo a conferma che Ã¨ disegnato piÃ¹ per professionisti per la nostra generazione, per l'Enterprise.
> Gemini lo sta cominciando ad apprezzare. Parentesi, tra l'altro, su Gemini avete visto che c'Ã¨ la nuova modalitÃ  vocale che Ã¨ molto piÃ¹ espressiva, molto piÃ¹ carina, molto piÃ¹ alla voce piÃ¹ assomiglia molto piÃ¹ a quella di Chat GPT, diciamo, e dell'hanno annunciato l'altro giorno. L'ho provato stamattina che dovevo chiedergli delle cose. Ho provato a chiederle a Gemini invece che a GPT.
> Torniamo alla parentesi dopo perchÃ© mi hai fatto venire in mente che volevo dire due cose, no? E invece l'uso che ne fa mia figlia e lo usa molto e Chat GPT dice andare meglio di Gemini, ma lei non l'ha ancora provato col tre, la stimolerÃ² a farlo col tre.
> Ãˆ una roba che io non avevo mai pensato sinceramente di fare cosÃ¬. Gli carica le fotografie del libro, ok? Fotografa il libro che in classe o mentre studia sottolinea con l'evidenziatore o magari anche con evidenziatori diversi e gli dice "Adesso fammi un riassunto, ma voglio che tu mi riassuma soltanto le cose evidenziate collegandole tra loro. Sappi che il giallo ha questo significato, il verde ha questo significato, eccetera eccetera" e gli fa un prompt scritto bene e funziona, cioÃ¨ gli fa dei riassunti veramente belli.
> Con grafici, con... No, allora con grafici no, perchÃ© ha usato GPT. Io ho provato a fare una roba simile dopo che l'ho visto fare a lei su Gemini. Gemini sapete che adesso genera anche tutta la le cose un po' piÃ¹ grafiche e lÃ¬ fa tutto fa tutto bello bello bello. PerÃ² io non ci avevo mai pensato al caricare il dispositivo cartaceo fotografato arricchito dal contesto dell'evidenziatore e chiedere il riassunto specifico. Ãˆ un utilizzo carino.

**Paolo Antinori**
> Non voglio smorzarti, ma io ci avevo pensato. Avevo cercato la feature e non l'avevo trovata e non avevo pensato di chiederlo a Gemini perchÃ© a me questo sembrava un problema da machine learning tradizionale. A me interessava le sottolineature dei miei libri stampati che sono cose interessanti, ma le vorrei avere in digitale per averle comode, magari da mettere in notebook LM e non le avevo e non ho mai pensato di fare la soluzione di tua figlia, ovvero darlo a un modello abbastanza potente che probabilmente riceve piÃ¹ informazioni di quelle che io gli sto chiedendo, ma che mi sa dare la risposta.

**Stefano Maestri**
> Beh, fa tanti passaggi, eh, quella roba lÃ¬, cioÃ¨ capisce cosa c'Ã¨ sotto. Prende una foto, analizza, capisce cosa c'Ã¨ sottolineato, fa l'OCR del testo sottolineato e poi fa il riassunto. CioÃ¨, tanta roba quando dicono che gli LLM non svolgono compiti reali, beh, insomma, questo nel suo piccolo Ã¨ un compito della Madonna.

**Paolo Antinori**
> SÃ¬, perÃ² fammi sottolineare che senza la parte di riassunto finale eravamo ancora nei limiti del machine learning standard, ovvero riconoscimento visuale, testuale e insomma gli OCR dei machine learning standard...

**Stefano Maestri**
> Ma e poi anche riconoscere il sottolineato, cioÃ¨ il ragionamento, ce lo dice meglio Alessio, ma il ragionamento che fanno sulle immagini, gli LLM visual eccetera Ã¨ un altro livello rispetto a uno OCR di 3 anni fa, non 100.

**Alessio Soldano**
> Guarda, su questo argomento qui Ã¨ uscito un blog di Gemini di Google che parla proprio delle migliorie che hanno fatto sul Visual Reasoning in Gemini 3 Pro. E vabbÃ¨, anche qui lasciando perdere un attimo i benchmark che comunque sono interessanti, si fa proprio vedere una serie di casi d'uso in cui hanno insomma ottenuto dei risultati impressionanti.
> E tra... mi Ã¨ venuto in mente prima quando citavi tua figlia Stefano, tra i documenti che davano tra virgolette da scannerizzare a questo tool per farglielo comprendere e digitalizzare c'erano addirittura anche dei manoscritti del 1800, eh, anzi XVI secolo, e che potete immaginare come dessero messi, diciamo, scritti in modo ordinato piuttosto che con calligrafia comprensibile.
> Quindi stiamo passando da "ok, riuscire a riconoscere il testo" a comprendere il testo magari su un formato, un documento di carta completamente magari rovinato piuttosto che con impaginazione assolutamente... mi verrebbe da dire incasinata, con grafia particolare eccetera e da lÃ¬ produrre una tabella di un Excel o qualcosa del genere.
> Un altro esempio si faceva vedere un manoscritto con delle note di un professore universitario e il tool capisce e produce il LaTeX del testo, diciamo, delle espressioni matematiche nel documento. Anche qui tanta roba.

## [44:24] Reasoning Visuale e Occhiali Google

**Alessio Soldano**
> Eh, poi si diceva reasoning, reasoning anche perchÃ© questi modelli servono, di nuovo in ottica robotica per capire dall'immagine, per dire se dobbiamo spostare degli oggetti ritratti in un'immagine presa da un sensore, da un insomma da una telecamera, comprendere la disposizione tridimensionale degli oggetti serve per capire quale movimento dovrÃ  fare, ad esempio, un braccio robotico per spostare un oggetto evitando gli ostacoli, nel insomma in modo da non romperlo, eccetera.

**Stefano Maestri**
> Eh sÃ¬, VLA, insomma, alla fine.

**Alessio Soldano**
> SÃ¬, esatto. E questa cosa viene anche associata a un, diciamo, un ragionamento su, per dire, ho degli oggetti, li devo riposizionare al loro posto e la comprensione dell'immagine serve anche per capire dove vanno gli oggetti, cioÃ¨ si passa da "metti le palline rosse nella scatola rossa" a "hai questi tre oggetti, sono un bicchiere, delle posate e qualcos'altro e mettili a posto". E sullo sfondo si vede un armadio con oggetti simili, ma non necessariamente uguali e il modello comprende dove potranno essere riposti nella credenza, nell'armadio, le cose.

**Stefano Maestri**
> E poi sai che tutta sta roba non serve solo per la robotica, ma anche per gli wearable, perchÃ© se immaginiamo gli occhiali che ti dicono qualcosa di quello che stai vedendo, devi poter riconoscere quello che stai vedendo.
> PerchÃ© tra l'altro questa settimana proprio Google ha annunciato che nel 2026 lanciano gli occhiali anche loro e prima che tutti quanti pensiate "e ma l'avevan giÃ  fatto e poi sono falliti con i Google Glasses e tutto", stavolta sono un po' piÃ¹ lungimiranti nel senso che hanno detto non li facciamo noi, noi facciamo la parte software e il modello che siamo capaci a farli. L'hardware lo fa Samsung basato su Android XR, la nuova versione di Android Augmented Reality e gli occhiali li fa Warby Parker.
> Ãˆ piÃ¹ credibile, cosÃ¬ tanto piÃ¹ credibile che Essilor Luxottica quel giorno lÃ¬ ha perso il 12% in borsa.

**Alessio Soldano**
> Beh, ma per darti l'idea della credibilitÃ , come sapete, vedete, io ho gli occhiali e giusto in queste settimane sono in mezzo a 2000 peripezie per riuscire a ottenere degli occhiali che vadano bene, perchÃ© quando uno diventa sia presbite che miope Ã¨ un problema.
> Eh, parlavo con l'ottico di fiducia del momento che mi diceva che insomma il mondo sta cambiando senza che io gli avessi chiesto particolari cose, no? E lui stesso mi ha citato l'intelligenza artificiale dicendo che nei convegni tra, diciamo, addetti ai lavori si parla giÃ  di come cambierÃ  il mondo della produzione degli occhiali da qui a 1, 2, 3 anni.
> Nel senso che anche proprio tutta l'idea di avere lenti che facciano correzione di difetti visivi in un certo modo cambia, perchÃ© quando il software ti permette di cambiare la... di avere un qualcosa proiettato sul diciamo sulla lente piuttosto che di fare una correzione anche via software vuoi... tutta l'idea degli occhiali come li abbiamo adesso evolve.

**Stefano Maestri**
> Ãˆ come quando passi dal fatto di che per fare una foto di un certo tipo senza scomodare l'arte, che Ã¨ un altro discorso, ma fare una foto di buona qualitÃ  10 anni fa, 15 anni fa, comunque avevi bisogno di lenti Carl Zeiss di un certo tipo. Adesso con delle lenti di plastica, perchÃ© sono di plastica che ci sono sul su certi telefonini e tanto software dopo viene una foto che di nuovo non Ã¨ quella lÃ , perÃ² all'occhio non allenato...

**Alessio Soldano**
> ...per il 99% degli occhi Ã¨ la stessa se non meglio. Che sÃ¬.

## [47:26] Gemini su Google Home e Translate

**Paolo Antinori**
> Scusatemi, tutta questa AI applicata mi ha fatto venire in mente che ho anche io ho qualche storia di AI applicata questa settimana, di cui probabilmente potrÃ² parlarvene meglio la prossima volta, ma vale la pena menzionarlo oggi.
> Ieri sera ero... prima di andare a dormire stavo aggiornando le app del telefono di Android, cosa che faccio piÃ¹ o meno ogni settimana e mi ha stupito il fatto che ieri sera le app da aggiornare erano 44 e ho detto "La Madonna". Quando sono cosÃ¬ tante il primo pensiero da addetto ai lavori Ã¨ c'era un buco di sicurezza core e che lo devono patchare tutti quanti e non ce l'hanno ancora raccontato, ce lo raccontano settimana prossima, ma il fix Ã¨ stasera.
> E ho detto boh, magari Ã¨ quello, perÃ² poi ho detto ma magari invece visto che adesso i tempi sono cambiati, magari non Ã¨ per forza buco di sicurezza, ma magari Ã¨ qualcosa legato all'AI, perchÃ© c'era quasi tutta la suite di Google in aggiornamento.
> Allora, ho provato ad aprire un'app a caso, Google Home, e ho detto... anche perchÃ© continuavo a leggere sul web che il rollout di Gemini su Google Home Ã¨ giÃ  iniziato, dovresti dovrebbero averlo tutti, a me nessuno mi ha detto niente, quindi ho aperto e mi sono accorto che c'era un pulsante che forse era lÃ¬ da mesi perchÃ© non l'avevo mai visto, in cui se clicchi sul microfonino in alto a sinistra c'Ã¨ un bordino arcobaleno, non si nota particolarmente bene, che suggeriva essere Gemini. Ho detto "uh Ã¨ arrivato", c'ho cliccato sopra e ho scoperto che voleva da me che accettassi l'uso di Gemini, quindi probabilmente quella feature era lÃ¬ che aspettava da due mesi, ma nessuno si Ã¨ mai preso la briga di dirmelo.
> Quindi io ieri sera ho attivato Gemini su Google Home, non l'ho ancora provato, mi sono ricordato solo adesso di averlo fatto ieri sera mentre l'ho letto, quindi non ho idea di come si comporti. Settimana prossima ve lo racconto. Questa Ã¨ una legato alla mia curiositÃ .

**Stefano Maestri**
> Paolo hai anche dispositivi Google Home?

**Paolo Antinori**
> Ho un Google Home singolo che principalmente uso per far suonare il telefono quando lo perdo in casa e perchÃ© trovo che non mi ha sempre dato la sensazione di non essere particolarmente piÃ¹ intelligente di Alexa e quindi sono piÃ¹ orientato da Alexa.

**Alessio Soldano**
> Ma quale dei dei vari sei? Il nest? Quello con lo schermino?

**Stefano Maestri**
> SÃ¬, un Nest Mini.

**Alessio Soldano**
> SÃ¬, un uno di quelli a forma di disco cosÃ¬. E vabbÃ¨.

**Paolo Antinori**
> E quindi vi saprÃ² dire perÃ² di piÃ¹ da stasera in poi che lo uso un po' di piÃ¹.
> L'altra cosa che stavo inseguendo Ã¨ che, peraltro, mi Ã¨ capitato di commentare in un post su LinkedIn ad Alberto Danese che parlava di altro. Stavo leggevo che Google Translate ha introdotto una nuova modalitÃ  che vorrebbe rubare un po' di mercato Duolingo. Che cos'Ã¨? Ãˆ la modalitÃ  learning personalizzato e anche lÃ¬ l'ho cercata l'altro ieri, non c'era una mazza. Ho detto "VabbÃ¨, non c'Ã¨".
> Ieri sera, dopo aver aggiornato tutto il telefono, ho trovato che c'era l'aggiornamento, c'era scritto "Uh, c'Ã¨ il profilo learning". Anche qui non l'ho provato perchÃ© era tardi, non potevo parlare, ma voglio provare a vedere se Google Translate adesso riesce a ad aiutarmi a imparare in inglese in una maniera alla Duolingo e sono curioso.

## [49:43] Fare il pane con l'AI

**Paolo Antinori**
> Terza cosa, questo adesso piÃ¹ per farvi sorridere, forse andava bene in chiusura, ma ve lo racconto adesso, sennÃ² mi dimentico. Mi Ã¨ capitato di usare in famiglia l'AI per uno scopo pratico questo weekend.
> Avevamo il pane nel forno e dovevamo andare via perchÃ© avevamo un impegno, altrimenti non ce la facevamo e quindi dovevamo decidere cosa facciamo. Spegniamo prima, non spegniamo prima, chi lo sa? E mia moglie mi fa: "Prova a chiedere a a Gemini che cosa ne pensa".
> E allora gli abbiamo detto, "Senti Gemini, sto cuocendo il pane e Ã¨ dentro a questa temperatura da questo momento, perÃ² io devo uscire e quindi vorrei che tu mi dessi delle indicazioni su quando impostare lo spegnimento automatico del forno che si puÃ² fare". PerÃ² il timore era che lasciando il pane nel forno, ma non potendolo aprire, il pane andasse avanti nella cottura man mano che si si rinfrescava il forno.
> E Gemini ha effettivamente interpretato questa situazione dicendo "Oh, perÃ² sÃ¬, se lasci il pane nel forno rischi che ti si brucia, in particolare ti si secca. Fammi fare due o tre calcoli." Quindi il forno Ã¨ ben coibentato, hai detto questa temperatura, bla bla bla. Se il pane Ã¨ grosso cosÃ¬ tienilo tra i 10-12 minuti, se Ã¨ grosso cosÃ  tienilo in quest'altra cosa qua.
> Mia moglie fa "era piÃ¹ o meno quello che mi aspettavo". L'abbiamo fatto, era uguale al solito, cioÃ¨ se c'erano delle leggere differenze non abbastanza da notarle e non Ã¨ andata a fuoco la casa.

**Alessio Soldano**
> Quindi ecco, siccome siamo green, verrebbe da dire la prossima volta lo fai on purpose, cosÃ¬ lo fai volutamente cosÃ¬, spegni il forno prima e usi il calore latente per finire la cottura.

**Paolo Antinori**
> Guarda, e l'esperienza Ã¨ stata super positiva. Se non c'Ã¨ il rischio dell'allucinazione sempre dietro un angolo, per cui rischi di dare fuoco a casa tua, lo farei molto piÃ¹ rilassatamente.

**Stefano Maestri**
> VabbÃ¨, Ã¨ super super interessante questo, visto che io faccio spesso il pane. Oddio, in questo periodo sto facendo i panettoni e con tutto il bene non mi fiderei troppo a cambiare neanche di una virgola qualunque cosa io faccia con i panettoni, tecnica affinata in anni, perÃ² vabbÃ¨, questo Ã¨ un altro discorso.

**Alessio Soldano**
> Scusa e sempre perchÃ© poi dopo uno parte la la le idee, ma a questo punto stiamo anche dicendo che magari un forno piuttosto che un piano cottura induzione AI infused del futuro, tu gli dici cosa stai cucinando e poi gestisce lui l'accensione, lo spegnimento, eccetera.

**Paolo Antinori**
> Ragionevole, i microonde credo che giÃ  ci siano su questo, no?

**Stefano Maestri**
> Ma anche i forni in teoria, eh, tipo, non facciamo pubblicitÃ  marche, ma una roba che si chiama sesto senso, fa quella roba lÃ¬. In teoria io non l'ho neanche mai comprato perchÃ© non non mi fidavo, perÃ² forse mo va meglio, puÃ² essere.
> Poi tu mi insegni che esistono anche i termostati giÃ  che fanno quelle cosa lÃ¬ in casa, ma poi in realtÃ  non Ã¨ vero che consumi meno e c'Ã¨ tutto una teoria.

**Paolo Antinori**
> Vi aggiungo una piccola pillola basata sul mio di elettrodomestico che non Ã¨ particolarmente intelligente, perÃ² per ricordare che un po' di intelligenza c'Ã¨: la mia asciugatrice prima di partire fa tre mezze rotazioni e facendo cosÃ¬ pesa il contenuto del cestello e pesandolo sa stimare quanto deve essere lungo il lavaggio.

**Stefano Maestri**
> SÃ¬, mio lo fa la lavatrice e poi tutte le asciugatrici hanno un sensore di umiditÃ  per decidere quando la roba Ã¨ asciutta, quindi un po' di intelligenza c'Ã¨.

**Alessio Soldano**
> Poi dopo comunque tra i nostri follower c'Ã¨ un mio amico che lavora in quell'azienda lÃ¬ che dici tu che ha sedi qui vicino dove abito io e che e quindi vediamo se ci fa sapere qualcosa.

**Paolo Antinori**
> Ma perchÃ© non possiamo nominarlo? Sono cosÃ¬ malvagi che ci bucano le gomme della macchina. Mi briccano la lavatrice da remoto.

**Stefano Maestri**
> No, perchÃ© non ci pagano. No, perchÃ© questo Ã¨ il famoso momento Mastrota, quello in cui tu dovresti entrare e dire "no, perchÃ© se volete che vi nominiamo dovete essere sponsor della puntata". No, scherzi a parte, no, invitiamolo il tuo amico che lavora nell'azienda. Ah, sembra Mastrota sicuro, eh, perÃ² vabbÃ¨. E gli facciamo fare a lui, mettete stelline, campanelline, bam, e lÃ¬ spacchiamo.

**Paolo Antinori**
> Secondo me Mastrota viene se lo invitiamo. Scusami, a proposito di, visto che abbiamo smesso di essere seri da tempo, prima raccontavi un punto di vista da Gen Z.
> Mi hai fatto rendere conto che noi non abbiamo avuto particolari Gen Z ospiti e la persona piÃ¹ giovane che abbiamo avuto penso sia stata Veronica e che per lei era molto nerd dell'ingegneria e quindi non so se sia rappresentativa del Gen Z che becchi in giro a scuola, diciamo, normalmente.

**Stefano Maestri**
> Neanche, neanche. Al massimo una millennial Veronica. Adesso io non chiedo l'etÃ  alle donne, perÃ² sÃ¬. Gen Z Ã¨ mia figlia.

**Paolo Antinori**
> Credo che sia sÃ¬, sÃ¬. Eh, adesso non non mi metto a invitare tua figlia, ma tendenzialmente qualcuno che possa rappresentare la generazione di tua figlia potrebbe essere interessante da avere a raccontarci la loro visione.

**Stefano Maestri**
> Io ce l'avrei uno in mente che magari ci ascolta pure, che Ã¨ un amico di mia figlia che che perÃ² che che mi aveva sentito parlare di queste cose a scuole che fa adesso ingegneria informatica. Insomma, Edoardo ha capito se che sto parlando di lui nel caso ci ascolti. Se vuoi venire volentieri.

**Paolo Antinori**
> Ha ragione Paolo, sarebbe bello avere un ospite generazione Z o studente dai primi anni comunque dell'universitÃ  per capire quello che vedete voi succedere.

**Stefano Maestri**
> Eh, mi piace l'idea molto.

## [56:51] Claude Code e integrazione Slack

**Stefano Maestri**
> Detta questa cosa qua, io invece volevo dirvi una cosa, no, volevo volevo chiedervi una roba. Abbiamo parlato di due cose, in realtÃ . Una Paolo, che Ã¨ il nostro piÃ¹ grande fan di Slack. No, ma avete visto, no, a parte la battuta, avete visto che Claude Code ha annunciato che si integra a Slack. Io c'ho dei mixed feeling su quella roba lÃ¬, ma io non sono un grande fan di Slack, quindi non non valgo.

**Paolo Antinori**
> Allora la mia reazione Ã¨ stata MB nella versione meno politically correct di MB e perchÃ© mi viene da dire, cioÃ¨ che potrebbe essere un tutorial di qualunque stack software come integrarti con Slack, cioÃ¨ non ti serve una multinazionale che lo fa per te, quindi dici vabbÃ¨. PerÃ² eh evidentemente non c'ero.
> Comunque era probabilmente un discorso di soldi. Quanti me li dai a me di soldi per farti fare questo? Quanti te ne do io per farlo? Quindi c'era questa parte qua.
> La parte invece che io trovo interessante da schiavo del sistema che lavora per grosse societÃ  con Procurement e IT Ã¨ che quando metti un'integrazione cosÃ¬ forte di default nel catalogo di quelle che possono essere attivate Ã¨ forse piÃ¹ facile che il tuo reparto IT dica "Ah, questo ha giÃ  passato tutte le varie autenticazioni di sicurezza FIPS Fed RAMP, you name it riguardo la privacy e quindi Ã¨ soltanto un flag e possiamo abilitarlo molto piÃ¹ facilmente" anzichÃ© normalmente se chiedi di integrare due sistemi ti dicono "No, mio Dio, non si puÃ² passare i dati".
> Quindi quello l'ho trovato molto interessante, cioÃ¨ rendere permeante questa integrazione e quindi molto piÃ¹ accessibile. Altrimenti, come vi dicevo, l'MCP Server di Slack conosco la gente che ce l'ha installato da 3 anni.

**Stefano Maestri**
> Eh no, perÃ² sai cos'Ã¨? La io invece quello che c'ho visto Ã¨ la risposta di Anthropic a OpenAI con le chat di gruppo di chat GPT, perchÃ© qui l'annuncio Ã¨ che Claude diventa uno dei membri del tuo Slack e lo puoi aggiungere ad un canale. E infatti nell'esempio che fanno vedere loro Ã¨ il team che parla del tal baco eccetera. A un certo punto Claude si inserisce e dice "No, avete capito un cazzo, non Ã¨ questo il problema vero della issue e ve lo spiego io che sono piÃ¹ intelligente."

**Paolo Antinori**
> Guarda, rimango che non lo so perchÃ© questo caso d'uso Ã¨ stato uno dei primi a essere implementato da chi sapeva come farlo, quindi io l'ho sempre incrociato in giro questa cosa. Magari possiamo parlare che adesso la UX Ã¨ migliorata, ovvero se prima dovevo saper scrivere un programma, adesso mi basta invitare un account e ho la feature. Quindi quello sÃ¬, ha il suo impatto, perÃ² non lo so, il primo pensiero mi va proprio alla privacy, piÃ¹ a gente che mi dice "SÃ¬, sÃ¬, si puÃ² fare ma non farlo".

## [59:26] Waymo: Espansione e Business

**Stefano Maestri**
> Ok. E invece l'altra roba che non Ã¨ per niente collegata a questa, ma siccome abbiamo parlato a lungo di Tesla che guida da sola eh e tutto, Waymo invece ci crede ci crede forte, nel senso che sta diventando addirittura qualcuno dice il business principale di Google da qui al 2030 che a me sembra era un po' tirata, perÃ² va bene che di macchine ce ne in giro tante...

**Alessio Soldano**
> PerÃ² ho sentito dire anche da gente che stimo molto...

**Stefano Maestri**
> ...perÃ² bah, mi sembra un po' un po' tiratina dal il principale business di Google, perÃ² sicuramente un business significativo, no?

**Alessio Soldano**
> Anche perchÃ© il business di Google non Ã¨ esattamente noccioline. La news comunque Ã¨ interessante perchÃ© c'Ã¨ questo fondo di investimento Tiger Global che dice sostanzialmente che continua a investire su su Waymo e afferma che attualmente fanno tipo 450.000 viaggi per settimana.

**Stefano Maestri**
> VabbÃ¨, ricordiamolo, Waymo sono macchine a guida autonoma in America soltanto che vabbÃ¨ hanno un sistema differente da quello di Tesla, hanno molti piÃ¹ sensori eccetera.

**Alessio Soldano**
> Solo taxi, precisiamo, sono macchine a guida autonoma, ma solo taxi, solo per... Sono un servizio pubblico sostanzialmente, invece di chiamare un taxi, uno chiama un Waymo.

**Stefano Maestri**
> Sono macchine di varie marche vestite con la roba di Waymo. Anche questa Ã¨ la differenza. CioÃ¨, non Ã¨ una macchina nata per, Ã¨ una macchina che Ã¨ stata empowered.

**Alessio Soldano**
> Esatto. E la news da un punto di vista tecnico interessante del momento Ã¨ che sostanzialmente questi dicono "VabbÃ¨, abbiamo girato per le cittÃ , per le metropoli americane, adesso iniziamo a andare anche nelle freeway" che credo sostanzialmente siano l'equivalente delle nostre superstrade, che Ã¨ uno step interessante, allarga ulteriormente il business.

**Stefano Maestri**
> E poi c'era anche l'annuncio non collegato che l'anno prossimo debuttano a Londra e Parigi, se ho capito bene.

**Alessio Soldano**
> Waymo? Eh, qua me lo sono perso, onestamente.

**Stefano Maestri**
> SÃ¬, Ã¨ un'altra cosa che ho letto in settimana. Sono abbastanza sicuro. Adesso non ho il riferimento sottomano, ma sono abbastanza sicuro.

**Alessio Soldano**
> E no, vabbÃ¨, comunque tanta roba, eh, perchÃ©...

**Stefano Maestri**
> Ah, no, no, sicuramente no. E poi Ã¨ comunque un altro avvicinamento al mondo della robotica, perchÃ© comunque alla fine una macchina guida autonoma Ã¨ un robot su quattro ruote che fa cose, non Ã¨ che...
> Allora, quindi a questo punto siamo all'ora, noi di solito chiudiamo l'ora e 10, ma salutiamo giÃ  gli ascoltatori non super nerd e super tecnici. Ãˆ stato un piacere, vi salutiamo qua perchÃ© mo parto con una cosa su cui sono sicuro che quando guarderÃ² il grafico va giÃ¹ a picco e se ne teniamo due di ascoltatori che sono io che quando ci riascolto e un altro matto.

## [01:03:36] Ricerca: Oltre i Transformer (Titans, Nested Learning)

**Stefano Maestri**
> Allora, quindi grazie intanto per essere stati qua, gli ascoltatori non tecnici e partiamo con l'ultima l'ultima cosa che abbiamo in scaletta, che sono un paio di ricerche, tre in realtÃ , molto verticali sul mondo dei modelli.
> Mondo dei modelli. Allora, abbiamo nominato Ilya Sutskever o come cavolo si pronuncia e tutti gli altri che stanno cominciando a dire "Ho sentito un podcast anche di quelli di Sakana AI, quelli giapponesi..."

**Paolo Antinori**
> Scusami, ti faccio vedere un'immagine di tutti gli altri che Ã¨ diventata popolare questa settimana, non so se l'avete vista. Questa Ã¨ la copertina della persona dell'anno di Times che Ã¨ stata dedicata e sia all'AI, ma hanno fatto anche loro stessi questo montaggio, il sito editimes.com con loro sulla famosa foto degli operai che costruiscono l'Empire State Building o qualche era.

**Stefano Maestri**
> Carino, carino, carino. Eh no, e comunque tutti quanti stanno cominciando a dire bello, bello, bello, bello i Transformer. La legge di scalabilitÃ , una figata. Facciamo Chat GPT 5.2, facciamo dei benchmark superlativi, ma... il ma Ã¨ fino la legge di scalabilitÃ  che dice sostanzialmente piÃ¹ risorse miglior risultato se lavori bene. Interessante per il business, interessante per tanti motivi, ma se vogliamo davvero raggiungere quella che abbiamo chiamato general intelligence o super intelligence non basta, serve qualcosa di piÃ¹.
> Da un lato ci sono quelli che insistono tantissimo sul i modelli hanno bisogno di un'esperienza diversa e quindi World Model e sto pensando a quello ex meta, aiutatemi, quello francese LeCun e Yann LeCun che se ne va per fondare un'azienda basata su World Model.
> Sto pensando a Demis Hassabis che crede moltissimo a tutta questa parte e i Gemini si vede che credo tantissimo in tutta questa parte, ma c'Ã¨ anche qualche voce un pochino piÃ¹ fuori dal coro se vogliamo. Vedi Ilya Sutskever e questi qui di Sakana e altri che dicono, ma anche lo stesso Yann LeCun l'ha detto chiare lettere, bisogna andare oltre il Transformer.
> Ricordiamo che il Transformer Ã¨ l'architettura in questo momento piÃ¹ usata, che non Ã¨ altro che una serie di reti neurali messe in maniera tali per avere attenzione sulle parole e si mima il linguaggio umano e si utilizza il linguaggio umano come forma di pensiero. Adesso questo veramente con grandissima sintesi.
> Una delle cose che piÃ¹ si dice Ã¨ quello che manca a questi oggetti Ã¨ la capacitÃ  di avere memoria, cioÃ¨ la memoria Ã¨ solo quella di breve termine, il famoso context, abbiamo parlato tante volte di context engineer e questo quando il contesto Ã¨ basato di nuovo sul meccanismo di attenzione e man mano che il contesto si allarga il meccanismo di attenzione diventa sempre piÃ¹ costoso, ma anche meno efficace per certi versi, ma soprattutto non ha una memoria di lungo termine.
> Se noi guardiamo l'intelligenza che stiamo cercando di mimare, che Ã¨ quella umana, oltre al ragionamento tipico della corteccia prefrontale, esiste anche due grandissimi aspetti, ce ne sono molti a tre o quattro grandissimi aspetti. Uno Ã¨ quello della percezione e torniamo ai World Model eccetera eccetera, quindi vedere, toccare, gustare, annusare. Ci sono esperienze diverse dal parlato, evidentemente anche per noi.
> Un altro Ã¨ le emozioni, l'ippocampo e le emozioni che hanno una funzionalitÃ  comunque, cioÃ¨ la paura, il senso di paura o il senso di disgusto, il senso di rabbia, hanno delle funzioni per il pensiero, soprattutto quello istintivo. E su questo c'Ã¨ un sacco di ricerca, ma piÃ¹ neuroscientifica in realtÃ  che tecnologica.
> E l'ultimo Ã¨ la memoria a lungo termine. E sulla memoria a lungo termine Ã¨ un altro degli aspetti insieme alla parte di percezione su cui si sta cominciando ad investire molto e la stessa Google che ho nominato per la visione sta investendo molto su questo.
> Sono usciti due paper collegati tra loro che meritano, secondo me, una menzione anche qui in podcast. Poi non Ã¨ che ho la pretesa di raccontarli perchÃ© sono complicati. Da chi Ã¨ interessato e vuole se li va a leggere, si fa aiutare da notebook LM. Li ho citati anche in newsletter con qualche parola in piÃ¹. Eventualmente li spiegherÃ² meglio, ma in un qualcosa di scritto. PerÃ² mi piaceva citarveli qui al volo.
> Sono tre paper in realtÃ . Uno di qualche settimana fa che da cui parte un po' tutta questa cosa qui che si chiama Nested Learning. Il Nested Learning Ã¨ un'architettura diversa dai Transformer, interna ai Transformer, per un'ottimizzazione piÃ¹ spinta interna.
> Questa Ã¨ davvero complicatina da spiegare a parole, ma immaginatevi questa cosa: i perceptroni che sono sostanzialmente l'equivalente dai neuroni umani nelle macchine sono questi singoli punti dove noi facciamo ragionamento e nella versione piÃ¹ semplificata degli anni 70 erano accesi o spenti nÃ© piÃ¹ nÃ© meno. Adesso c'Ã¨ una funzione che si chiama ReLu di attivazione che Ã¨ piÃ¹ morbida, non Ã¨ esattamente acceso spento, ma alla fine ci avviciniamo molto lÃ¬.
> Semplificando tantissimo e chiedo perdono per chi ha letto la ricerca e sa che i dettagli sono diversi, l'idea Ã¨ quella di portare qualcosa che assomiglia ad un piccolo modello a livello del perceptrone, quindi non Ã¨ un'attivazione secca, sono acceso, sono spento, ma ho un micro-ragionamento all'interno del singolo perceptrone perchÃ© le ultime ricerche neuroscientifiche evidenziano come i nostri neuroni quella roba lÃ¬ in realtÃ  la facciano, che spiega perchÃ© cosÃ¬ pochi neuroni rispetto alla dimensione dei modelli in realtÃ  diano dei risultati eclatanti, eclatantemente migliori se paragoniamo dall'intelligenza umana. E questa Ã¨ la ricerca di partenza.
> Le ultime due uscite che sono collegate tra loro invece si chiamano Titans e Miras o Titans e Miras, non so come pronunciarli, se all'inglese o alla latina, che portano invece il discorso della memoria. Il discorso della memoria, questo lo spiego in maniera semplificata sempre, ma che secondo me lascia l'idea di che cosa parlano questi due paper. Poi vi lascio il link agli articoli di Google che sono piÃ¹ semplici da leggere che poi hanno al loro interno il link al paper vero e proprio.
> Allora, Titans cerca di mimare con una cosa che si chiama multilayer perceptron, che assomiglia un po' a quello che ho descritto prima, la memoria di lungo termine, ma la cosa che a me colpisce dal paper Ã¨ come fanno a decidere che cosa memorizzare all'interno del modello a lungo termine o no? E si basano sull'effetto sorpresa.
> CioÃ¨, se una cosa che va a finire in contesto ha un livello di sorpresa alto Ã¨ qualcosa che merita di essere memorizzato perchÃ© spezza un pattern noto. Ovvero stiamo dicendo che con il pretraining i modelli imparano pattern noti e quelli Ã¨ difficile che io riesca a migliorarli, ma quando con un lavoro di context engineering spingo i modelli a fare qualcosa di diverso, quello Ã¨ il momento in cui memorizzare e fare imparare qualcosa di nuovo al modello. I risultati nel paper sono ottimi in ambiente controllato, con modelli piccoli, come sempre nei paper. PerÃ² l'idea in sÃ© Ã¨ affascinante ed era quella che volevo trasmettere qui a voce.
> Il secondo invece Miras che Ã¨ collegato Ã¨ invece proprio piÃ¹ sul meccanismo di memorizzazione, e lÃ¬ si utilizza un'architettura diversa dai Transformer. Quella, leggetevela se volete, ma a me piaceva farvi vedere per chiudere questo grafico qua che trovate nel paper dove c'erÃ² la condivisione qua, share screen, questo grafico che fa vedere come il livello di perplexity, cioÃ¨ piÃ¹ Ã¨ alta la perplexity, piÃ¹ Ã¨ bassa l'accuratezza, in realtÃ  sia in grado di scendere molto meglio.
> Guardate le linee gialle, cioÃ¨ lasciate stare quella blu. Quella blu Ã¨ un modello normale in pratica modello normale. Al crescere dalla dimensione del contesto ha dei comportamenti divergenti, come vedete, migliora fino ad un certo punto, poi quando il contesto cresce troppo comincia a dare risposte a caso. Ed Ã¨ una cosa che chi ha usato cose con contesti veramente lunghi ha avuto esperienza.
> Quando dico che Ã¨ meglio far ripartire Claude Code da zero su ogni task che andare avanti tutto il giorno con un Claude Code aperto... Ã¨ esattamente quella roba lÃ¬, perchÃ© ad un certo punto, quando il contesto si riempie e diventa troppo lungo, anche senza andare oltre la dimensione del contesto, si cominciano a perdere dettagli.
> Invece questo i grafici sono quattro diversi grafici di utilizzo di diverso di memoria usando questo schema qua di Miras fanno vedere come alla peggio si stabilizzi non va mai in controtendenza l'accuratezza.
> E questo che cosa ci dice? Che probabilmente la prossima ricerca dal prossimo anno sarÃ  in con questo metodo o altri metodi avere contesti sempre piÃ¹ grandi utilizzabili. I contesti sempre piÃ¹ grandi utilizzabili potrebbero portare ad ulteriori miglioramenti dell'uso dei Transformer, perchÃ© i Transformer con contesti molto grandi fanno una cosa che in gergo viene chiamata in-context learning, quindi gli posso dare cosÃ¬ tante informazioni da spingere il modello oltre quello che giÃ  sa.
> Bene, adesso negli ultimi 3 minuti mi sono perso anche l'altro nerd che ascoltava e ci sono solo io che ascolto, quindi saluto me stesso, il me stesso del futuro. Bello.

**Paolo Antinori**
> Confermo che hai perso anche me.

**Alessio Soldano**
> Beh, il nerd si iscriva al canale.

**Stefano Maestri**
> Il nerd si iscrive al canale assolutamente. E comunque intanto io saluto il me stesso del futuro che Ã¨ interessante questa qui, no?

**Paolo Antinori**
> Eh, cercando di contribuire qualcosa di ragionevole quello che hai detto, avevo sentito interpretazioni di questi paper in altri podcast e c'era qualcuno che aggiungeva un altro elemento di metacognizione umana che potrebbe avere un ruolo in tutti questi ragionamenti. Ed Ã¨ quello per cui il nostro cervello umano, quello delle persone quantomeno neurotipiche perlopiÃ¹, ricorda dettagli fino a una certa soglia. Il famoso esperimento delle sette cifre. Dopo un po' lasciamo perdere. PerÃ² cosa significa? Significa che il nostro cervello butta via tutto quanto, no?
> Che tendenzialmente il nostro cervello con le informazioni che ha ricevuto e che sa che dimenticherÃ  le prende e le astrae, cioÃ¨ le riassume e si tiene neanche il riassunto, ma proprio i concetti essenziali di che cosa c'Ã¨ dietro. Qua si ricorda, che ne so, la struttura di una storia, il buono, e il cattivo, tutte queste cose qua che sono ad alto livello, poi i dettagli se li perde.
> Qualcuno ha proposto come questo possa essere un approccio naturale da andare a cercare, a replicare, quindi senza andare a inseguire il contesto e quindi quella curva che risale, che diverge, come facevi vedere tu, e magari Ã¨ quella la chiave e so che c'Ã¨ chi ci sta guardando.

**Stefano Maestri**
> SÃ¬, sÃ¬, sÃ¬. Eh, Ã¨ in parte contenuto nell'altro paper, il primo che ho nominato, quello chiamato Nested Learning. SÃ¬, sÃ¬, Ã¨ assolutamente un punto di vista super interessante anche quello che non Ã¨ fare la sintesi del contesto, Ã¨ un'altra cosa di un altro livello di astrazione perchÃ© dove peccano oggi quando vedete Claude sta comprimendo il vostro contesto, ecco, lÃ¬ Ã¨ l'inizio dei problemi di solito.
> PerchÃ© chiaramente fare un summary, fare un riassunto puramente testuale, come riassumeremmo un libro letto, perde dettagli che magari sono quelli significativi.
> La sfida Ã¨ tenere i dettagli significativi e buttare quelli superflui. Il famoso esperimento delle sette cifre, in cui tu dai una serie di numeri che hanno anche sette cifre, la maggior parte delle persone non si ricorda le sette cifre, ma sa dirti qual era la cosa che aveva un prezzo piÃ¹ alto. Quella lÃ¬ Ã¨ l'informazione da tenere, e non Ã¨ detto che facendo invece una summary pura basata sul testo questa roba funzioni, anzi non funziona.
> Bene, siamo oltre. Risaluto il me stesso del futuro e saluto tutti quei pochi che sono rimasti fino qua. Grazie. Ciao ciao ciao ciao.