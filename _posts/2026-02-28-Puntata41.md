---
title: "Claude Code, OpenClaw e l'Effetto Slot Machine: la nuova era dell'AI Engineering #41"
categories:
  - Puntate
tags:
  - AI
  - Agenti
  - coding
layout: single
author_profile: true
---

{% include video id="Vwl7stdsxj0" provider="youtube" %}

ðŸ‘‰ [Ascolta su Spotify](https://open.spotify.com/show/16dTKEEtKkIzhr1JJNMmSF?si=900902f2dca8442e)<br/>
ðŸ‘‰ [Guarda su YouTube](https://www.youtube.com/channel/UCYQgzIby7QHkXBonTWk-2Fg)<br/>
ðŸ‘‰ [Segui su LinkedIn](https://www.linkedin.com/company/risorseartificiali)<br/>



## **[00:00] Introduzione e Riflessioni Iniziali**

**Stefano**

> Buongiorno. Buongiorno a voi due. Buongiorno agli ascoltatori. Buongiorno alle ascoltatrici.

**Alessio**

> Ciao!

**Stefano**

> Sei uscito dal corpo di Antonello. Perfetto. Hai mantenuto solo i capelli piÃ¹ o meno di Antonello, che sei l'unico dei tre che ne ha seriamente. Io sono quello che ne ha di meno. Devo dire che raramente ci riascolto ultimamente. Mi sono riascoltato un pezzo stamattina perchÃ© mio figlio di 6 anni per farmi piacere mi ha detto che voleva sentire in macchina. Non Ã¨ vero niente, probabilmente, ma voleva farmi piacere, quindi lo ringraziamo. Ãˆ stata divertente la puntata. Chi non l'avesse sentita puÃ² andare, non perchÃ© non c'era Alessio, ma in generale. Paolo Ã¨ stato abbastanza cazzone, io anche e quindi avanti cosÃ¬, no? Allora, siamo partiti di nuovo.

**Alessio**

> Ma fammi capire, sei stato un'ora e 17, no, un'ora e 10 in macchina da ascoltarlo?

**Stefano**

> No, ho sentito i primi minuti. No, ho sentito i primi minuti e mi hanno fatto sorridere. Basta. No, no, solo i primi minuti.

**Alessio**

> O l'hai ascoltata a 7x?

**Paolo**

> Ma soprattutto hai schiacciato le campanelline, le stelline, quelle cose lÃ¬?

**Stefano**

> No, io le ho giÃ  schiacciate le campanelline e le stelline. Invitiamo gli altri ascoltatori. PerchÃ© intanto, guardando i dati, abbiamo un sacco di ascoltatori extra campanelline e stelline che chiameremo gli ascoltatori occasionali, cosÃ¬ come i rapporti occasionali. PerÃ² sempre per restare sullo stesso livello, tanto non ci ascoltano i bambini, a parte mio figlio. No, perÃ² mettetele ste stelline, ste campanelline, ci date una mano se siete lÃ¬ che ascoltate, perchÃ© lo vedo che il numero di ore non sono compatibili. Non mi vedo il nostro ascoltatore normale che ci ascolta 2, 3, 4, 5 volte. Sarebbe strano. Ãˆ vero che forse sono strani chi ci ascolta, perÃ².
>
> VabbÃ¨, questa era una marchetta venuta quasi simpatica, perÃ² bravi. Non l'abbiamo preparata, siamo solo cosÃ¬ scemi.


## **[02:30] Disagi e Riflessioni sull'AI**

**Stefano**

> Allora, no, un sacco di cose, ma parliamo prima di tutto dei disagi di Paolo.

**Paolo**

> Infame. Volevo io rompere il tuo flusso, invece tu l'hai rotto a me.

**Alessio**

> Era giÃ  rotto di partenza.

**Paolo**

> SÃ¬, Paolo ha qualche disagio. Ammetto di forse avere un problema e per fare riferimento a fare anche qualcosa di intelligente, ovvero fare riferimento a un popolare articolo che Ã¨ stato ripostato queste ultime settimane che parlava, intitolato AI Vampire, i vampiri dell'AI, che era a sua volta un riferimento a What We Do In The Shadows, per quelli che l'hanno visto, e Colin Robinson, il vampiro dell'energia, che Ã¨ un concetto meraviglioso. Dovresti guardare la prima puntata solo per quella cosa, fa riderissimo. L'idea Ã¨ che questa AI ci sta rubando un po' le energie, insomma, ci sta rubando un po' l'attenzione, il focus e tutte queste cose qua.
>
> Ãˆ strano perchÃ© leggevo di queste cose negli articoli che parlavano di San Francisco, in particolare con la visione un po' particolarmente negativa in cui si diceva che le startup piÃ¹ spinte, quelle che giÃ  prima lavoravano un sacco di ore, adesso gli si vuole fare lavorare ancora di piÃ¹ perchÃ© gli si dice "avete tutti questi agenti che fanno cose per voi", l'aspettativa Ã¨ altissima e quindi nei contesti piÃ¹ aggressivi, diciamo, l'ambiente Ã¨ decisamente stressante e tossico. A me viene in mente un pochettino Wolf of Wall Street con DiCaprio, quello stile lÃ¬, non faccio fatica a immaginarlo. PerÃ² ho detto "VabbÃ¨, ok, cioÃ¨ problema della Silicon Valley, problema di San Francisco, sti cazzi".

**Stefano**

> SÃ¬.


## **[04:27] PotenzialitÃ  e Limiti delle Tecnologie AI**

**Paolo**

> E insomma, diciamo che quando ho iniziato a rendermi conto delle potenzialitÃ  applicabili alla mia quotidianitÃ  di queste tecnologie, ovvero riassunto in una frase, letteralmente trasformare qualunque idea in una realizzazione da lÃ¬ a breve. Questo Ã¨ quello che Ã¨ per qualcuno che sa, ha delle aspettative, sa vagamente quello che sta facendo. Il tempo, quasi sÃ¬...

**Stefano**

> Quasi qualunque, quasi qualunque, perchÃ© l'audio di Linux non puoi risolverlo comunque, neanche con l'AI, quello non ci si riesce. Le virtualizzazioni leggere, lo so perchÃ© ho buttato via parte della mia vita questa settimana su quella cosa lÃ¬ e non la risolvi comunque. Per cui quasi ogni problema.

**Paolo**

> Ok, grazie per la correzione, ci sta tutta. PerÃ² insomma, no, il potenziale Ã¨ di dire come se avessi tre desideri da esprimere per un programmatore, in realtÃ  non ne hai tre, ne hai 3 miliardi e puoi farne quanti ne vuoi. Qual Ã¨ la risorsa che poi va a mancare a questo punto? Banalmente il tempo, nel senso che tu puoi fare tutte queste cose che prima rimandavi ad un weekend lungo, a quando la famiglia andava in vacanza e tu potevi concentrarti con calma e invece adesso tecnicamente puoi farla in qualunque momento.

**Stefano**

> Fai dal letto con Telegram, tanto per dire.

**Paolo**

> Stavo costruendo, non rompere la mia narrativa. Puoi fare in qualunque momento. Cosa ti serve? Ti serve, beh, probabilmente ti serve un abbonamento senza limiti di token, perchÃ© altrimenti a un certo punto il provider ti dice "nice try", ma hai speso tutti, sÃ¬, basta.
>
> E quello perÃ², il problema l'avevamo piÃ¹ o meno risolto grazie al fatto che Stefano lo pagano per vendere account di ZAI e quindi l'ha fatto comprare a me, ad Alessio. Io l'ho fatto comprare a un'altra persona, quindi questo Ponzi scheme sta funzionando.

**Alessio**

> SÃ¬, va bene, basta.

**Paolo**

> Io gli ho dato l'abbonamento annuale Max Ultra Giga, quello in 4K.

**Alessio**

> Io gli ho dato pochi pochi soldi, cioÃ¨ 10 euro ogni 3 mesi.

**Stefano**

> Che Minimax. Thank you.

**Paolo**

> Che comunque ha un prezzo contenuto, 250 euro totali, qualcosa del genere, insomma, che Ã¨ accettabile, e si sta ripagando tutto. CioÃ¨ io sto abusando di questa cosa quasi costantemente perchÃ© fa delle cose. Poi, come dicono, giusto per non dare false aspettative, ogni tanto Ã¨ giÃ¹ il servizio di ZAI, e loro stessi dicono "Noi vi diamo il servizio, costa poco, perÃ² gli SLA sono quelli che sono, quindi portate pazienza se ogni tanto siamo offline."

**Stefano**

> E sono 15 euro al mese alla fine, fatti i conti piÃ¹ o meno.

**Paolo**

> Ed Ã¨ ragionevole. Ãˆ ragionevole in termini di uno che offre il servizio e uno che lo paga poco. Diventa difficile ragionarci sopra quando sei nel bel mezzo di un ragionamento, di qualcosa che stavi facendo e finalmente eri arrivato al punto e boom, non ti risponde piÃ¹ l'AI remota e tu dici "Oh mio Dio, Ã¨ committata la transazione, le informazioni ci sono oppure no?"
>
> E lÃ¬ Ã¨ un po' di sudori freddi, Ã¨ di panico, che portano a un eventuale fork. O aspetti, oppure fai quello che ha fatto Stefano nel cuore della notte l'altro giorno. Cosa hai fatto?

**Stefano**

> Cosa ho fatto? Io non dico i miei peccati, perÃ². No, scherzo. Allora, no, GLM, siamo onesti, io l'ho sponsorizzato e continuo a sponsorizzarlo perchÃ© secondo me come modello Ã¨ notevole. Dopo ci arrivo a questo, perÃ² oggettivamente sÃ¬, ogni tanto Ã¨ giÃ¹ e non Ã¨ velocissima la risposta.
>
> Io che prendo in giro Paolo, ma che ultimamente lavoro con quattro finestre aperte tutte su Claude, per fare due cose per volta per progetto, ne ho di solito due progetti, due cose per volta. Questa lentezza mi dava fastidio e allora sono andato a farmi l'abbonamento di Minimax, Minimax M2.5, di cui si dice un gran bene come modello eccetera, ma poi su quello arrivo perchÃ© secondo me il GLM Ã¨ meglio, ma che ha un abbonamento normale e poi ha l'abbonamento super fast in cui invece loro hanno un SLA di 100 token per secondo. Quando l'ho visto non ho potuto resistere. 100 token per secondo mi sembrava la cosa dei miei sogni, piÃ¹ o meno.

**Paolo**

> La fibra ottica dei modelli.

**Stefano**

> La fibra ottica degli LLM e l'ho preso ed Ã¨ bellissimo, cioÃ¨ ad una velocitÃ  meravigliosa. Secondo me i risultati in senso stretto dal punto di vista del coding, ovviamente sono meglio quelli di Claude Opus, che dÃ  una mezza pista a tutti. GLM si avvicina di piÃ¹ ad Opus secondo me di quanto non lo faccia Minimax. Ãˆ comunque un buon modello che paragonerei perÃ² ad un Sonnet 4.5, perÃ² veloce in un modo incredibile per fare cose minime, anche di buon livello, va bene e dÃ  una velocitÃ  pazzesca.


## **[09:27] Gamification e Coding nell'Industria**

**Stefano**

> E qua viene il punto, perchÃ© tu dici giustamente, no, l'elemento tossico della Silicon Valley. Ma il fatto Ã¨ che per come sono fatti i coding agent, quelli da riga di comando soprattutto, ne parlavamo con Antonello l'ultima puntata di come la nostra generazione abbia portato la gamification all'interno dell'industria. Queste hanno portato la gamification all'interno del coding perchÃ© c'Ã¨ proprio il meccanismo di reward. Tu descrivi una cosa e la vedi succedere, magari non sempre esattamente, quindi hai voglia di fare l'improvement e sei sempre lÃ¬ a ciclare su questa cosa perchÃ© sai che arriverÃ .
>
> Mentre paradossalmente, quando scrivi codice di tuo, ma adesso per gli ascoltatori magari che non scrivono codice, magari PM o anche non tecnici, quando scrivi una mail ti devi mettere lÃ¬, pensare, o un documento, mettere lÃ¬, pensare. E quindi il tipo di reward, il tipo di soddisfazione che hai Ã¨ man mano che la mail si costruisce, Ã¨ un processo piÃ¹ lento.
>
> Qui la reward Ã¨ scriverne tante di mail, per fare il paragone con la mail, e quindi sei sempre lÃ¬ che cicli e diventa davvero difficile staccarsi, anche senza avere la pressione della Silicon Valley, nei side project che faccio la sera, che Ã¨ poi quello che diceva anche Paolo.

**Paolo**

> SÃ¬, sÃ¬, esatto. Ci sarebbe da decidere se vuoi citare o spoilerare il riferimento che arriva sempre dall'articolo di AI Vampires su come questo modello sembra che si avvicini a quello del gioco d'azzardo.

**Stefano**

> SÃ¬. Ne parlavo in pre-intervista con una persona che intervisterÃ² a breve, che me l'ha citato, me l'ha ricordato lui, quindi gli diamo la paternitÃ  di questa cosa. Di solito non spoilero le interviste, ma ormai siamo qua. Intervisteremo a breve Gabriele Venturi.

**Paolo**

> Io l'ho presa larga. Tu potevi uscire da questa trappola, invece ci sei cascato dentro.

**Stefano**

> No, tu mi tendi le trappole, io sono bravo a cadere nelle trappole, lo sai. E quindi l'hai fatto apposta. PerchÃ© dovete sapere che Paolo non Ã¨ d'accordo con me sul fatto che non spoilero le interviste. Allora, in tutti i modi sta cercando di convincermi del contrario, compreso l'inganno. PerÃ² io sono facile da ingannare, quindi. No, comunque mi ricordava questa cosa per tornare. Magari nessuno, immagino che il nostro ascoltatore medio sappia chi Ã¨ Gabriele Venturi, ma se non lo sapete ascoltate l'intervista che arriverÃ , non so dirvi quando ma arriverÃ .

**Alessio**

> Presto lo scoprirete.

**Paolo**

> Sono un whistleblower. Whistlebrother.


## **[12:34] Riferimenti al Gioco d'Azzardo e AI**

**Stefano**

> No, mi ricordava proprio questa cosa qua di cui poi abbiamo parlato in settimana. C'Ã¨ proprio un paper di ricerca su questa cosa, su quanto assomiglia al gioco d'azzardo, in particolare alle slot machine. PerchÃ© ci sono due meccanismi in questa cosa. Il primo Ã¨ quello del "schiaccio invio per andare avanti" che Ã¨ tipico proprio, no, delle slot machine, quando schiacci i bottoni per continuare. Quindi ti tiene collegato a questa cosa. E la seconda cosa Ã¨ proprio l'indeterminismo intrinseco dell'AI, per cui tu hai un risultato che perÃ² a volte ti soddisfa, a volte no, e hai sempre la volontÃ , anche quando ti soddisfa ma non pienamente, di migliorarlo, che Ã¨ un po' come vinco un po' di monetine e le rigioco perchÃ© spero di vincere di piÃ¹. Il meccanismo con cui viene fatto il paragone Ã¨ questo. Poi io non so se sono stati costruiti per questo motivo o se Ã¨ un effetto collaterale. Io voglio pensare che sia un effetto collaterale.

**Alessio**

> Sicuramente.

**Paolo**

> La teoria vorrebbe che Ã¨ un effetto collaterale perchÃ© se tu fossi bravo ad usare lo strumento vinceresti sempre. In realtÃ , quando stai facendo vibe coding puro su progetti come il mio in cui non padroneggio la tecnologia target, metÃ  dei miei prompt sono "mi sento fortunato".

**Alessio**

> Io non voglio essere autoreferenziale, ma vi dico che tutte queste cose sono la lampada di Aladino. Voi sapete perchÃ©. Tu descrivi l'idea che hai, ma volente o nolente ti scappa qualcosa e finisce che la lampada ti tira fuori la soluzione che perÃ² ha qualche difettuccio perchÃ© tu non sei stato bravissimo a descrivere il problema e poi devi lavorarci, diciamo.

**Stefano**

> E poi ti desideri di migliorare.

**Paolo**

> Bella, non l'avevo mai capito. Sai, forse l'avevi anche proposto per scegliere il nome di questo podcast a suo tempo, ma non l'avevo colto. Mi scuso.

**Stefano**

> No, perÃ² questa Ã¨ una notizia che non so se Paolo ha giÃ  letto, ma che... vai, dimmi. Finisci, scusa.


## **[15:08] Ottimizzazione e Automazione nel Coding**

**Paolo**

> Volevo finire l'excursus della scimmia e della tossicitÃ  di questa cosa perchÃ© si aggancia comunque a delle news che vale la pena citare. Stefano diceva che lui ha deciso di andare in direzione velocitÃ , non aveva tempo di aspettare. Io in realtÃ  il tempo di aspettare ce l'ho, nel senso che mi aiuta che ogni tanto vada lento cosÃ¬ posso smettere di concentrarmi su quella singola attivitÃ , staccare la testa dallo schermo e anzichÃ© vivere la mia vita, fare un'altra attivitÃ  sullo schermo. Mi aiuta, mi forza il multitasking, l'attesa praticamente, che Ã¨ una proprietÃ  piÃ¹ che un bug per quello che serve a me.
>
> Qual Ã¨ lo svantaggio di questa cosa perÃ²? Che alcuni dei task che lancio di programmazione, in particolare su questo progetto in cui di tanto in tanto devo lanciare dei benchmark per scoprire se troverÃ  mai una soluzione al mio problema oppure no, per la natura stessa di questi problemi NP-complessi, per cui non c'Ã¨ un algoritmo noto in un tempo finito, quindi devo lanciarlo e vedere come va e in particolare vedere se la soluzione migliora o se a un certo punto si incastra e non va piÃ¹ avanti di cosÃ¬. E sono job lenti. Molte volte passavo del tempo lÃ¬ a guardare, un po' come quando avevamo il 56k e scaricavamo gli MP3.

**Alessio**

> Ma perchÃ© sono delle euristiche.

**Paolo**

> SÃ¬, sÃ¬. E quando avevamo il 56k passavamo il tempo a guardare la barra di completamento che ti diceva "ci mancano 30 secondi, 50.000 anni, 30 secondi", quelle cose classiche di Windows. E niente, ed era tempo un po' perso e quindi ho cercato di capire cosa potevo fare per migliorare questa faccenda.
>
> Una delle prime ottimizzazioni, ma non Ã¨ quella di cui andrÃ² a parlare adesso, Ã¨ quella di rimuovere l'umano dal loop quanto piÃ¹ possibile. Quindi, anzichÃ© dover schiacciare quegli "yes" per dire "vai avanti, posso leggere questo file, posso provare questo", ho cercato di fornire al mio agente di coding locale dei sotto-strumenti che lui poteva utilizzare che erano giÃ  permessi da me, quindi non doveva chiedere il mio permesso. E quello ok, Ã¨ stato utile, quindi non ero piÃ¹ io il collo di bottiglia. Il collo di bottiglia perÃ² Ã¨ l'attivitÃ  stessa, quindi questi servizi lenti.
>
> Cosa ho potuto fare in questa direzione? Qui arrivano le storie di Stefano e del suo modello OpenClaw che gira e sta prendendo una laurea su internet o qualcosa del genere. La cosa di cui ero piÃ¹ invidioso era l'integrazione con Telegram, WhatsApp, non lo so, al punto che mi chiedevo "ma ne vale la pena installare il demonio sul computer per fare questa cosa oppure no?" E poi ho pensato "ma no dai, probabilmente posso semplicemente rubare la funzionalitÃ ". Vado da Gemini o da qualche modello e dico "senti, devo reimplementare la possibilitÃ  di controllare il mio Claude da remoto". PerÃ² visto che siamo nel 2026, non sarÃ² mica l'unico ad aver avuto questa idea. Ho guardato GitHub, pieno di progetti che fanno questa cosa. Me ne ha suggeriti 2-3, uno di questi tre sembrava quello minimale che faceva poche cose. L'ho guardato e ho detto "bene, lo installo". Poi mi sono fermato un istante, ho detto "forse non Ã¨ una buona idea comunque laddove non mi fido di OpenClaw, fidarmi di un altro random progetto su internet".
>
> Quindi che cosa ho fatto? Ho chiesto al modello stesso, a Claude stesso, "senti, stavo per installare sta roba, ma forse Ã¨ meglio che prima mi guardi se non fa delle cose losche". Quindi ho chiesto a Claude di dirmi se quel progetto era fidato. Lui ha fatto una piccola analisi e mi ha detto di sÃ¬. Siccome non gli voglio credere, gli ho chiesto di farne una piÃ¹ approfondita, quindi ho fatto un deep research. Lui l'ha fatto per bene, ha guardato, mi ha detto "sÃ¬, c'Ã¨ qualche libreria vecchia, ma normale, niente di strano". E soprattutto lo sviluppatore ha una presenza online, quindi in teoria Ã¨ una persona vera. It's not a robot or a troll, poi fino a un certo punto. Ora, questa non Ã¨ una garanzia che non ci siano problemi col progetto che ho scelto. Ad ogni modo, ho fatto le mie verifiche pigre, diciamo, per verificare che il progetto funzionasse.

**Alessio**

> Due diligence.


## **[20:14] Integrazione di AI e Strumenti di Lavoro**

**Paolo**

> A questo punto l'ho preso, l'ho lanciato e non sono riuscito a farlo andare. PerchÃ© non sono riuscito a farlo andare? PerchÃ© lui assumeva che io usassi Claude Code, quando io in realtÃ  nel mio setup non uso Claude Code ma uso un clone di Claude Code che mi tiene separata la configurazione. E quindi non andava. Che palle! E quindi che cosa fai? Quello che si fa di questi tempi. Chiedi a Claude Code, "senti Claude Code, mi fixi questa cosa? PerchÃ© io non uso Claude Code, uso una variante." Fa "certo, non c'Ã¨ problema." Me l'ha fixato e funzionava. Ho detto wow!
>
> A questo punto ho detto "cosa faccio, me lo tengo per me?" No, non tenermelo per me. "Claude Code, per favore, crea un task sul progetto originale da cui arrivo dicendogli che non funziona in questi setup." Lui me l'ha creato. Io ho rivisto la definizione dell'issue perchÃ© volevo evitare di sbattere in faccia un eventuale AI slop allo sviluppatore di questo progetto che mi Ã¨ stato utile. Quindi il minimo che potevo fare era verificare a mano di non dirgli cazzate e infatti Claude Code non aveva fatto un lavoro perfettissimo a spiegare il contesto, quindi ho corretto quello, perÃ² gli ho aperto l'issue. Dopo che gli ho aperto l'issue, gli ho aperto la PR. Gli ho aperto, ha fatto tutto Claude. Io sapevo che cosa volevo, lui ha fatto la fatica.
>
> PerÃ² pensateci, sono passato da una funzionalitÃ  che volevo, l'ho trovata, non funzionava. Il codice, la gente me l'ha fixata e ha fatto anche la parte di contribuirla indietro. La PR Ã¨ stata mergiata, peraltro, anche relativamente in fretta, ma io avrei vissuto anche se non succedeva mai con il mio fork.
>
> E niente, e quindi io adesso ho la possibilitÃ  di controllare Claude tramite Telegram. Sono contentissimo innanzitutto perchÃ© mi permette di poter mandare avanti le cose lente. Ogni tanto guardo, funziona molto meglio di quanto avrei mai potuto immaginare la UX. Davvero, pensavo che sarebbe stato impossibile, invece funziona. Ovviamente funziona meglio per alcune cose, tipo il mio progetto ha una parte di interfaccia web, quella parte ora come ora non la sto controllando. Non che non potrei, ma non sto esponendo il web a essere accessibile da remoto, quindi in realtÃ  non lo sto vedendo. Quindi funziona meglio per il backend, se volete, ma di per sÃ© posso farlo.
>
> E quindi io adesso, mentre aspetto mia figlia che esce a scuola o vado al bar, oltre che guardare e fare altre cose, guardo la chat di Telegram e vedo che c'Ã¨ il bot che ha finito questa roba e gli dico "senti, fai questo, fai quest'altro". E se alcune di queste attivitÃ  pensate che richiedono un'interazione attiva, Ã¨ vero, alcune, non tutte le attivitÃ  di sviluppo con Claude Code puoi fare dal telefono perchÃ© devi dargli un feedback, devi leggere attentamente, ma molte altre sÃ¬. Ricerca, verifica stessa della tua coda delle attivitÃ , per la quale ho iniziato ad usare Backlog.
>
> Stamattina ad esempio gli ho fatto backlog grooming di tutte le mie attivitÃ , una roba che di solito odio fare, l'ho fatta tranquillamente dal telefono mentre ero a letto. Gli ho detto "senti, prova a controllare se abbiamo delle cose che sono marcate come da fare ma in realtÃ  le abbiamo giÃ  fatte, ci siamo dimenticati". E lui mi ha fatto tutte queste cose, ha effettivamente risparmiato ore di stare davanti al laptop in maniera tradizionale per fare tutte queste attivitÃ .
>
> Io sono contentissimo di questa cosa. Sto spammando tutti i miei amici dicendo "guardate che si puÃ² fare questa roba, Ã¨ facile". Potrebbe essere l'inizio di un problema ancora piÃ¹ consistente di addiction, ma la comoditÃ  Ã¨ assolutamente lÃ¬. E perchÃ© vi faccio tutto sto pippone? PerchÃ© questa settimana Anthropic ha ufficialmente rilasciato la stessa funzionalitÃ .


## **[22:37] Rilascio di Nuove FunzionalitÃ  da Parte di Anthropic**

**Stefano**

> Via web, perÃ². Via web, non via chat.

**Paolo**

> Scusate, il sistema ha rilasciato la stessa funzionalitÃ , ha rilasciato la stessa UX, l'app mobile per fare sessioni long running di Claude Code. La versione fatta in Telegram Ã¨ piÃ¹ casereccia, open source, se volete. La versione loro Ã¨ piÃ¹ servizio che paghi, perÃ² diciamo che hanno validato la UX. Probabilmente il merito di questo va a Pete di OpenClaw che Ã¨ il primo che ci ha fatto vedere che poteva essere una buona idea.

**Stefano**

> No, no, perÃ² allora l'ho guardata, Ã¨ un po' diverso il concetto, nel senso che a me piace di piÃ¹ la versione tua, OpenClaw style, cioÃ¨ che si interfacci con il tuo Claude Code eccetera. Quella che hanno rilasciato in Anthropic Ã¨ molto simile a quello che fa Codex di OpenAI, quindi Ã¨ un'istanza web di Claude Code che si clona il tuo repo e vive su un AWS, su un cloud da qualche parte, ma non Ã¨ quella del tuo computer, che sia VPS che sia il computer vero. Mentre invece quello che fai tu Ã¨ quella cosa lÃ¬.
>
> E perchÃ© a me piace di piÃ¹? PerchÃ© volendo poi dopo lÃ¬ Ã¨ dove metti il taglio della tua sicurezza, gli dai accesso a tante cose sul tuo computer, i tuoi file eccetera eccetera. Infatti non escludo di provare a mettere la stessa cosa che hai tu e Claude Code sul VPS che ho, spegnendo OpenClaw un attimo.


## **[23:48] Sicurezza e Memoria nel Cloud**

**Stefano**

> PerÃ² Anthropic ha riconosciuto la validitÃ  dell'idea di OpenClaw decisamente perchÃ© ha aggiunto l'altra funzionalitÃ  ed era la notizia che non so se hai ancora letto perchÃ© Ã¨ di stanotte e credo che potrebbe farti molto piacere sapere che esiste. Hanno aggiunto la memoria di lungo termine su Claude Code, che Ã¨ esattamente l'altra cosa che ha OpenClaw.
>
> Si sono fatti delle belle pippe, nel senso che hanno fatto una cosa seria ed evoluta. Hanno messo insieme il concetto di Claude MD, rendendolo perÃ² gerarchico, per cui tu puoi avere un Claude MD per ogni componente del tuo progetto. Per cui ad esempio se hai una parte di interfaccia web e una parte di backend, puoi avere dei Claude MD diversi o che completano il Claude MD di base nello stile di programmazione, nel tipo di linguaggio che usi eccetera eccetera. Hanno aggiunto tutta una serie di rules, come le chiamano loro, che assomigliano moltissimo a quelle che erano le Cursor rules di Cursor. E in piÃ¹ hanno aggiunto il Memory MD dove Claude, esattamente come fa OpenClaw, durante la sessione capisce quali sono le cose rilevanti e se le sintetizza lui nel Memory MD.

**Paolo**

> Non ti voglio sputtanare, ma io sto usando da tre giorni sta roba, almeno. Non so che dirti perchÃ© Memory MD lo sto letteralmente usando da un po'. Rules...

**Stefano**

> L'annuncio su X di Anthropic Ã¨ di stanotte, 12 ore fa. No, no, Memory MD c'era giÃ  ma lo dovevi editare tu o gli dovevi dire di memorizzare con slash memory. Adesso c'Ã¨ l'auto memory che vuol dire che lui durante la sessione si accorge di che cosa stai facendo e si sintetizza le cose fondamentali da mettersi nel Memory MD. Ãˆ quello che hanno annunciato stanotte, l'auto memory.

**Paolo**

> Ok, perchÃ©...

**Stefano**

> Le rules, no, hai ragione. Rules e Memory MD Ã¨ una settimana, dieci giorni che le hanno introdotte.

**Paolo**

> Infatti le rules le sto usando da questa settimana e mi chiedevo "ma ci sono sempre state e non me le ero sempre perse?" perchÃ© sono comode.

**Stefano**

> No, no. Ãˆ dalla 2.158, Ã¨ dalla 2.159, che Ã¨ quella di stanotte, che c'Ã¨ l'auto memory.


## **[26:22] Innovazioni di Claude Code**

**Paolo**

> Ok. In compenso, problema concreto. Tutta questa memoria il contesto te lo mangia. Giusto ieri stavo guardando che io quando inizio un'attivitÃ  mi brucio un 25% di contesto tutte le volte e ho detto "perchÃ©?"

**Stefano**

> Quelle sono gli MCP anche.

**Paolo**

> SÃ¬, in teoria sÃ¬, perÃ² ho lanciato Context per vedere che cosa c'era dentro e ti fa lo spaccato. E il mio spaccato, adesso vabbÃ¨, una curiositÃ , lo citiamo in podcast, magari qualcun altro ci va dentro a guardare e scopre. Era che la maggior parte che sprecavo di contesto non erano gli MCP Tools, con mia sorpresa, ma erano la collezione di Markdown files che io ho creato, in particolare quelli sotto la cartella /Claude Docs, l'equivalente delle memorie a lungo termine dei poveri, prima che Anthropic ci desse questo nuovo meccanismo che ho scoperto questa settimana. A quanto pare sono sempre lÃ¬ e io alcune di quelle...

**Stefano**

> SÃ¬. Lui se li legge, i Claude Docs, lui se li rilegge tutte le volte. Mentre il Memory no, il Memory l'hanno impostato in maniera simile alle skill. Hanno un descriptor della memoria e si legge solo quello che serve quando serve.

**Paolo**

> Ãˆ solo che Ã¨ un'implementazione con un indice praticamente.

**Alessio**

> C'Ã¨ tipo che quando arriva in una situazione in cui si chiede cosa fare, dice "vediamo un po' qual Ã¨ la linea guida del progetto".

**Stefano**

> SÃ¬, corretto. Se ho qualcosa, se l'ho giÃ  fatta questa cosa in buona sostanza. PerchÃ© tipo si ricorda sessioni di debug, cose di questo genere. Tiene l'auto memory, quindi se trova bug simili o behavior simili cerca di non reinventarsi la ruota, ma di capire cosa ha fatto e che cosa ha funzionato.

**Alessio**

> Va a vedere alla fine tutto il ragionamento come ne Ã¨ uscito.

**Stefano**

> SÃ¬. L'esperienza di chi lo usa Ã¨ che magari gli chiedete di fissare una cosa, lui fa un tentativo, non va a buon fine, ne fa un altro, alla fine ci riesce. Invece di fare questi tre tentativi, sa giÃ  che il tentativo buono Ã¨ fatto in quel modo ed Ã¨ il primo che prova, quantomeno. Poi se non va in quella situazione ricomincia a tentare, ma...


## **[29:18] FunzionalitÃ  di OpenClaw**

**Stefano**

> E quindi quella lÃ¬ Ã¨ un'altra funzionalitÃ  tipica di OpenClaw. Quello che manca ma che Ã¨ facile da implementare Ã¨ di svegliarlo, di svegliarlo ogni tot da solo con un elenco di cose da fare. Tutto sommato con Backlog giÃ  piÃ¹ o meno lo potresti fare. CioÃ¨, tu metti un cron ogni 10 minuti e nel prompt gli fai i prossimi due task che hai di Backlog, lui lo fa e hai ottenuto piÃ¹ o meno OpenClaw.
>
> Poi dopo, un altro discorso sono le estensioni, tutte le skill che ti puoi scaricare. PerchÃ© in realtÃ  dove c'Ã¨ la parte di pericolo, di sicurezza soprattutto, Ã¨ nell'installare qualunque cosa. E OpenClaw per scelta, loro hanno fatto questa scelta, ha una grande facilitÃ  di estenderlo per le skill. Ha proprio un repository che si chiama ClawHub, di skill, di cose che puoi installare, e lo puoi fare direttamente lui se glielo chiedi, di auto-installarsi le cose. E questa Ã¨ la parte di pericolo.
>
> PerÃ² i componenti fondamentali che erano quei tre lÃ¬, l'interfaccia diciamo remota in qualche modo, la memoria e lo svegliarsi ogni tanto, 2 su 3 li hanno riconosciuti come "ok, lo facciamo anche noi", che Ã¨ in qualche modo una validazione che l'idea di base non era una minchiata.

**Paolo**

> Come vi annunciavo privatamente, nella mia to-do list il prossimo step Ã¨ da supportare i messaggi vocali perchÃ© ogni tanto mi accorgo che mi perdo un po' a scrivere, mi farebbe comodo lanciargli una nota breve a voce. Questa cosa il mio setup non ce l'ha. Probabilmente farÃ² girare un modello locale tipo Whisper o Parakeet per convertire e lo farÃ².
>
> E questo mi ha ricordato peraltro che uno dei tanti problemi personali che adesso il micro-software ci permette di risolvere Ã¨ che odiando i messaggi vocali di WhatsApp mi ha sempre dato noia che li devo ascoltare, non posso leggerli, e adesso lavorerÃ² a scrivermi un convertitore di messaggi vocali in testuali completamente privato, deployato. Probabilmente proverÃ² a usare i modelli Gemma di Android per farlo girare come app custom Android. Era la cosa a cui stavo lavorando stamattina al bar.

**Alessio**

> Avvisami quando lo fai.

**Stefano**

> SÃ¬, no, Ã¨ molto comodo effettivamente.


## **[31:43] Rilasci di Modelli e Aggiornamenti**

**Alessio**

> Adesso io perÃ² non vorrei dire, abbiamo un elenco di rilasci di modelli nuovi che sono usciti in queste settimane che per quanto faccia un po' la lista della spesa inizia ad essere imbarazzante da ignorare. Potremmo magari fare un excursus.

**Stefano**

> SÃ¬, sÃ¬. Io intanto ricordo a chi preferisce leggere che io quell'elenco lÃ¬ lo faccio in newsletter tutte le settimane, se volete. PerÃ² hai ragione, e partiamo dalla fine allora, visto che tu ci punzecchi e tu ti pigli la pagliuzza piÃ¹ corta. Ãˆ uscito Nano Banana 2.

**Alessio**

> Ecco, Ã¨ uscito Nano Banana 2 proprio a brevissimo. Io ho fatto qualche prova e per il momento posso solo dirvi che le immagini che genero sono molto belle.

**Paolo**

> Funzionava prima, funziona adesso, commento di quando Ã¨ uscito Gemini 3.

**Alessio**

> SÃ¬, esatto. Due commenti cosÃ¬. Uno di impressione molto personale: ho provato a generare qualche immagine di soggetti umani e la primissima sensazione cosÃ¬ a pelle Ã¨ "ma questo Ã¨ un Nano Banana 2 o una versione nuova di Grok?" PerÃ² vediamo, non so chi ha usato Grok, magari capisce cosa intendo.

**Stefano**

> Io non ho capito, spiegamelo.

**Paolo**

> Ce l'abbiamo nella macchina noi.

**Alessio**

> Ma nella macchina non genera le immagini comunque. No, vabbÃ¨, ma a parte quello. Era tanto che non uscivano aggiornamenti sulla generazione di immagini da parte di Google, da quando Ã¨ uscito il Nano Banana Pro. C'erano stati ultimamente degli altri rilasci di modelli open weight, quindi bene.

**Stefano**

> SÃ¬.

**Alessio**

> Si inserisce all'interno del rilascio piÃ¹ grosso di Gemini 3.1 Pro. Ribadisco, molto fresco, non l'ho ancora guardato bene. PerÃ² in realtÃ  quello di cui volevo parlare io, Stefano, era anche di tutti gli altri. Di GLM 5, di Quen 3.5, di Sonnet 4.6, GPT 5.3, Codex Sparks, Minimax 2.5, l'hai citato tu prima. Tutto questo per dire che intanto si nota una velocitÃ , cioÃ¨ Ã¨ aumentato il ritmo dei rilasci, se vuoi. Tra l'ultimo state of the art di ognuno dei vendor principali e quello successivo, la sensazione che i tempi tra un rilascio e l'altro si siano accorciati. Non so se anche tu hai questa sensazione.

**Stefano**

> SÃ¬, l'esponenziale rimane, sÃ¬.

**Alessio**

> E poi c'Ã¨ tutto un discorso di benchmarking. Ci ragionavo giusto l'altra sera. Inizia a diventare anche difficile capire come spiegare all'utente "ok, questo modello che Ã¨ uscito Ã¨ meglio di quello che c'era prima". Per i modelli open weight ho visto che la tendenza Ã¨ quella di dire "ok, questo modello rispetto allo state of the art dei modelli closed source si posiziona piÃ¹ o meno qua", come dire "siamo quasi, per dire, al livello di Opus 4.5 piuttosto che Opus 4.6". Invece sugli altri il problema Ã¨ che i benchmark non sempre sono significativi, ma non ti danno davvero l'idea di quanto sia migliorato un modello rispetto ai precedenti.

**Paolo**

> Scusami Alessio, era un po' quello che ci chiedevamo tra di noi in queste settimane quando era arrivato l'annuncio di Gemini 3.1. Che citiamo lui perchÃ© Ã¨ piÃ¹ facile che le persone l'abbiano incrociato essendo di Google, e ci chiedevamo "ma che cosa fa di diverso?" E Stefano ci spiegava e per quanto sia, adesso glielo faccio ripetere, ma per quanto sia interessante il dubbio era sempre "boh, ok, cioÃ¨ quanto mi impatta direttamente come persona?" Stefano, scusa, ricordaci cosa fa 3.1 rispetto a 3 che era giÃ  ottimissimo.


## **[36:44] Benchmarking e Performance dei Modelli**

**Stefano**

> Allora, 3 era giÃ  pazzesco e sui benchmark Ã¨ migliorato tantissimo, cioÃ¨ 3.1. Ne cito uno perchÃ© Ã¨ quello che mi ricordo, ARC-AGI 2, che Ã¨ un benchmark relativamente nuovo.

**Alessio**

> Ãˆ un benchmark relativamente nuovo, tra l'altro.

**Stefano**

> ARC-AGI 2, che dovrebbe testare la capacitÃ  AGI del modello, cioÃ¨ la capacitÃ  di essere meglio della media dell'uomo sulle varie attivitÃ . 3.0 era stato salutato come incredibile perchÃ© faceva il 48%, 46%, rispetto ad un ChatGPT che faceva 37%, per intenderci, no? Ecco, 3.1 fa 86%, che Ã¨ quasi il doppio.

**Paolo**

> Prima non era promosso, adesso Ã¨ promosso.

**Stefano**

> Tanto che, tanto che hanno dovuto fare ARC-AGI 3, perchÃ© cosÃ¬ non ha piÃ¹ senso, Ã¨ a tappo. Ed Ã¨ uscito ARC-AGI 3 e se vuoi, sulla velocitÃ  dei modelli che dicevamo prima con Alessio, Ã¨ incredibile anche, sono andato a vedere questo dato qua per preparare la puntata, la velocitÃ  di rilascio dei benchmark anche. CioÃ¨, non ci stanno dietro con i benchmark. Anche la velocitÃ  di rilascio dei benchmark Ã¨ accelerata, per forza, perchÃ© li mandano a tappo.

**Alessio**

> Che comunque devi pensare a dei test che siano sufficientemente challenging, ma non tra virgolette fuori dal mondo, perchÃ© deve essere come dire progressiva la capacitÃ  di passare, di migliorare eccetera.

**Stefano**

> E gli altri dati impressionanti, quelli li vediamo anche su Opus 4.6 e su Kimi K2.5, che Ã¨ uno degli altri da citare come rilasci, Ã¨ la capacitÃ  di andare multi-agente con il sub-agent nativo e fare compiti estremamente lunghi. CioÃ¨, Codex Ã¨ arrivato a 28 ore di compito svolto correttamente. Ti do il prompt, diciamo il contesto, "fai questa cosa" e 28 ore dopo Ã¨ arrivato con il risultato corretto, senza altra interazione umana.

**Alessio**

> SÃ¬.

**Stefano**

> Questo qui Ã¨ l'altro dato a cui si fa molta attenzione in questo momento, la capacitÃ  di svolgere compiti lunghi e complessi e magari di parallelizzare. Tipo, Kimi ha spinto tantissimo con il 2.5 su quella roba qua, hanno avuto un miglioramento pazzesco da quel punto di vista. E anche Minimax, anche se io non l'ho provato su Minimax.

**Alessio**

> I leitmotiv che ho visto in questo giro di rilasci sono: uno, la tendenza ad allinearsi su un nuovo standard di un milione di token come dimensione della finestra del contesto, che piÃ¹ o meno tutti, non tutti, perÃ² vabbÃ¨, Gemini 3.1 Pro, Quen 3.5, insomma, adesso Ã¨ la nuova...

**Stefano**

> Opus 4.6.

**Alessio**

> Esatto. Ãˆ il nuovo, diciamo, nuovo target: un milione di contesto. E poi l'altra cosa Ã¨ specializzazioni, reinforcement learning, training eccetera, specifici per il coding, quasi tutti. Che vabbÃ¨, l'abbiamo giÃ  detto altre volte, ci sta perchÃ© Ã¨ l'ambito all'interno del quale si stanno vedendo soprattutto i risultati, perchÃ© Ã¨ tra virgolette facile, perchÃ© Ã¨ ben verificabile eccetera, perÃ²...

**Stefano**

> Ed Ã¨ quello che ha piÃ¹ impatto anche.

**Alessio**

> Esatto. Anche perchÃ© ti serve per sviluppare nuovi modelli e quindi di conseguenza...


## **[41:04] Ottimizzazione e Inferenza Locale**

**Stefano**

> O nuovi software. CioÃ¨ quello che in questo momento ha piÃ¹ impatto perchÃ© essendo i modelli confinati nell'ambito virtuale, passatemi il termine, nel cloud eccetera, le due cose che hanno piÃ¹ impatto Ã¨ se riesci a migliorare il workflow di lavoro di una persona, ma ancora di piÃ¹ se riesci a scrivere codice o progetti effimeri anche che vadano in quella direzione.
>
> Infatti c'Ã¨ un post di CloudFlare su X di questi giorni, che adesso non ho sottomano ma l'ho letto, che riprende una vecchia idea di Hugging Face con un progetto che si chiama SmallAgent, con cui avevo giocato e contribuito un po' tempo fa. Ãˆ quello che invece di generare chiamate API o MCP, genera codice e lo fa eseguire al volo ai modelli. Questa cosa ovviamente Ã¨ interessante perchÃ© il codice Ã¨ piÃ¹ espressivo di una semplice API. Banalmente ci puoi mettere i for e gli if nel codice e concatenare piÃ¹ cose.
>
> PerÃ² ci dice anche che i modelli stanno diventando abbastanza maturi a generare il codice, almeno piccole porzioni di codice, in maniera cosÃ¬ affidabile che molti, anche Opus fa questa cosa, molti stanno cominciando a dire "va bene, l'estensione del modello oltre la reasoning Ã¨ auto-scriversi del codice per risolvere sotto-parti del problema in maniera deterministica". E questo potrebbe essere un ulteriore salto interessante.

**Alessio**

> Invece un'altra cosa che ho notato, che iniziano a vedersi anche dei tentativi per i modelli open weight, chiaramente, di esplicitare e di conseguenza tendere a ridurre quanto impatti la quantizzazione sui modelli o l'utilizzo di Mixture of Experts, diciamo un attimo, aggressivi, sul risultato finale, sulla qualitÃ  dei risultati ottenuti con l'inferenza.
>
> E mi riferisco ad esempio a Quen 3.5, che nel suo rilascio in realtÃ  ha rilasciato un gruppo di modelli, non solo uno, con dimensioni varie da 400 billion fino a scendere a 27, con Mixture of Expert diversi che scendono a 17, a 10, a 3 miliardi di esperti. E ha fatto i benchmark con tutte queste versioni, ha fatto vedere quanto perde il modello man mano che lo tagli e lo fai diventare piÃ¹ piccolino. Come di nuovo dimostrare che la frontiera Ã¨ anche nel cercare di ottimizzare la riduzione delle dimensioni per poter ottenere ancora dei risultati accettabili anche con risorse piÃ¹ basse.

**Stefano**

> PerchÃ© credo che gli open weight stiano scegliendo come target, a tendere man mano che i computer diventano piÃ¹ potenti, l'inferenza locale e quindi per loro Ã¨ interessante andare a ridurre le dimensioni. Mentre invece gli state of the art delle big tech al momento sono focalizzati e dichiaratamente a raggiungere l'AGI perchÃ© cosÃ¬ fanno scopa.

**Alessio**

> SÃ¬, sÃ¬, no matter what, esatto. SÃ¬, sÃ¬, assolutamente.

**Stefano**

> Lasciami fare una digressione tecnica che ha a che fare anche con questa cosa, ma che magari qualche utente si potrebbe chiedere e pensare che le big tech stiano cercando soltanto di fare piÃ¹ soldi con questa manovra. In realtÃ  ci sono delle giustificazioni. E qual Ã¨ la manovra? Scusate, non ho messo il soggetto. Vi sarete accorti che tutti vanno verso un milione di token, ma sopra i 200k i token costano molto di piÃ¹. CioÃ¨ hanno un pricing fino a che usi il contesto piccolo e c'Ã¨ un pricing diverso se usi il contesto grande e uno dice "vabbÃ¨, ok, vuoi farmi pagare di piÃ¹ perchÃ© voglio fare di piÃ¹?" SÃ¬, magari una parte di veritÃ  Ã¨ questa.

**Alessio**

> Beh, costa anche molto di piÃ¹ poi fare l'inferenza con i contesti pieni.


## **[46:44] La ComplessitÃ  dei Contesti Lunghi**

**Stefano**

> No, non solo costa di piÃ¹ fare l'inferenza con contesti pieni, ma Ã¨ proprio quello che volevo spiegare. Il contesto va a finire in una memoria, una cache che si chiama KV Cache. Ne abbiamo parlato nella puntata domande e risposte che abbiamo fatto tanto tempo fa, se volete andate a pescare. Qua dico soltanto in maniera intuitiva, cioÃ¨ il problema Ã¨ che la quantitÃ  di memoria utilizzata non scala in modo lineare alla quantitÃ  di token che metti nel contesto. Non Ã¨ neanche esponenziale, una curva un pochino piegata, diciamo.

**Paolo**

> Barzotta si dice.

**Stefano**

> PerchÃ© il problema della KV Cache, che cosa fa con i contesti lunghi? Deve mantenere il contesto, appunto, delle singole parole con quelle precedenti, ma siccome il collegamento delle parole correnti esplode non soltanto perchÃ© il contesto indietro Ã¨ piÃ¹ lungo, ma perchÃ© il contesto dietro Ã¨ piÃ¹ ramificato e potresti avere piÃ¹ collegamenti con un numero piÃ¹ alto di parole precedenti. PerchÃ© magari hai detto "ancora" e "ancora" ha il legame con un sacco di roba.
>
> Questa cosa qua fa scalare la dimensione della RAM in maniera non lineare, ma piÃ¹ che lineare, rispetto alla dimensione del contesto. Quindi per contesti grandi il costo anche per chi fa inferenza aumenta molto e quindi te lo fanno pagare. Chiusa la parentesi tecnica, ma giusto per capire anche che c'Ã¨ una complessitÃ  dietro diversa da quella a cui siamo abituati. CioÃ¨ noi siamo abituati a "prendo piÃ¹ RAM, ho RAM X, prenderÃ² il doppio e sarÃ  X per 2". In realtÃ  non serve il doppio della RAM, serve tre volte circa la RAM per fare il doppio del contesto.


## **[48:13] Modelli di AI: Minimax e GLM**

**Stefano**

> Ok. E dunque io cosa ho provato? Ecco, parliamo di questi modelli nuovi che sono usciti. Io ho provato Minimax M2.5 e GLM 5. Di GLM c'Ã¨ anche il paper che Ã¨ super interessante, soprattutto nella parte di training perchÃ© prendono molte delle idee che c'erano in DeepSeek R2 e le estendono sulla parte di training. Non mi ci addentro qua, se avete voglia andatevelo a leggere, perÃ², perchÃ© c'Ã¨ sia il paper che il loro blog, diciamo, piÃ¹ divulgativo, che si capisce molto bene.
>
> Allora, i modelli vanno distinti un po' in due modi. Le risposte. Una Ã¨ la qualitÃ  della risposta, ma c'Ã¨ anche un discorso di consistenza delle risposte.


## **[49:32] Consistenza e QualitÃ  delle Risposte**

**Alessio**

> Spieghiamo cosa si intende per consistenza, magari. Spiegalo. No, no, spiega tu, spiega tu.

**Stefano**

> Ah, spiego io. No, beh, consistenza delle risposte: che a stessa domanda ottengo una risposta che ci si avvicini molto. Banalizzando molto, se non la stessa, la stessa Ã¨ impossibile perchÃ© c'Ã¨ l'indeterminismo, ma se faccio due domande uguali con lo stesso contesto mi aspetto che le due risposte siano simili o indistinguibili nella versione ideale.

**Alessio**

> Se vuoi, non Ã¨ sufficiente che il modello ti risponda giusto una volta sola, deve risponderti sempre la stessa cosa.

**Stefano**

> Detto in altri termini, sÃ¬, certo, non deve rispondere sempre sbagliato, sennÃ² Ã¨ una consistenza brutta. Mentre invece la qualitÃ  della risposta, parlando di coding, visto che io li ho provati per coding, Ã¨ che il codice generato sia di buona qualitÃ , faccia quello che gli Ã¨ stato chiesto eccetera eccetera.
>
> Allora, sulla qualitÃ , diciamo che del caso migliore sono paragonabili e guardando i benchmark si avvicinano molto tutti e due a Opus 4.5, quindi la versione precedente di Opus. Sulla consistenza, GLM Ã¨ molto molto meglio, anche guardando dati in giro, ma anche nella sensazione che ne hai nell'utilizzarlo.


## **[51:12] VelocitÃ  e Performance dei Modelli**

**Stefano**

> Poi c'Ã¨ un terzo parametro ed Ã¨ la velocitÃ  e Minimax Ã¨ di una velocitÃ  spaventosa. Ãˆ piÃ¹ veloce di Sonnet. Ãˆ velocissimo a rispondere e quindi la fase di reward che ho io dalla mia slot machine Ã¨ velocissima e quindi continuo a cliccare come un...

**Paolo**

> Stavo pensando, in questi giorni stavo capendo, ho scoperto, anzi parliamone in live. Ho scoperto, perchÃ© me l'ha detto Claude Code, ultimamente le cose le scopro da lui principalmente, che quando usi subagent in Claude Code, lui per i subagent sceglie Haiku, quello che Ã¨ configurato come Haiku per fare le attivitÃ .

**Stefano**

> Sempre?

**Paolo**

> Non lo so, non ho verificato, perÃ² lui mi ha detto questa cosa, diciamo che indipendentemente se sia completamente vera oppure no, era interessante ed era plausibile e mi faceva ragionare e diceva "ok, quindi forse allora non sono sempre cosÃ¬ contento di demandare i subagent perchÃ© Haiku ha comunque delle capacitÃ  limitate e quindi va bene quando lo si manda sui binari, ma se si esce dai binari forse non Ã¨ una buona idea."


## **[52:35] Utilizzo dei Subagents e Ottimizzazione**

**Paolo**

> E allora stavo dicendo, ne parlavo con degli amici, gli dicevo "ho scoperto questa cosa, forse smetterÃ² di usare un pochettino i subagent", timorito da questa cosa. E qualcuno di loro giustamente mi ha detto "ma non puoi cambiare le carte in tavola, non puoi rimappare Haiku a quello che vuoi tu?" E sÃ¬, quello Ã¨ stato possibile. Non ci avevo pensato io. E quindi adesso che tu Stefano parlavi di queste cose, stavo dicendo "perchÃ© non provi a giocare con questa cosa? PerchÃ© non rimappi il tuo Haiku a Minimax e vedi se ti dÃ  delle risposte velocissime per delle cose piÃ¹, diciamo, che non richiedono super intelligenza, e usi il tuo modellone principale per quelle altre?"

**Stefano**

> No, questa Ã¨ un'idea interessante molto ed Ã¨ una delle cose che volevo provare.

**Alessio**

> Invece ho una domanda filosofica, nel senso che noi stiamo parlando di velocitÃ , che poi bisognerebbe distinguere tra velocitÃ  nel processare il prompt e velocitÃ  nel generarti la risposta. Una cosa, cioÃ¨ ci sono se vuoi due modi di utilizzare questi coding agent: uno Ã¨ tra virgolette in puro vibe coding, chiedo, quando mi Ã¨ arrivato il risultato passo allo step dopo eccetera, oppure sto lÃ¬ e leggo anche tutto il reasoning che il modello sta facendo nel darmi la risposta, che se vuoi ha anche un aspetto di formazione, di learning, non so come dire. Il fatto che il modello sia piÃ¹ veloce, che tu magari non fai neanche in tempo a leggerti tutto il suo flusso e te lo devi guardare dopo, ha una sua rilevanza per voi o cosa?


## **[53:39] VelocitÃ  vs Accuratezza nel Coding**

**Stefano**

> Dipende come lo usi, cioÃ¨ nel senso che io il Ctrl+O lo schiaccio raramente. Ctrl+O Ã¨ per vedere tutta la parte di reasoning perchÃ© di default ormai Claude Code ce l'ha disabilitata e compressa. Dipende. Il plan lo leggo, ma una volta che sono contento dal plan che lui faccia il tentativo, "no cazzo, non sono riuscito, non passa il test, faccio quel..."  No! CioÃ¨ solo quando non c'ho niente da fare, voglio giudicare. Ãˆ come guardare Twitch, non so.

**Paolo**

> Io vi dico la veritÃ , io lo leggo ed Ã¨ come guardare Twitch, esattamente. Ma lo leggo perchÃ© non devo schiacciare Ctrl+O. Il mio alternativo di Claude Code che uso lo tiene aperto in automatico, tant'Ã¨ che mi Ã¨ successa una cosa curiosa che ci stava nella storia precedente. Sono stato bloccato dopo essere riuscito ad aumentare la mia produttivitÃ  mettendomi Claude Code su Telegram, sono stato bloccato dall'ulteriore vincolo che ho colpito i limiti di messaggi di Telegram. Generavo talmente troppi messaggi col mio Claude Code che quindi mi ha cappato Telegram stesso.
>
> PerchÃ© mi ha cappato Telegram stesso? Probabilmente perchÃ© io stavo facendo questa cosa: su Telegram ricevevo anche tutti i reasoning. E sono molto interessanti, soprattutto se non sai cosa stai facendo come nel caso del mio progetto. CioÃ¨ impari man mano che vai, alcune cose ovviamente sono lui che cerca un concetto, scopre che era nell'altra classe, cioÃ¨ chi se ne frega. Altre volte invece lui si rispiega le cose da solo e dice "allora facciamo cosÃ¬, cosÃ  per via di questi motivi". Ed Ã¨ molto interessante.
>
> CuriositÃ  ulteriore. Ho beccato, leggendo tutti questi log di esecuzione, che deve esserci o qualche errore oppure lui racconta male la storia perchÃ© ogni tanto non trova dei file e nella stessa frase dice "il file che dovrei guardare Ã¨ quest'altro che ha esattamente lo stesso nome". Quindi o Ã¨ un rendering sbagliato di versioni del file, che perÃ² lui si mostra il percorso e quindi il percorso Ã¨ corretto e uguale, ma lui intende due punti temporali diversi di quel file, oppure c'Ã¨ un bug e me ne sono accorto leggendo quello che combina.

**Alessio**

> Un problema con i tool, dico.

**Paolo**

> Problema con i tool, potrebbe essere un problema con il modello o potrebbe essere solo un problema di logging, come vi dicevo. Magari in realtÃ  lÃ¬ c'Ã¨ un hashcode, i due hashcode sono diversi ma il nome del file invece Ã¨ lo stesso.
>
> Comunque li leggo e sono interessanti, perÃ² piÃ¹ in generale, Alessio, mi hai fatto venire in mente che questo problema che tu manifestavi, ovvero riusciamo a dare retta a tutto quanto il flusso di esecuzione? La risposta Ã¨ no, ma questo problema si manifesta forse piÃ¹ visivamente nei progetti di collaborazione, di coding con gli open source in particolare, in cui Ã¨ aumentata cosÃ¬ tanto la produttivitÃ  per produrre nuovo codice PR che adesso il collo di bottiglia Ã¨ la revisione di questo codice. I tech lead, i project lead seri che non vogliono accettare qualunque cosa, non hanno letteralmente il tempo per stare a leggere tutta la roba che gli arriva. E alcune delle robe che gli arrivano, in alcuni progetti piÃ¹ popolari, era famoso l'esempio del tizio di cURL, Ã¨ pattumiera. E loro sprecano del tempo a leggere pattumiera di AI slop o di concetti sbagliati, cattive idee in generale, che adesso il primo che passa gli lancia addosso e lascia loro l'incombenza di decidere quale rumore, quale segnale.

**Stefano**

> Intanto che parlavamo ho verificato quella cosa che dicevi, giusto per dare l'informazione completa e perchÃ© mi incuriosiva a me. Allora, per i subagent usa Haiku soltanto quando vanno in Explore, cioÃ¨ quelli read-only, subagent di Explore. Ma quando va in Plan o General Purpose lo eredita, eredita il modello dalla sessione madre e quindi se sei partito con Opus vai con Opus.

**Paolo**

> SÃ¬, Ã¨ piÃ¹ ragionevole, perÃ² appunto era comunque affascinante l'idea che ci sia questo livello di ottimizzazione dentro Claude Code che uno volendo puÃ² andare a interferire.

**Stefano**

> No, no, quella Ã¨ affascinante. Ero giÃ  pronto a giocare con le variabili d'ambiente come mi avevi consigliato tu, ma la parte di Explore mi interessa poco. Quindi usando il mio modello e che tanto piace a Paolo.
>
> No, ecco, perÃ² la velocitÃ  Ã¨ un tema. La velocitÃ  Ã¨ un tema, tanto che quelli di GPT ci si sono messi pesanti perchÃ© GPT 5.3 Spark va a livello di velocitÃ  di numero di token 6 volte piÃ¹ veloce di 5.3 normale, allo stesso prezzo con meno token, quindi a prezzo piÃ¹ alto per token.

**Alessio**

> Ma infatti qui la domanda, se vuoi, Ã¨: ma fino a che punto uno puÃ² sacrificare l'accuratezza per avere invece velocitÃ ? In quale tipo di utilizzo?

**Stefano**

> Quella Ã¨ una domanda... Allora, nell'utilizzo generico come assistente personale secondo me assolutamente sÃ¬. Nel senso, se lo uso per spostare file, organizzare directory, farmi le slide, quelle cose lÃ¬, che Minimax vada come un fulmine mi interessa di piÃ¹ che sia perfettamente accurato, perchÃ© tanto poi le rivedo.

**Alessio**

> PerchÃ© tu sai giÃ  che il task Ã¨ sufficientemente facile per cui in ogni caso ce la farÃ .

**Stefano**

> Esatto. Sul codice complesso, boh, non lo so. Intanto che io sto pensando di smettere di pagare tutti i cinesi che pago e pagare soltanto un americano, cioÃ¨ Opus e fine. Stavo giusto guardando quante chiamate mi fa fare la versione 5x e sono a livello di Minimax, quasi quasi proprio a livello.

**Alessio**

> Proprio a livello di brainstorming.

**Stefano**

> 20, li do giÃ  ad Anthropic. 40 li do a Minimax. Li metto insieme, sono giÃ  60. Con 40 in piÃ¹ mi trovo con Opus 4.6, forse che forse dal mese prossimo faccio questa scelta qua, cosÃ¬ mi avete fatto tutti i conti in tasca.

**Alessio**

> Poi dai qualcosa anche a ChatGPT.

**Stefano**

> Ma quello non posso non darglieli perchÃ© mia figlia mi ammazza se smetto di pagare ChatGPT.


## **[01:00:20] Rischi e Sicurezza nell'Utilizzo di AI**

**Paolo**

> Comunque, scusami, prima stavi raccontando che per spostare file sul desktop ti va bene un modello veloce, piÃ¹ che uno bravissimo. Mi richiami alla storia, una delle news di questa settimana, la capa della sicurezza di...

**Stefano**

> L'abbiamo preparata questa, quindi riesco a stare al passo.

**Paolo**

> C'Ã¨ un post su Twitter, credo, c'Ã¨ uno screenshot di una chat dove, non vi ricordo piÃ¹ come si chiama, la persona che Ã¨ a capo della sicurezza in Meta, gli Ã¨ scappato di mano il suo OpenClaw nonostante avesse tutte le precauzioni del caso definite e gli ha iniziato a cancellare tutta la mail che trovava. E lei racconta come in una scena di film d'azione dove corri a cercare di smantellare la bomba prima che stia per esplodere, Ã¨ dovuta correre davanti al computer a cercare di bloccarlo. E sei la capa della sicurezza di Facebook, che quindi qualcosa ci dovresti capire, e le persone che lavorano insieme a te ti avrebbero dovuto dire esattamente come fare a far sÃ¬ che questo non succedesse, ed Ã¨ successo nonostante tutto.

**Stefano**

> Ecco, tra l'altro questo, spieghiamo un secondo per gli ascoltatori anche tecnicamente che cosa Ã¨ successo, perchÃ© poi sono andato a leggere. Allora, premesso che doveva mettere dei guardrail piÃ¹ importanti sulla sua mail eccetera, ma lei pensava di essere a posto perchÃ© tra le OpenClaw, come anche Claude Code, gli puoi dare una sorta di system prompt all'interno del Claude MD, di cose che deve sempre rispettare, e lei le aveva detto "suggerisci soltanto modifiche sulla mail, non prendere mai iniziativa, non fare mai cambiamenti." Allora, la cosa giusta era dargli delle API che non facessero cambiamenti e fossero in sola lettura. Lei si Ã¨ fidata dal dirlo al modello.
>
> E che cosa Ã¨ successo tecnicamente? Questo anche per far capire perchÃ© tante volte abbiamo insistito anche nella puntata quando c'era Alex, l'abbiamo spiegato bene. Tra l'altro credo che ormai sia il nostro ospite piÃ¹ citato, glielo dirÃ² questa cosa. PerÃ² lo spiegavamo dal fatto che uno dei motivi per usare Backlog o sistemi simili Ã¨ che tu fai una sessione, usi tutto il contesto, chiudi, riapri o fai clear in modo da partire da sessione pulita e di non arrivare mai al comprimere la sessione perchÃ© quando comprimi potresti perdere un po' di qualitÃ .


## **[01:02:20] Compressione del Contesto e QualitÃ  delle Risposte**

**Stefano**

> Quello che Ã¨ successo lÃ¬ Ã¨ esattamente quello. La sua casella di mail era cosÃ¬ grande che quando ha cominciato a leggere i messaggi che doveva cancellare, erano cosÃ¬ tanti, perchÃ© evidentemente non faceva zero inbox come policy, ne ha letti cosÃ¬ tanti che ha riempito il contesto, ha deciso di comprimere, l'ha riempito ancora, ha deciso di comprimere, l'ha riempito ancora, ha deciso di comprimere e nell'ultima compressione, lei dice la terza, nella compressione si Ã¨ persa l'istruzione del "non prendere iniziativa" e ha deciso che stava riempiendo ancora il contesto e ha detto "ma via, invece di continuare a comprimere perchÃ© non zappo via tutta sta roba che mi libera il contesto?" E cosÃ¬ ha fatto.

**Alessio**

> Le sue informazioni sono state diluite nel resto.

**Paolo**

> Ho due aneddoti su questa cosa. La prima Ã¨: ovviamente questi rischi su scala diversa ce li hai anche quando fai Claude Code normale per sviluppare cose, e gli sviluppatori saggi di Anthropic ci hanno donato gli hooks per intercettare prima che vengano eseguiti i comandi, i comandi stessi. CosÃ¬ tu puoi avere dei guardrail strong, ovvero non con del testo che ogni tanto puÃ² perdersi, proprio dei passaggi software tradizionale, per cui il codice non va avanti finchÃ© non viene fatto questo ragionamento.
>
> Ed Ã¨ molto importante a mio avviso perchÃ© mi veniva da commentare prima, si vede che quella persona in Facebook non aveva mai passato abbastanza tempo a fare vibe coding perchÃ© ci incappi inevitabilmente che tu gli dici di non fare una cosa, lui la fa, e a un certo punto lo devi proprio bastonare e dire "adesso mi sono arrabbiato, voglio verificare ogni singola cosa che provi a fare, ti impedisco di fare quello che ti ho detto di non fare".

**Alessio**

> Che Ã¨ una lampada di Aladino.

**Paolo**

> Tipo la lampada di Aladino, sÃ¬. Ovviamente poi funziona questa cosa, non ci sono mai contri. Ãˆ semplicemente un lavoro lungo e laborioso perchÃ© tu ti ricordi tutti i 99 casi su 100, ma ti dimentichi il centesimo e scopri che in realtÃ  erano 500 i casi. Quindi quel problema in realtÃ  Ã¨ sempre dietro l'angolo, perÃ² migliorano le cose ed Ã¨ una cosa su cui faccio molta attenzione per il mio sviluppo e per quello del mio team quando gli spiego queste cose.

**Stefano**

> PerchÃ© non Ã¨ sufficiente?

**Paolo**

> La seconda cosa invece, che me la sono giÃ  dimenticata qual era... Era che sÃ¬, i contesti, il valore dei contesti e della compressione del contesto. Ho sempre preso sotto gamba questa cosa perchÃ© ogni tanto avevo piÃ¹ cose da fare di quello che il contesto mi permetteva e non avevo la sensazione che si potesse fare di meno. Dovevo fare delle cose grosse e lui doveva avere informazioni tutte insieme, prendere o lasciare. Quindi l'ho sempre preso come un male accettabile e tendenzialmente vedevo che le performance degradavano un pochettino, ma era piÃ¹ una sensazione spannometrica, quindi magari era tutto un film nella mia testa o cosÃ¬.
>
> Questa settimana invece l'ho verificata con mano in una maniera quasi buffa. Non so se mi Ã¨ impazzito il terminale, mi ha fatto vedere delle cose che non doveva, o semplicemente mi Ã¨ capitato di guardarlo tra una compaction e la fine della compaction, ma mi ha sputato fuori il system prompt compattato successivo ed era terribile. Ma non era terribile in cui lui aveva selezionato a caso delle frasi, quello potrebbe essere accettabile. Era proprio sbagliato. C'erano ripetizioni di parole, parole mangiate insieme, cioÃ¨ proprio come qualcuno che ha picchiato la testa e non ragiona piÃ¹. E questa, vederlo con i miei occhi mi ha fatto capire quanto non ci si possa fare affidamento su quella cosa lÃ¬.

**Stefano**

> Quando Ã¨ successa questa cosa? Questa settimana? PerchÃ© un paio di settimane fa c'era un bug su Claude Code sulla compressione che hanno fissato nel giro di qualche ora, ma in quelle ore si Ã¨ scatenato il mondo perchÃ© non andava piÃ¹ nulla in compaction.

**Paolo**

> Questa settimana, potenzialmente sÃ¬, potenzialmente buggata. Ma guarda, finchÃ© se ne parla a livello astratto ti dico "vabbÃ¨, ok, capisco, Ã¨ meglio se..." Quando l'ho vista ho proprio capito che lÃ¬ io umano non capivo cosa c'era scritto, cioÃ¨ perchÃ© ci deve capire lui? Ho proprio capito il punto. Non si scherza con quello.


## **[01:07:16] Strumenti per la Gestione del Contesto**

**Stefano**

> No, no, infatti bisognerebbe sempre cercare di lavorare a contesto pulito sulle singole issue, portarle in fondo, uscire, rientrare. Ed Ã¨ il motivo per cui gli strumenti che esternalizzano questa cosa, siano PRD, spec, Backlog, quello che Ã¨. PerchÃ© tutto il ragionamento lo fai prima, lo consolidi lÃ¬ e poi gli dici "ok, prendi quel pezzettino", lui si legge quello che gli serve in contesto e si muove.
>
> Ecco, tanto tempo fa ve l'avevo raccontato, non mi ricordo se in podcast o in privato, avevo il mio personale flusso in cui facevo la spec e poi gli facevo creare un work in progress file in modo che lui si tenesse traccia di cosa aveva fatto. Strumenti tipo i vari, giÃ  citato Backlog e altri, fanno esattamente quella roba lÃ¬, ma Ã¨ veramente fondamentale.
>
> Tanto che, lo dicevamo con Paolo, no? Ci scrivevamo questa cosa, sia io che te lo usiamo anche per task non di coding. CioÃ¨ per prendere appunti delle prossime cose che facciamo con l'aiuto di Claude Code, ma non necessariamente di coding.

**Paolo**

> Ti dico la veritÃ , adesso che ce l'ho sul telefono ancora di piÃ¹, Ã¨ un'estensione dell'Alexa/Siri che non ha mai funzionato bene.
>
> Peraltro, scusami, ho fatto una PR a Backlog MD e me l'hanno approvata. Era una classe di CSS la mia PR, perÃ² era un bug effettivo che avevo beccato e quindi volevo far presente che si puÃ² contribuire ai progetti.

**Stefano**

> Ne ho fatte due anch'io. Gli ne ho fatte due anch'io proprio questa settimana e penso che mi ha detto che le deve guardare, perÃ² ne ho fatte due anch'io perchÃ© c'erano due cosette che servivano a me personalmente, ma che credo siano utili al mondo. E sÃ¬, contribuite! SÃ¬, no, perÃ² contribuite, Ã¨ un bel progetto, open, ha tante stelle, piÃ¹ di 1000.

**Alessio**

> Sempre perchÃ© Alex Ã¨ l'ospite piÃ¹ citato.


## **[01:09:30] Conclusioni e Riflessioni Finali**

**Paolo**

> Cliccateci le stelline e campanelline al progetto degli altri.

**Stefano**

> No, ma anche al nostro! Le campanelline vanno messe qua. Poi dopo ci sono le stelle... No, lo so che al prossimo altro non lo diciamo.

**Paolo**

> Anche punti Fragola valgono.

**Stefano**

> I punti Fragola solo se ce li regalano, perchÃ© mi piacciono gli zaini. No, scherzo.

**Alessio**

> Vogliamo parlare dei piatti della pizza di Carrefour, non lo so.

**Stefano**

> Potrebbe essere. Ricordiamo che Esselunga e Carrefour sono gli sponsor di questa puntata. Va bene, credo che abbiamo giÃ  fatto i pirla abbastanza. Possiamo, cosa dite, chiudere?

**Alessio**

> SÃ¬, sÃ¬.

**Stefano**

> Va bene. Grazie a tutti e tutte di averci ascoltato. E campanelline, stelline, quelle cose che Paolo non vuole che si dicano. Alla prossima, ciao!
