---
title: "AI, social e innovazione: cosa cambia davvero con Gemini 3 e i nuovi modelli multimodali - #27"
categories:
  - Puntate
tags:

  - AI
  - Gemini
  - Multimodali
layout: single
author_profile: true
---


{% include video id="OrfrC0OKFNk" provider="youtube" %}

üëâ [Ascolta su Spotify](https://open.spotify.com/show/16dTKEEtKkIzhr1JJNMmSF?si=900902f2dca8442e)<br/>
üëâ [Guarda su YouTube](https://www.youtube.com/channel/UCYQgzIby7QHkXBonTWk-2Fg)<br/>
üëâ [Segui su LinkedIn](https://www.linkedin.com/company/risorseartificiali)<br/>

## Into.... (00:00)

## Inizio Puntata (01:00)

**Stefano Maestri (01:02)** Ciao a tutte e tutti, bentornati a Risorse Artificiali.

**Alessio Soldano (01:17)** Ciao Ale, ciao Paolo.

**Paolo Antinori (01:21)** Ciao.

**Stefano Maestri (01:22)** E bene, bene. Bentornati e bentornate. Dicevamo, grazie dai feedback che ci date anche di persona, perch√© ieri eravamo tutti e tre, l'avevamo anche scritto credo da qualche parte, eravamo tutti e tre al Cloud Day ed √® stato simpatico poter scambiare, soprattutto la sera nella community AI, qualche parola e avere qualche feedback. Ci avete detto di essere pi√π nerd. Di pi√π √® difficile, ma promettiamo che non lo saremo di meno, anche perch√© non ci riusciamo. Quindi, nerdismo per nerdismo, partiamo con Paolo che ha un'osservazione interessante.

**Paolo Antinori (02:09)** Ecco, grazie di aver alzato le aspettative per poi deluderle istantaneamente. Allora, no, volevo fare, visto che si parla di essere nerd, volevo fare un pochettino un rant con tanto di osservazione nerd per sentirsi sarcastici e intelligenti con atteggiamento tipico da nerd. Non so quanti di voi o anche chi eventualmente ci ascolta √® stato impattato da l'ennesimo outage di un servizio internet che tieni su internet. Non era AWS questa volta, questa settimana, ma era Cloudflare.

Magari ve ne siete accorti perch√© io in quel momento in particolare stavo cercando di guardare dei video online di un evento che avevano pubblicato, che aveva appena pubblicato i video, e i video non andavano. E come al solito mi viene sempre da maledire Linux quando queste cose non vanno e in realt√† forse non era colpa di Linux, ma era colpa di Cloudflare, cio√® era colpa di qualcuno, di solito AWS, in questo caso non AWS.

E quando mi sono accorto che non funzionava, oltre ad avere condiviso il solito fumetto di XKCD, quello in cui ci sono i due omini che combattono sulle sedie a rotelle anzich√© lavorare ‚Äî non so se avete presente, e che a quanto pare non tutti ancora hanno visto e quindi riesce sempre a strappare qualche sorriso...

**Alessio Soldano (03:38)** Beh anche se l'hai visto, lo rivedi e sorridi.

**Paolo Antinori (03:49)** S√¨, s√¨, s√¨. Anche perch√© sto cercando di condividere lo schermo e non ci sto riuscendo, quindi in realt√† portate pazienza. Ma detto questo, fondamentalmente ci sono due omini che con la spade combattono anzich√© lavorare, qualcuno gli dice "Ehi, tornate al lavoro" e loro rispondono con un'operazione che sta rallentando. Credo che quello originale fosse "il codice sta compilando" e pian pianino nella storia dell'informatica si √® evoluto con "AWS √® gi√π", "stanno girando i test", "stanno girando i test", qualunque variante, insomma. E questa cosa mi fa sempre riflettere...

**Alessio Soldano (04:07)** Stanno girando i test.

**Paolo Antinori (04:16)** Qui mi parte un pochettino il rant. Di solito quello che non funziona quando AWS o Cloudflare sono gi√π √® GitHub e dici "vabb√®, ok, lavoro tanto con GitHub, non riesco a lavorarci". E poi da l√¨ parte la riflessione: siamo riusciti proprio a fare andare l'informatica al contrario di come dovrebbe, considerando che Git nasce come un sistema di condivisione file punto-punto decentralizzato, perch√© io posso collegarmi al tuo repo, tu puoi collegarti al mio... e invece abbiamo deciso di rendere meno utile questa cosa: centralizziamola in GitHub, cos√¨ se GitHub √® gi√π nessuno riesce pi√π a fare una mazza. E quindi vabb√®, qui mi parte appunto un po' il rant da boomer su queste faccende.

La vera riflessione era: come mai internet non sta andando? Come vero boomer, che non √® che identifica il problema, dice "internet non va". E mi sono chiesto, boh, magari come al solito dici: √® iniziata una guerra? √à successo qualcosa? Trump ha detto che dobbiamo pagare per la rete? Qualunque cosa sia. E ho guardato un po' in giro e qualcosa √® successo pi√π o meno in quel periodo. Google ha fatto un enorme rilascio di un modello LLM: ha rilasciato Gemini 3 che a detta loro, e a detta di tutti quanti gli influencer su internet, √® la cosa migliore che sia mai successa nel mondo dell'informatica degli ultimi anni.

**Stefano Maestri (05:45)** Ma quindi tu stai dando la colpa al rilascio di Gemini 3, che √® tendenzialmente sul cloud di Google, per aver tirato gi√π un altro servizio cloud?

**Paolo Antinori (05:56)** Ma mi chiedo, nel senso, potrebbe essere che abbia avuto cos√¨ tanto successo che fondamentalmente la rete non faceva nient'altro che usare quello e quindi √® riuscito a mettere in ginocchio Cloudflare? Piuttosto che era clamorosamente buggato e la prima persona che gli ha chiesto di fare crawling sulla pagina, lui ha iniziato a scaricare tutto internet e ce l'ha buttato gi√π per tutti? Mi sono chiesto quali ipotetici scenari potevano essere collegati a questa cosa. Di solito la spiegazione degli hyperscaler e dei provider √® "l'intern ha inciampato nel cavo di rete". √à quasi sempre questa la spiegazione che a me sembra poco credibile, per√≤ posso capire che se sei un VP e devi trovare una scusa sacrifichi il primo agnello sacrificale che ti passa davanti.

**Stefano Maestri (06:36)** L'agnello sacrificabile mi piace molto. No, per√≤ dai, io su questa cosa non ho un'opinione. Pi√π che altro l'unica cosa che mi sembra un po' stretchata √® che tirino gi√π un altro provider e non loro stessi rilasciando sul loro cloud. Per√≤ tutto pu√≤ essere perch√© magari c'era un giro di DNS eccetera. Questo in realt√† ci spinge, per i non addetti ai lavori, a sottolineare ‚Äî ai non addetti ai lavori che ci ascoltano, c'√® qualcuno comunque ‚Äî quanto tutto √® interconnesso. √à un po', per fare un esempio che √® finito su tutti i giornali, simile a quello che √® successo in Spagna quando c'√® stato il blackout quest'estate. Per quanto riguarda le reti elettriche, non a caso si parla di grid, cio√® di griglia, perch√© se salta un pezzo viene gi√π tutto. Internet non √® strutturata come una griglia elettrica, √® completamente diversa, √® punto-punto eccetera... s√¨, √® pi√π un grafo, per√≤ il livello di interconnessione che oggi c'√®, specialmente tra i grandi provider cloud AWS, Cloudflare, Google, Microsoft √® tale che l'effetto farfalla, diciamo, pu√≤ partire da uno dei grandi cloud provider e paradossalmente colpire l'altro.

**Alessio Soldano (07:46)** Un grafo se vuoi.

**Stefano Maestri (07:58)** Effetto farfalla al contrario, se mi passate il termine.

**Paolo Antinori (08:16)** Scusami, la mia faccia √® perch√© non l'avevo mai sentito citare in italiano e mi suona davvero maluccio in italiano "l'effetto farfalla", mentre Butterfly Effect o...

**Alessio Soldano (08:26)** L'ho sentito ieri forse alla conferenza.

**Stefano Maestri (08:29)** Sono d'accordo, l'ho sentito ieri alla conferenza e l'ho sentito in un'intervista l'altro giorno perch√© l'ha usato in italiano Serena Sensini. Andate a sentire l'intervista che √® molto interessante, l'ha usato lei in italiano e non so, anche io l'ho sempre detto Butterfly Effect e adesso invece parlando mi √® rimasto quanto avevo sentito probabilmente in intervista.

**Paolo Antinori (08:50)** Credo in italiano di averlo sentito come effetto di Lorenz che √® pi√π oscuro, ma si presta meno alla misinterpretazione.

**Stefano Maestri (09:00)** Eh, s√¨, s√¨, s√¨. Da questo punto di vista concordo, per√≤ diciamo che almeno √® abbastanza chiaro anche a chi non bazzica troppo l'inglesismo, ecco. Per√≤ a parte questo, torniamo invece a quello che c'era dentro quello che hai detto tu. Esatto. Parto proprio da questo possibile Butterfly Effect.

**Alessio Soldano (09:19)** Vediamo cosa ha fatto Gemini 3\.

**Stefano Maestri (09:30)** Perch√© il CEO, Sundar Pichai, ha proprio detto "abbiamo rilasciato Gemini 3 e l'abbiamo fatto at Google scale". E dicendo questo intendeva che non hanno soltanto rilasciato il modello. Ovviamente qui c'√® una guerra economica e di supremazia, ma anche quasi di religione, tra le startup ‚Äî che ormai non sono pi√π startup, sono anche pi√π che scale up, sono dei super unicorni ‚Äî OpenAI e Anthropic, che si focalizzano moltissimo sul modello e poi eventualmente nel tempo aggiungono feature. Tipo adesso ChatGPT sta aggiungendo una feature che potrebbe essere molto interessante che sono le chat di gruppo, in cui noi tre possiamo chattare tra noi e avere un quarto membro che √® ChatGPT, che pu√≤ avere un suo perch√©...

**Alessio Soldano (10:37)** Senza dover condividere con il mondo la chat sostanzialmente.

**Stefano Maestri (10:41)** No, e poi proprio farla partecipare ad una chat di gruppo, quindi prendere un contesto ampliato dalle nostre discussioni ed inserirsi per aggiungere i suoi int o punti di vista, se vogliamo chiamarli cos√¨. Per√≤ chiusa parentesi, loro tendono a rilasciare il modello e poi le feature. Google, che √® Google, quella che conosciamo tutti per il motore di ricerca, ha deciso di fare appunto, come ha detto il CEO, un rilascio "at Google Scale". Per cui hanno s√¨ rilasciato Gemini 3, ma contemporaneamente l'hanno messo anche nell'Advanced Search, contemporaneamente l'hanno reso disponibile a tutti nell'app, contemporaneamente hanno rilasciato un nuovo IDE, contemporaneamente l'hanno messo all'interno del loro CLI. Un giorno dopo, perch√© probabilmente hanno avuto qualche problema di rilascio, hanno rilasciato anche il modello per generazione delle immagini Pro, quindi Nano Banana Pro. E poi di quello parliamo magari pi√π nel dettaglio. E poi cos'altro hanno aggiunto? Credo... ah no, la parte Advanced AI per la parte agentica, per quei pochi che gi√† negli Stati Uniti la vedono, l'hanno messo dentro a Vertex, l'hanno messo dentro l'API, cio√® hanno fatto tutto insieme. Hanno veramente fatto quel rilascio che √® l'incubo di tutti i DevOps, probabilmente, perch√© cosa pu√≤ andare storto in una cosa cos√¨? Al massimo viene gi√π Cloudflare\!

**Paolo Antinori (12:29)** Vedi? Vedi?

**Stefano Maestri (12:35)** No, per√≤ a me ha impressionato un paio di cose... manca Vio, per√≤. Manca Vio, ma ce lo aspettiamo settimana prossima? Allora storicamente se guardiamo Vio √® sempre shiftato di una o due settimane dal modello delle immagini, perch√© comunque tu mi insegni che forse l'uno √® la base dell'altro, in parte almeno il tipo di lavoro.

**Alessio Soldano (13:04)** S√¨, ma poi secondo me √® anche un discorso di marketing. Cio√® loro lasciano che la gente si guardi le nuove magie del modello video, audio, immagini e poi quando la gente √® contenta passano allo step dopo. Per√≤ punti di vista.

**Paolo Antinori (13:26)** Beh, √® una cosa nota, nel senso... magari chi non ha avuto questa esposizione al lavoro non ci ha fatto caso, ma se lavori in una grossa organizzazione che ha molte linee di prodotto, talvolta una delle richieste √® di non pestarsi i piedi a vicenda, quindi lasciare un po' di air time per stare al centro del palco per abbastanza tempo per generare clamore. E a me √® capitato talvolta di sentirmi dire che non potevamo rilasciare perch√© dovevamo lasciare il microfono a qualcun altro e quindi ci hanno chiesto in occasioni di aspettare dopo Natale, ad esempio.

**Stefano Maestri (13:59)** S√¨, s√¨, anche quelle volte che eravamo eccezionalmente pronti prima di Natale.

**Alessio Soldano (14:05)** S√¨, c'√® la No Release Zone.

**Stefano Maestri (14:08)** S√¨, esatto. No, allora a me ha colpito questa bomba mediatica, se vogliamo, all'interno della nostra piccolissima nicchia che guardiamo quando esce il nuovo pelo sul nuovo modello, ma anche tutto sommato abbastanza per il grande pubblico. Perch√© vedevo i dati, lo studio di McKinsey che ho pubblicato in newsletter settimana scorsa sull'utilizzo all'interno delle aziende... Gemini ha, cio√® se un anno fa eravamo qua a chiederci ‚Äî circa un anno fa, mi ricordo che in una discussione che poi √® quella che in parte mi ha spinto dopo aver parlato con voi in privato a cominciare a scrivere la newsletter qualche mese dopo ‚Äî ci dicevamo: "Ma Google in tutto ci√≤? Il modello di business messo in ginocchio, sembrano ampiamente in ritardo". Ricordo al tempo c'era Gemini 2.0 che poi dopo a gennaio hanno rilasciato 2.5 e gi√† avevano abbastanza sistemato le cose perch√© 2.5 era a livello degli altri. Ma in quest'anno penso possiamo essere d'accordo quanto siano stati impressionanti a coprire il gap sul linguistico, sul parlato, sul video, sull'audio...

**Alessio Soldano (15:44)** S√¨, altro che coperto.

**Stefano Maestri (16:02)** No, no, coperto e sorpassato per molti versi. La generazione delle immagini con Nano Banana, tutta la parte di codice con CLI e Jules ‚Äî ecco, anche su Jules hanno rilasciato Gemini 3.0 ‚Äî e tutto il mondo enterprise. Per cui diciamo che i capitali non gli mancano, le capacit√† neanche, e hanno fatto vedere in quest'anno che sono tutt'altro che fuori dai giochi. Poi tutto pu√≤ succedere in questo mondo e prendere una piega diversa o magari aver investito troppo e non fare revenue, questo io non lo so. Per√≤ sicuramente dal punto di vista tecnico e tecnologico hanno detto "No, noi siamo ancora Google e facciamo un po' quello che ci pare."

**Paolo Antinori (16:38)** Detto questo, Stefano, che cosa ci rappresenta questo super rilascio di Google? Al di l√† del fatto che sono capaci a rilasciare cose senza spaccare tutto... cio√® nel senso, perch√© tutti ne parlano? Perch√© speculiamo che sia caduto internet in contemporanea? Perch√© io ho letto commenti qua e l√† di persone che dicono "Oh, wow, √® la cosa migliore che io abbia mai visto". Allora sono andato subito a curiosare, a provare, e ho provato e funzionava pressapoco come quello del giorno prima nella mia esperienza da uomo comune che gli ha chiesto di generare un'immagine a caso o un prompt su una domanda specifica che avevo ancora nella history dell'interazione con Gemini. Cio√® dove sta tutto questo clamore, tutto questo valore? In altre parole: esticazzi\!

**Stefano Maestri (17:24)** Allora, sulle immagini lascio rispondere Alessio. Sul modello in s√© invece io ti dico che ho un'esperienza completamente diversa. Tant'√® che ho postato ‚Äî per stare sul tuo "sticazzi" ‚Äî ho postato invece su LinkedIn che spacca il culo ai passeri. E ne sono convinto, perch√© ho fatto la stessa cosa che hai fatto tu. Cio√® sono andato a prendere... io Gemini lo uso parecchio ultimamente, sono andato a prendere quello che ho fatto negli ultimi 10 giorni, pi√π o meno, con 2.5 e l'ho rifatto con 3.0 la sera del rilascio, almeno le cose principali. Allora, da un lato ti do ragione: sulla domanda "normale", quella in cui gli chiedo cose o faccio brainstorming o gli chiedo di scrivere un pezzo, tradurre ancora di pi√π... probabilmente la differenza c'√®. Se lo chiedo ad un madrelingua, la traduzione fluisce di pi√π, √® diversa, non √® identica, ma io non sono in grado di cogliere un miglioramento significativo. Per√≤ quando sono andato a prendere prompt dove gli chiedo di digerire grossi volumi di dati e fare cose, produrre ‚Äî triggero qualcuno dei nostri hater nascosti ‚Äî e di produrre pensiero, √® assolutamente game changer.

Io gli ho fatto fare questo compito specifico: sapete, sono stato al Devoxx, grande conferenza in Europa, quello Belgium. 248 video da un'ora circa. Non sono tutti da un'ora, ce ne sono alcuni da 3 ore, ma la maggior parte sono da un'ora. Gli ho chiesto di andare a scaricarsi dalla playlist tutti i video. Mi ha detto "No, guarda, mi spiace, io non sono capace perch√© YouTube non permette di fare questa cosa, per√≤ ci sono i trucchi, fai quella cosa qui e mi fornisci il CSV". Cos√¨ ho fatto. Lui si √® guardato tutti i video, mi ha generato 10 slide ‚Äî perch√© io gli avevo chiesto di fare 10 slide ‚Äî con i tre trend per ogni argomento o 10 trend in tutto, adesso non mi ricordo i numeri perch√© √® una cosa vecchia, con tutti i riferimenti ai video dove apre i video pi√π significativi su quel trend, di spiegare il trend eccetera eccetera.

Gemini 2.5 non c'era minimamente riuscito, cio√® ha tirato fuori una cosa che io, che ero alla conferenza, dico: "Ma cosa hai visto?". Cio√®, ha tirato a caso rispetto ai titoli dei video. 3.0 mi ha fatto uno slide deck che avrei potuto scrivere io per contenuti, che ero l√¨ e che mi sono ascoltato la maggior parte dei talk, poi ho rivisto in video di quello che era in parallelo e non riuscivo ad ascoltare. Questo compito mi ha particolarmente impressionato. Particolarmente.

**Alessio Soldano (20:47)** Io credo che ci sia un discorso proprio di complessit√† delle cose che si chiedono. Cio√® siamo partiti mesi, anni fa da modelli che capivano, facevano cose e sbagliavano spesso, eccetera. Mano a mano, un pezzettino alla volta abbiamo migliorato fino ad arrivare ad uno stato in cui comunque di base anche ci√≤ che c'era prima di Gemini 3 non √® che faceva proprio schifo, anzi. E quindi a volte √® difficile cogliere l'epsilon di miglioramento sui vari comportamenti, le varie specifiche attivit√† che si pu√≤ far fare a questi modelli. Tant'√® che se vai a vedere le specifiche, cio√® quando esce un nuovo modello forse ne parlavamo anche negli episodi scorsi, sono tutti i vari benchmark. S√¨, ne abbiamo parlato, e si dice "Ah, guarda, su questo benchmark questo nuovo modello √® il 2% migliore, 1% migliore eccetera". E se vai a vedere i risultati nei benchmark di Gemini 3, s√¨, migliora praticamente su tutti i fronti, di pochino qui e l√†, a parte un paio di metriche. E ce n'√® una in cui di base tutti i modelli erano bassi, tipo 30%, 20% una cosa cos√¨, e su questa Gemini 3 migliora tipo del 50%, una roba del genere.

**Stefano Maestri (22:18)** Che sono proprio le capacit√† agentiche, a cui ti riferisci, quel benchmark che misura le capacit√† agentiche che √® questa roba qua che io ho descritto a grandi linee con un mio esperimento.

**Alessio Soldano (22:22)** S√¨, e magari sull'uso generalista dell'utente medio si vede poco, per√≤ quando inizi a andare a vedere cose specifiche, ad esempio questa, la differenza c'√®.

**Stefano Maestri (22:46)** Io mi aspetto, non l'ho ancora provato, non ho ancora avuto modo perch√© appunto √® uscito ieri l'altro, ieri eravamo a una conferenza, io mi aspetto comunque dei grossi miglioramenti anche in CLI o "Antigravity", il loro nuovo IDE per fare sviluppo agentico. Non perch√© sia migliorato in maniera spropositata nella generazione della singola porzione di codice ‚Äî che √® una confusione che spesso si fa. Se tu guardi i benchmark, che ne so, di Qwen Code ‚Äî Alessio ha anche provato e nel suo blog ne ha parlato di questa cosa ‚Äî se tu guardi i benchmark di coding di Qwen Code sono molto vicini a quelli di Claude Code. I benchmark, dopo aver letto il tuo articolo sono andato a vedere dove differiscono molto: √® proprio su quel benchmark che nominavamo adesso, sulle capacit√† agentiche. Dov'√® che Claude Code √® sempre stato molto pi√π bravo di tutti gli altri? √à proprio nel fare tante cose una in fila all'altra che hanno a che fare con il codice. E quindi l'esperienza finale dall'utente che gli ha dato il prompt dicendogli "fammi il tic tac toe", per fare l'esempio pi√π stupido, √® molto migliore. Ma non perch√© √® pi√π bravo a scrivere la singola funzione, la singola funzione la scrivono bene tutti, √® pi√π bravo a mettere insieme le cose. E su quello, da benchmark, bisogna poi provarlo perch√© poi non √® soltanto il modello, c'√® anche tanta roba nel CLI vero e proprio. Per√≤ da benchmark Gemini 3 √® ancora pi√π bravo di Opus, ma nettamente pi√π bravo di Opus a fare planning. E le classiche capacit√† agentiche sono la capacit√† di fare planning e di eseguire compiti di larga scala per molto tempo tenendo tutto quanto connesso in buona sostanza. E su questo io dalle prime prove che ho fatto √® veramente veramente notevole e anche gli esempi che vedo su internet... per chi non segue Ethan Mollick, dovrebbe. Ethan Mollick su LinkedIn, che ha avuto accesso prima come beta tester, ha postato tutte le cose che nelle ultime due settimane ha provato e ci sono cose abbastanza impressionanti. Cio√® ha fatto tipo una macchina del tempo in cui, dandogli un prompt ragionevolmente lungo, Gemini 3 ha codificato questa applicazione in cui all'interno usa le API di Gemini per dire: tu gli dici data, giorno e ora e lui nel passato va a pescare le informazioni, nel futuro se le inventa e ti dice "Sei arrivato qui in questo posto, succede questo". Io ho provato a mettere Cremona a marzo del 2020 e mi ha descritto le strade deserte, il Covid, generato la foto e quant'altro. Secondo me √® un passo avanti interessante. Poi non so se tu pensavi altro, Paolo.

**Paolo Antinori (26:11)** No, non era tanto quello. Era nata in me l'effetto psicologico di vedere tutti quanti che dicono tutti la stessa cosa sui miei feed dei social media che mi genera repulsione. Di solito mi fa dire "Oh, questa roba √® davvero cos√¨ clamorosa oppure tutti si copiano, si stanno facendo generare i messaggi dall'AI e basta". Quindi questo effetto di diffidenza iniziale mi faceva venire dei dubbi. Ho provato, andava, ma andava anche il giorno prima, ho detto boh. Poi appunto per via anche della scala massiva del rilascio che hai citato tu prima, magari c'erano delle gemme nascoste qua e l√† in cui io non mi ci sono imbattuto e chi si preoccupa di fare il solito post da influencer molto poco profondo non cita nemmeno, e quindi mi chiedevo dove fosse tutto questo valore.

**Stefano Maestri (27:02)** S√¨, s√¨. No, no, ma sono vere entrambe le cose. Cio√® nel senso anche il mio feed √® stato invaso da un sacco di post chiaramente generati e con contenuto zero. Infatti io ho fatto un post vagamente polemico per chi l'ha visto, quello dello "spacca il culo ai passeri", ho scritto: evito di fare l'ennesimo post con la sintesi di quello che hanno gi√† detto nel blog ufficiale. Vi dico solo che l'ho provato e spacca il culo ai passeri. E l'avevo provato con quella cosa che ho descritto meglio qua, cio√® la roba del Devoxx. E l√¨ davvero a me ha colpito perch√© √® un compito che nessuno dei modelli era in grado di portare a termine prima, perch√© ci avevo provato. Era una cosa che mi serviva in realt√† nelle settimane scorse per fare un attimo la sintesi di tutto quello che avevo visto e non c'ero riuscito a farlo con nessun modello alla fine. E invece boh, Gemini 3 l'ha fatto e l'ha fatto bene. E poi ci sono le immagini... le immagini che avrai pasticciato tu, Alessio, appena uscito.

**Alessio Soldano (28:06)** S√¨, tra l'altro prima di arrivare alle immagini c'√® anche un miglioramento significativo che ho visto. Io ho visto un paio di esempi di gente che provava Nano Banana e Gemini 3\. Sugli aspetti multimodali di Gemini 3 hanno migliorato molto la capacit√† di... li chiamano visual puzzle, una roba del genere, cio√® di come il modello riesce a interpretare i dettagli delle immagini che vede. Per dire: gli si d√† un'immagine in input e si dice di fare un ragionamento su quell'immagine eccetera. Piuttosto che ‚Äî e qui invece si passa alla generazione delle immagini ‚Äî di fare ragionamenti legati alla forma piuttosto che i colori delle cose che ci sono nelle immagini. Per dire, c'√® un esempio che ho visto in cui c'√® un'immagine di una catasta di legna e c'√® un gatto ben nascosto e mimetizzato nella catasta di legna, esattamente del colore della legna eccetera, e si chiede al modello di trovare il gatto sostanzialmente. Nessun altro modello fino ad allora era riuscito; in questo caso lo identifica e dice "Guarda bene, sulla seconda pila di legna alla certa altezza trovi... si pu√≤ capire perch√© il pattern delle linee √® differente, eccetera". E quello corrisponde a uno di nuovo dei benchmark in cui si nota, a differenza di tutti gli altri, una miglioria significativa. Comunque passando a...

**Stefano Maestri (29:51)** Poi scusami Ale, per aggiungere un paio di cose pi√π interne. Adesso io non ho ancora letto bene, anche perch√© il paper vero e proprio non l'hanno ancora pubblicato, dicono che lo pubblicheranno, ma tra le righe degli articoli che sono usciti... intanto faccio un attimo uno screen sharing per far vedere un paio di esempi che portano loro che sono abbastanza impressionanti. Ma tra le righe di questi articoli c'√® da dire che dicono che usano le immagini e potenzialmente anche i video come step di reasoning, che √® un passo avanti doppio carpiato se funziona. Perch√© vuol dire non pi√π ragionare soltanto con token testuali, ma usare token video ‚Äî o video forse no, ma anche solo immagini ‚Äî per ragionare. Perch√© se vuoi ci riporta molto al modo che ha l'uomo di ragionare. Quante volte vi √® capitato di farvi uno schemino per ragionare meglio su un concetto? Loro cercano di introdurre questa cosa. Qui invece nel video che stavo condividendo analizza un video e testualmente dice quali sono gli errori nel giocare a padel, che √® abbastanza bello. E poi anche questo che fa una foto di tanti post-it e ne genera un website bilingue di questi post-it che diventa un vero e proprio libro, cio√® fa cose estremamente evolute, apparentemente spiega le neural network e cos√¨ via. Vabb√®, adesso non vi tedio con quelle che sono le cose che appunto potete vedere nel blog ufficiale che poi magari mettiamo in descrizione, per√≤ l'impressione √® che il passo avanti sia significativo. A me la cosa che ha colpito di pi√π √® leggere che nei passi di reasoning possono entrare le immagini, perch√© ha una complessit√† veramente alta da un punto di vista neural network fare quella roba l√¨, perch√© vuol dire portare le immagini ‚Äî immagini e testo ‚Äî nello stesso momento in latent space. Non sto ad andare nei dettagli, per√≤ √® comunque una cosa estremamente complessa da gestire e da monitorare che non porti a spike di allucinazioni, che speriamo di non vedere. Ma se l'hanno rilasciato, immagino che loro pensino di aver mitigato quella cosa l√¨.

**Alessio Soldano (32:32)** S√¨, mi verrebbe da dire la multimodalit√† spinta di pi√π verso il reasoning.

**Stefano Maestri (32:39)** Eh, multimodalit√† anche in latent space, non soltanto come input ed output, che poi era quello che pi√π o meno succedeva fino ad oggi. Scusa, ti ho interrotto, vai con le immagini.

**Alessio Soldano (32:47)** No, ma √® perfettamente collegato al discorso, nel senso che questa cosa qui va di pari passo con uno dei, diciamo, dei gruppi di migliorie pi√π significative di Nano Banana Pro. Il cui vero nome sarebbe Google 3 Pro Image, ma siccome Nano Banana ha avuto grande successo come nome della 2.5 si chiama ancora cos√¨. E ci sono, hanno fatto vedere diversi esempi. In particolar modo ce ne sono alcuni riguardo non tanto questo che stai mostrando adesso Stefano del testo, ma pi√π gli esempi riguardo la generazione ad esempio di immagini che spiegano determinati concetti. Tipo questo, ad esempio, fammelo vedere un po' pi√π da vicino... eh, s√¨, tipo questo. Oppure guarda, se prosegui ce n'√® altri dopo.

**Stefano Maestri (33:34)** Puoi scrollare in gi√π, guarda. S√¨, s√¨, vai ancora. Beh, anche questo √® notevole che vedo, perch√© una delle cose che faceva male Nano Banana rispetto a Imagen, che era l'altro modello di generazione immagini, era il testo.

**Alessio Soldano (33:46)** Quello era la generazione prima.

**Stefano Maestri (33:46)** Esatto. E s√¨, ma che ancora avevano tenuto in Vertex proprio perch√© quello faceva bene il testo, mentre Nano Banana faceva benissimo le immagini ma male il testo. E invece questo vedo che fa delle immagini con testo abbastanza notevole.

**Alessio Soldano (34:00)** Esatto. Quello √® l'altro blocco di miglioria, diciamo, sul testo. Testi coerenti, corretti e vabb√®, le immagini sono l√¨ da sole. Si commentano da sole, direi. Peraltro questa cosa qui si presta molto bene a tutti i discorsi di campagne pubblicitarie eccetera come uso pratico.

**Stefano Maestri (34:22)** Eh, infatti loro nell'annuncio parlano spesso delle campagne pubblicitarie.

**Alessio Soldano (34:44)** Ecco, quello che a me colpisce tantissimo √® questa cosa che stai mostrando adesso. Allora, √® scritto piccolissimo per cui non lo vedete, ma magari poi se andate a vedere la pagina lo leggete. C'√® il prompt con cui hanno generato questa immagine. Per chi non vede l'immagine, adesso stiamo vedendo uno schemino. Allora il prompt era: fammi tipo un'infografica in cui si spiega come funziona la generazione di corrente utilizzando i pannelli solari. Sostanzialmente quello che √® stato creato √® perfettamente chiaro, oltre a essere bello da vedere ‚Äî sembra quei grafici professionali che si fanno per le presentazioni quelle serie, diciamo. Ma a parte quello √® assolutamente corretto da un punto di vista scientifico, tecnico. E la miglioria qui √® stata proprio nel dire che, ok, questo modello di generazione immagini non √® tanto e solo... cio√® √® tanta roba nel generare le immagini, ma la parte diciamo di modello che c'√® prima rispetto alla generazione delle immagini √® a livello di Gemini 3, √® proprio Gemini 3 stesso. Per cui se con un classico modello di generazione immagini noi abbiamo un tokenizer, prima ancora il clip, insomma un qualcosa che va a capire quali sono i termini e le parole nel testo e da questa generiamo l'immagine, qui c'√® proprio tutto un ragionamento, un'intelligenza, mi verrebbe da dire, nel processare il problema come se fosse un vero LLM prima e poi tradurre tutta questa comprensione in un'immagine che spieghi. Perch√© nell'esempio l√¨ della generazione della corrente con il pannello solare, ma anche in questo della ricetta di cucina, nel prompt non ci sono scritti tutti i passaggi che dicono "Ok, fai per dire un'infografica in cui si fa vedere che prima pestiamo...".

**Stefano Maestri (36:54)** Questa che stiamo vedendo, il prompt dice: "Create an infographic that shows how to make elachi chai", che non so neanche cosa sia. Io immagino sia un t√®, un qualche tipo di bevanda, diciamo. Per√≤ questo √® il prompt fine.

**Alessio Soldano (37:07)** Esatto. Cio√®, non c'√® la ricetta. La ricetta l'ha recuperata il modello e ha capito tutti i passaggi e ha creato delle immagini che spiegano i vari passaggi correttamente.

**Stefano Maestri (37:25)** Mentre invece a livello di curiosit√†, l'altro prompt, quello dei pannelli solari, gli dice addirittura: il contenuto deve essere basato su questa pagina di Wikipedia. Quindi lui va a leggersi la pagina di Wikipedia e la trasforma in un'immagine facile da capire.

**Alessio Soldano (37:43)** Quindi l√¨ c'√® lo step, diciamo, di utilizzo di Tool esterno, si legge, comprende il testo. Cio√® siamo ben lontani da "generami un'immagine in cui c'√® il pannello solare, il sole, l'inverter, la casa, la batteria e i flussi sono da qui a l√† e io ti descrivo la cosa e tu la devi impaginare". Qui c'√® tanta intelligenza prima della generazione effettiva delle immagini. Le altre migliorie, se vuoi, sempre in questo gruppo sono, ad esempio, riguardanti alla traduzione e alla localizzazione delle idee, che √® quella esattamente qui sopra. Anche qui le immagini sono da guardare, cio√® fa la rilocalizzazione di cartelloni pubblicitari piuttosto che di prodotti. C'erano delle lattine con delle scritte...

**Stefano Maestri (38:41)** Il pullman anche con il tram.

**Alessio Soldano (38:59)** Esatto. A seconda qui in questa immagine viene rilocalizzata una pubblicit√† e siccome si passa dall'Inghilterra alla Germania o qualcosa del genere, il testo passa dall'inglese al tedesco, ma anche il double decker diventa un bel tram elettrico di Monaco o di Francoforte. E poi c'√® quest'altra cosa qua, no? Che serve appunto per campagne pubblicitarie, prototipi di design, cose di questo genere, per cui data un'idea, un'immagine anche 2D, la si veste sopra prodotti di ogni genere. Quindi questo era un blocco di migliorie che √® quello che onestamente mi ha colpito di pi√π, cio√® il fatto che siamo ben oltre la pura generazione di immagini, ma c'√® tutto un ragionamento prima. Poi c'√® quest'altro blocco di migliorie che, se vuoi, √® controllo e modifica delle immagini che abbiamo visto in tanti altri modelli, in realt√† probabilmente non fatti cos√¨ bene eccetera. Fammi le luci in un certo modo, l'inquadratura in un altro modo, la messa a fuoco di qui piuttosto che di l√†: questo genere di controlli fini, se vuoi fotografici o cinematografici, tradotti in prompt che vengono poi capiti dal modello. Quest'altra parte invece riguarda l'editing anche delle immagini. Di nuovo idee note implementate magari non cos√¨ bene in altri modelli, ma che vanno ad aggiungersi a tutte le migliorie di questo nuovo Nano Banana 3\. E upscaling... tra l'altro questo riguarda anche, cio√® c'erano dei rumors prima che uscisse Nano Banana 3, col fatto che avevano lasciato trapelare che si sarebbe chiamato Nano Banana 3 Pro, la gente aveva supposto che la vera miglioria sarebbe stata semplicemente pi√π dettagli, immagini a dimensioni superiori, upscaling migliorato eccetera. Come abbiamo visto c'√® ben altro, per√≤ c'√® anche quella cosa l√¨, per cui generazione di immagini con tantissimi dettagli in pi√π. C'√® il controllo dell'aspect ratio esatto.

**Stefano Maestri (41:08)** Generami un'immagine 16:9 piuttosto che ultrawide, cose cos√¨.

**Alessio Soldano (41:11)** Questo che mette insieme tante immagini in una sola. Carino, non l'avevo ancora visto questo. Esatto. Ci sono queste cose, anche qui esistono dei modelli specifici per fare queste cose: tipo ti do N immaginette di personaggi, me ne fai una nuova in cui ci sono tutti in una certa posa. Anche questa adesso √® disponibile direttamente insieme a tutto il resto.

**Stefano Maestri (41:52)** E cos'altro? Questa simile in qualche modo?

**Alessio Soldano (41:55)** Eh s√¨, in generale. Poi "Next Level Generation", qui parla della capacit√† di avere dettagli su tantissimi aspetti. Anche l√¨ quando scrivi un prompt magari tu gli fornisci tanti... ti soffermi molto su una certa parte dell'immagine, dei dettagli che vorresti, e non sempre tutto viene colto. Qui c'√® questa immagine in cui si vedono tutte le persone che sono ferme su questo sagrato con tutte le ombre e il dettaglio √® tanto in tanti punti differenti. Ieri giocando io e Stefano stavamo provando a generare un'immagine per vedere com'era migliorato e in passato abbiamo forse fatto vedere, quando comparavamo i modelli, generavamo immagini di "personaggio nella metropolitana a Milano". Io avevo ancora quel prompt, gliel'ho fatto fare di nuovo e poi gli ho detto "Aggiungimi pi√π persone nel vagone in metropolitana" e abbiamo ottenuto un vagone popolato di passeggeri in posizioni diverse che fanno cose differenti, degno di nota.

**Stefano Maestri (43:12)** Con il nome della fermata giusto, scritto giusto, non scritto "Gabiraldi" invece che "Garibaldi" e queste cose qua.

**Alessio Soldano (43:18)** Certo, perch√© poi quella √® il terzo e ultimo area di miglioramento che √® il testo, generazione di testo corretto. Tant'√®, quindi provate Nano Banana 3 Pro.

**Stefano Maestri (43:31)** Bene, bene. Quindi adesso che abbiamo fatto tutto quanto su Google, che ricordiamo √® lo sponsor di questa puntata... eh no, eh no\! Per√≤ gli altri non stanno fermi, no? Gli altri non stanno fermi perch√© pure GPT √® uscita con 5.1, appena prima perch√© era nell'aria che sarebbe uscito 3.0 e si diceva che sarebbe uscito per il Black Friday e non ci siamo andati molto lontani. E poi anche Grok √® uscita con la versione 4.1 che pare essere molto molto buona, cio√® prima in LM Arena in questo momento, cio√® gli utenti preferiscono le risposte di Grok 4.1 rispetto a quelle di Gemini 3.0 in LM Arena. Che per√≤, anche l√¨... LM Arena... Ecco, io devo dire questa cosa, ne abbiamo parlato con Alberto l'ultima volta, no? Dei benchmark e anche dall'esistenza di queste arene dove provare un modello, cio√® due modelli e vedere quale ti piace di pi√π. Abbiamo anche fatto vedere. Ci pensavo proprio l'altra sera quando provavo Gemini 3: che forse stanno diventando un pochino troppo verticali, un pochino troppo stretti per fare una valutazione vera del modello. Perch√© l√¨ dentro non hanno l'accesso a tool eccetera. Quindi la parte agentica che io ho descritto non sarei in grado di farla fare in LM Arena neanche a Gemini 3, no?

**Alessio Soldano (45:13)** E quindi torni al discorso tuo Paolo.

**Stefano Maestri (45:18)** Che se gli dai un prompt normale di conversazione secondo me ormai siamo arrivati con Gemini 2.5, GPT-5, Claude 4 e Grok 4... questi quattro "State of the Art", siamo arrivati ad un livello tale per cui con il prompt secco, tolte le immagini, parlo di testo, √® dura dire preferisco uno, preferisco l'altro. Diventa veramente a gusto di come ti ha risposto pi√π che di correttezza. S√¨, a parte quella volta che fa l'allucinazione, ok, per√≤ perch√© adesso i modelli stanno migliorando in capacit√† diverse, appunto, pi√π agentiche direttamente dentro al modello. E questo apre una discussione nuova: abbiamo sempre detto i benchmark, s√¨, hanno il loro valore, ma boh, molto meglio l'LM Arena perch√© √® l'uso reale degli utenti. Il problema che sta diventando √® che non √® pi√π l'uso reale degli utenti perch√© ne manca un pezzo grosso, non so. E mi apre anche un'altra domanda a me che ho ricevuto proprio stamattina due mail, una di Manus e una di Jen Spark, che sono questi sistemi ad agenti che sono nati nei mesi scorsi. Di base usano Claude sotto la maggior parte, ma ti danno un'esperienza utente agentica, quindi tu gli dici una cosa tipo "fammi le slide di bla bla bla" ‚Äî che non c'erano riusciti a fare quelli che avevo chiesto io ‚Äî per√≤ fanno compiti di questo tipo, lunghi eccetera. Chi ha scommesso su quello come startup e come societ√† che ci fa business deve trovare un selling point, perch√© sempre di pi√π GPT da una parte con Codex e comunque anche GPT con il Legend Mode, cos√¨ come Gemini ‚Äî vediamo adesso dall'altra ‚Äî e tutti gli altri e anche Claude con il suo CLI eccetera, sono diventati dei competitor anche di quelle soluzioni l√¨. Con il vantaggio che loro il loro modello sanno veramente come fare, no? Vanno a tentativi di prompt, hanno dalla loro gli scienziati che l'hanno definito il modello, quindi sapranno come sfruttarlo al massimo delle sue possibilit√†. Per cui mi chiedevo: tutte queste realt√† che in qualche modo sono nate sopra gli altri modelli devono veramente trovare un selling point, cos√¨ come Cursor un po' si sta riinventando perch√© sono arrivati tutti gli altri anche a fare codice. Non so se voi avete un'opinione particolare su questo, per√≤ io ce la vedo questa cosa qua, questo rischio da parte di chi ha fatto questo genere di investimenti.

**Paolo Antinori (48:41)** S√¨, √® interessante quello che dici. √à naturale nell'industria, nel senso: laddove matura un prodotto di solito riesce ad incorporare feature esterne. Pensate alle automobili quando compravamo le autoradio esterne, adesso non si usa quasi pi√π, l'autoradio fa quasi la maggior parte delle cose che ti servono ed √® stata inglobata, diciamo, dall'oggetto principale. E questo discorso rimane, forse lungimiranti coloro che inizialmente hanno deciso di concentrarsi su aspetti diversi, alcuni magari non funzionali. Ad esempio quelli che ti garantivano, che ne so, che il modello possa girare in locale, piuttosto quelli che ti garantiscono validazione sui dati che hanno composto il modello, piuttosto che questo genere di aspetti che se inizialmente uno magari non dava tanta importanza, adesso rimangono dei chiari elementi distintivi, laddove invece a livello di funzionalit√† si sta tutto un pochettino normalizzando.

**Stefano Maestri (49:39)** Eh s√¨, anche perch√© il passo qua di normalizzazione comparato a quello delle autoradio √® leggerissimamente pi√π rapido.

**Paolo Antinori (49:51)** Guarda, io ogni volta che devo accettare questa evidenza di un passo sempre pi√π lungo, non riesco a non pensare alla famosa singolarit√† di Kurzweil, che mi sembrava una cazzata, e che in realt√† sempre un pochettino un passo pi√π vicino ci si sta arrivando sempre pi√π velocemente. Quindi magari rimane sempre asintoticamente irraggiungibile, ma boh, sono meno scettico di quanto non ero in passato.

**Alessio Soldano (50:19)** Devi continuare a zoomare per vedere quanto distante siamo dall'asintoto?

**Paolo Antinori (50:23)** No, no, certo, certo. Il paradosso di Zenone rimane l√¨, cio√® la freccia deve sempre fare un punto a met√† strada di quello che ci rimane, quindi non arriva mai al bersaglio, per√≤ non lo so. Eh, boh, non lo so pi√π.

**Stefano Maestri (50:34)** Io sono d'accordo con te, Paolo, molto. Anche se porto ‚Äî non √® mia, ma porto la voce anche di chi scherzando e mi ha fatto molto sorridere in un post che ho letto in settimana ‚Äî dice un po' il contrario. Cio√® un anno fa Sam Altman postava su X dicendo "Siamo ad un passo dalla singolarit√†", adesso posta su X dicendo "Potete togliere i trattini da ChatGPT". Diciamo che ha abbassato un po' la mira nei suoi post...

**Alessio Soldano (51:21)** A gran furore.

**Paolo Antinori (51:23)** Come dici che l'hanno ottenuta? Hanno usato Gemini 3 per farsela risolvere su OpenAI?

**Stefano Maestri (51:23)** Non lo so. Allora, per√≤ guarda che qui mi apri e mi triggeri. Bravo, triggerami. Che √® invece una riflessione che io ho fatto in settimana ragionando su del codice che stavo scrivendo con un'altra persona, una persona che lavora con me. Stavamo pensando di fare un refactoring e ho iniziato a fare questo refactoring e devo dire che, sbagliando ‚Äî metto la premessa, sbagliando ‚Äî sono partito per abitudine: ho aperto il mio bravo Claude Code perch√© ormai √® diventata un'abitudine e ho cominciato a conversare con Claude per fargli fare il refactoring. A parte che non sono arrivato dove volevo arrivare dopo un quarto d'ora di conversazione, mi stavo gi√† innervosendo, ma poi mi sono fermato un attimo. Io adesso lavoro tantissimo col terminale e meno con l'IDE. Mi sono fermato un attimo e ho pensato: ma questo per√≤ √® un problema gi√† risolto? Perch√© ho bisogno di un AI? Cio√® tutti gli IDE del mondo mi rinominano una classe o mi spostano metodi da una classe all'altra, perch√© poi era un refactoring semplice, non √® che dovevo ribaltare il codice. Semplicemente avevamo delle classi grandi, volevamo spezzarle e fare un po' di SPI eccetera eccetera. Ma quello l√¨ √® un problema gi√† risolto, cio√® apri VS Code, JetBrains, quello che vuoi, tasto destro sul nome della classe, rinomina e lo fa.

Questa cosa qui √® secondo me il rischio che abbiamo: chi come me ha preso molta l'abitudine, e ancora di pi√π il rischio che hanno gli Junior o AI Native che arrivano e che vogliono fare tutto cos√¨. E sentivo anche dire poi questa cosa, io ero arrivato a pi√π o meno alla stessa conclusione, poi lui l'ha detta meglio eccetera. Ho sentito un'intervista di Martin Fowler ieri in macchina e diceva che ha riscritto il suo libro "Refactoring" 20 anni dopo. E quando ho fatto pubblicare questa cosa, primo pensiero che ho avuto: ho pensato cazzo, scrivi il libro del refactoring 20 anni dopo, cio√® non √® che √® cambiato il mondo. Poi continuo a pensare questa cosa, penso che sia un modo per fare revenue, ma il refactoring in s√© non serve farlo con un AI, cio√® √® un problema gi√† risolto. O all'AI diamo l'accesso al tool che lo fa gi√† e lo chiama per noi, va bene, √® comunque superfluo, ma questo ci sta. Ma chiedere in maniera conversazionale ad un CLI di fare quel refactoring che io faccio con quattro click di tasto destro in un IDE √® follia pura. E io ci son cascato, non so come ma... e questo vale sul nostro mondo perch√© √® facile, ma mi viene da dire su tante altre cose. Cio√® ci sono dei problemi che sono gi√† risolti che puoi fare con un conversazionale, ma anche no. Cio√® le immagini generate da Nano Banana: √® vero che posso fare delle cose mirabolanti che io non saprei mai fare con Gimp o Photoshop, ma se devo cambiare il colore di una cosa tutta solida, magari faccio prima a farci un click sopra e cambiare il colore o no?

**Alessio Soldano (55:12)** Posso ti interrompo per farti una domanda, ma a questo punto come vedi tu invece l'utilizzo delle AI per dire dentro al refactoring? Eh, succede che magari dici "cambia questa cosa" cos√¨ lui parte e tocca 50 classi e a un certo punto arriva un punto e ti dice "Ah, guarda, qui sono in dubbio, non so se cosa fare" e ti chiede l'input per risolvergli la questione e andare avanti.

**Stefano Maestri (55:37)** S√¨, s√¨, assolutamente. E credo, tra l'altro, che Cursor questa cosa la faccia. Mi pare di aver letto. Io non l'ho provato perch√© francamente sto usando soprattutto Claude Code, poi uso VS Code quando ho bisogno dell'IDE e non Cursor. Potrei pensare, non c'ho mai pensato, per√≤ effettivamente avendolo potrei usare Cursor come IDE sopra a Claude Code, perch√© no? Ma magari ci provo in settimana.

**Paolo Antinori (56:06)** Le cose che raccontavi tu, Stefano, mi hanno ricordato in realt√† riflessioni che avevamo gi√† fatto in passato da queste parti ed era il timore di abusare lo strumento per funzioni che non sono idiomatiche. Che √® quello che hai detto tu con l'esempio articolato del refactoring, ma la mia versione pi√π semplificata √® usare ChatGPT per fargli fare 7x3. Lui lo fa. C'ho bisogno di una rete e una GPU per darmi questa risposta? No. Poi adesso sappiamo che 7x3 probabilmente lui lo ottimizza e lo fa fare a una shell in Python contestualmente, che √® ragionevole, ma √® una di quelle cose per cui tecnicamente bastava il Commodore 64 per farlo. Esatto. Ma se volete per√≤... questo √® chiaro, siamo dei programmatori, questo √® un chiaro esempio di antipattern. La linea per√≤ diventa un pochettino pi√π sfocata laddove gli si chiedono delle informazioni. L'altro esempio tipico di tutti quelli che provano un LLM √® "Qual √® la capitale della Francia?" e ti risponde Parigi. Bene, serviva l'LLM per farlo? Anche Google lo sapeva qual era la capitale della Francia quando non usavano le AI. Forse lo sa anche chi l'ha chiesto.

**Stefano Maestri (57:11)** Forse anche chi l'ha chiesto lo sapeva.

**Paolo Antinori (57:25)** E a livello, diciamo, filosofico o di filosofia del linguaggio, di ontologia, di Kant, mondo delle idee, quello che volete... come si definisce una domanda per cui vale la pena sfruttare questa tecnologia? Quindi vale la pena chiedere a un modello rispetto a quelle per cui invece no, sono note. Dobbiamo avere una serie infinita di euristiche che lascia un po' il tempo che trova o c'√® una definizione universale che riesce a far s√¨ che il modello o del software intorno a lui autodeterminino quando deva farlo oppure no. Quindi questo dubbio...

**Stefano Maestri (57:52)** Bisogna chiederlo al modello.

**Paolo Antinori (57:54)** Eh, devo chiederlo al modello. Ma... no, cio√® rimane una domanda aperta. Dobbiamo chiedere a Noam Chomsky o quella gente l√¨ che ci dia una risposta, perch√© io non so se ci arrivo alla mia risposta da solo.

**Alessio Soldano (58:04)** Io ci vedo quello che probabilmente vuole o vorr√† fare o in parte sta gi√† facendo Google in questa cosa qui. Cio√® quando una domanda √® da motore di ricerca e quando √® da LLM. Nel momento in cui c'√® lo stesso tool che ti d√† la risposta e decide di offrirti la risposta LLM piuttosto che i risultati secchi, si prende il tool, come dire, l'onere di capire come risponderti e quale tipo di investimento di risorsa fare.

**Paolo Antinori (58:34)** Ma infatti era uno degli highlights del routing interno dell'ultimo rilascio di GPT-5 ‚Äî adesso sto andando in confusione coi numeri ‚Äî erano loro che dicevano quale modello usare e quel che ci leggo tra le righe anche quale tool usare sotto sotto, quindi in realt√†...

**Stefano Maestri (58:34)** S√¨, s√¨, s√¨. Eh, ma √® anche parte di "AI mode" di Google che diceva Alessio: adesso quando cerchi a volte ti d√† l'elenco di link e a volte ti d√† la risposta sintetizzata e decide lui quando farlo. Per√≤ pi√π in generale, secondo me, hai ragione Paolo, √® un problema in qualche modo di UX, cio√® di interazione con l'utente. E se vuoi si va alla Apple maniera: l'utente non sa di che cosa ha bisogno, ma io s√¨. Perch√© questo √® il loro mantra. E mentre parlavi proprio mi viene in mente che qui, come Google ha fatto il recupero del gap che aveva, su questa cosa Apple potrebbe fare un grosso recupero del gap che ha specializzandosi su questa cosa. Perch√© tutti i tool che utilizzer√† l'utente sempre di pi√π saranno AI infused, ma sempre di pi√π avranno bisogno di questo genere di governance. E siccome la mia fiducia in Microsoft in questa cosa √® meno di zero, vedo delle grosse possibilit√† per Apple. Cos√¨ mi sono tirato... nel senso anche Microsoft √® sponsor di questa puntata.

**Paolo Antinori (1:00:06)** Io forse... io forse la riassumerei in parole diverse, per√≤ questa cosa che hai detto: ci piace farci dire da Apple cosa fare e quindi potrebbe essere un'occasione per Apple per dirci cosa fare nell'ambito dell'AI.

**Stefano Maestri (1:00:23)** S√¨? Mettila gi√π come preferisci, per√≤ dal punto di vista di Apple il concetto √® lo stesso. Per√≤ dal punto di vista nostro pi√π generale il discorso che facevamo √® in realt√† il gap che va colmato da qualcuno oggi √® proprio quello l√¨, cio√® aiutare l'utente a utilizzare il tool giusto per il compito giusto. A volte il tool √® un LLM, a volte √® un mouse, a volte √® toccare lo schermo, a volte parlare, a volte muovere la mano perch√© c'√® il braccialetto di Zuckerberg. Ce ne son tanti, no?

**Paolo Antinori (1:01:08)** Per√≤, per quanto sia assolutamente vero quello che dici, in un altro podcast avevo sentito qualcuno fare un'altra riflessione ed era a commento di quando era uscito questo modello, questa funzionalit√† di routing interna di ChatGPT che andava su modello pi√π light quando riteneva il caso. Le persone dicevano "Ma cavolo, io sto pagando un abbonamento pro, fanculo la versione leggera, fammi sempre la versione pi√π potente, sto pagando e dammi tutto quello che posso, mi rispetto". E la gente rideva perch√© dicevano: √® probabilmente un comportamento umano, un'aspettativa umana, per cui lo facciamo. Quando Alessio ha comprato un computer ha comprato il computer pi√π potente che poteva comprare, forse pi√π anche di quanto gli serviva. Lo facciamo sempre perch√© ci viene da...

**Alessio Soldano (1:01:33)** Vabb√®, adesso, avessero fatto la versione da 256 GB la prendevo.

**Stefano Maestri (1:01:53)** Ne vorrebbe cinque da mettere in parallelo.

**Paolo Antinori (1:02:00)** Sono abbastanza certo che ognuno di noi conosca qualcuno che ha una macchina fuoristrada per guidarci in citt√†. Ce l'ha lo stesso perch√© ha deciso che la voleva. E quindi questa parte irrazionale le societ√† ce la stanno razionalizzando per noi, ma noi rimaniamo sempre troppo irrazionali su questo aspetto e non so se ne usciremo mai.

**Stefano Maestri (1:02:21)** Beh, ma infatti forse devi cambiare il modello di vendita, no? Dire: io ti vendo la soluzione a tutti i tuoi problemi e la paghi cos√¨ e ti d√† sempre la cosa migliore da fare e non ti devi curare del modello. In questo momento siamo in un momento in cui la competizione si fa a colpi di benchmark e quindi quell'aspetto l√¨ che dici tu √® predominante. Ma perch√© √® un po' l'effetto pubblicit√†, no? Cio√®, se dici che queste scarpe da running sono migliori, ammortizzano di pi√π, eccetera eccetera, anche se vai a camminare finisce che, se te le puoi permettere, compri quelle scarpe da running perch√© c'√® questa spinta. Nel momento in cui invece si sposta l'attenzione su qualcos'altro, sul risultato, pu√≤ essere che si riesca meglio. Non lo so, √® un po' quello che ha fatto Apple, che dice "A te non ti interessa che cosa utilizzare. L'importante per te √® che quando fai la foto sia puffosa e carina e puoi postarla sui tuoi social. Non ti interessa che lenti ho messo". Perch√© sai, i cinesi fanno la guerra su quali lenti mettono, Zeiss piuttosto che altre sul cellulare, ma chi sa di fotografia sa che su quelle dimensioni l√¨ contano fino a un certo punto. Apple non sta a dirti, Apple ti dice "faccio la fotografia puffosa".

**Alessio Soldano (1:03:35)** Dicevo, si potrebbe fare anche una piccola digressione sull'aspetto: ok non ci interessa tanto pi√π il risultato, ma anche come l'abbiamo ottenuto, o magari quanto ci √® costato ottenere il risultato. In questi giorni abbiamo visto un articolo in parte legato agli aspetti green dell'intelligenza artificiale, di tutto questo utilizzo che stiamo facendo dell'intelligenza artificiale, e c'era un ragionamento di: ragioniamo in termini di intelligenza per watt o di inferenza per watt. Perch√© anche oggi abbiamo parlato "ah guarda questo modello fa questa cosa leggerissimamente meglio eccetera", ma magari a un 1% in meno di risultato nel benchmark avevamo un'alternativa che consumava la met√†, per dire. E allora per l'utente del caso medio o banale che diceva Paolo prima, piuttosto che per l'utente che cerca la capitale della Francia, √® sufficiente un LLM da 2 billion parameter e il risultato √® lo stesso, √® uguale. E allora magari la tensione va spostata su quello e quindi il routing intelligente che dice Paolo potrebbe essere per andare su modello ancora pi√π piccolo ‚Äî non soltanto reasoning piuttosto che non reasoning ‚Äî e oppure sul fatto che un certo tipo di modello lo possiamo magari far girare su hardware differente che ha un profilo di consumo completamente differente. Che magari questo hardware non √® in grado di... non pu√≤ essere utilizzato per gli ultimi modelli, le ultime soluzioni disponibili, per√≤ √® molto pi√π efficiente. √à un aspetto che io in parte sto guardando, vabb√®, di nuovo tornando al mio computer, ma in generale ‚Äî per chi non lo sapesse ‚Äî le CPU AMD Ryzen AI Max 9 hanno una parte di NPU e una parte di GPU. Entrambe le parti di questo processore possono fare conti che servono per l'inferenza con prestazioni differenti perch√© sono pensate per fare cose leggermente differenti. Non con tutto puoi fare tutto, per√≤ di nuovo, se quello che ti interessa √® non soltanto il risultato, ma quanta energia, quante risorse, quanto calore hai generato... cos√¨, come dice Stefano, non scaldi la stanza eccetera, anche questi aspetti vanno considerati.

**Paolo Antinori (1:07:05)** E direi che con questo dovremmo avere coperto la richiesta di essere pi√π nerd.

**Stefano Maestri (1:07:08)** S√¨, direi di s√¨. Direi di s√¨. Abbiamo coperto la richiesta di essere pi√π nerd, ma non √® mai abbastanza. Anche pi√π green, dai, anche pi√π green. E promettiamo che saremo pi√π nerd la prossima puntata, magari quella dopo saremo addirittura anche pi√π enterprise perch√© faccio uno spoiler: non √® uscito il radar semestrale di Thoughtworks sulla tecnologia in generale ‚Äî ma che √® 95% AI, come potete immaginare ‚Äî adottata in azienda. √à uscito l'altro giorno, io gli ho dato un'occhiata. Ne parler√≤ un pochino in newsletter, ma magari non il prossimo episodio, quello dopo, avendolo guardato tutti con un attimo di calma, potremmo tirarne fuori qualcosa che sono certo che ai nostri ascoltatori interessa, anche per i feedback che abbiamo ricevuto ieri di quella che √® l'applicazione in mondo enterprise di queste tecnologie. E quello ci d√† degli ottimi spunti. Quindi se voi ascoltatori volete andare a vedervelo, √® gi√† disponibile, lo scaricate gratuitamente da Thoughtworks e poi noi sicuramente ne parleremo, magari non settimana prossima che ‚Äî spoiler ‚Äî abbiamo un ospite. No spoiler chi √®, ma appunto magari ci ripromettiamo di parlare degli aspetti pi√π enterprise dell'AI. Anche perch√© nel frattempo siamo gi√† alla classica ora e 8 minuti, quando io o qualcun altro fa notare che siamo a 1 ora e 8 minuti e dobbiamo cominciare a chiudere per stare nell'ora e 10 circa che ci diamo come target. Per cui se avete altre riflessioni che non potete trattenere fino alla settimana prossima, fatele ora o ci vediamo settimana prossima.

**Alessio Soldano (1:09:02)** Iscrivetevi al canale.

**Stefano Maestri (1:09:04)** Iscrivetevi assolutamente al canale, ascoltate tutte le puntate, tutte le interviste. Facciamo le marchette. Paolo scuote la testa. Ma perch√© non sa ancora che da settimana prossima sar√† lui a dover fare le pubblicit√†. Avremo proprio delle pubblicit√†, abbiamo detto Apple, Google, abbiamo anche sponsor importanti e Paolo far√† il momento Mastrota cercando di vendere un telefonino cinese spacciandolo per Rune.

**Paolo Antinori (1:09:37)** Mastrota, non √® un podcast per giovani questo, eh. Mi raccomando.

**Stefano Maestri (1:09:41)** Non √® un podcast... eh, vabb√®. Io non guardo la televisione da 10 anni, non so chi adesso fa le televendite. Figurati che in un'intervista, la prima, ho anche detto alla fine che mi mettevo il cappello di Marzullo e poi mi son fermato un attimo, ho detto "per chi sa chi √® Marzullo", perch√© va bene. Ciao a tutti.

**Alessio Soldano (1:10:06)** Ciao a tutti\!

**Paolo Antinori (1:10:06)** Ciao. Ciao.

üëâ [Ascolta su Spotify](https://open.spotify.com/show/16dTKEEtKkIzhr1JJNMmSF?si=900902f2dca8442e)<br/>
üëâ [Guarda su YouTube](https://www.youtube.com/channel/UCYQgzIby7QHkXBonTWk-2Fg)<br/>
üëâ [Segui su LinkedIn](https://www.linkedin.com/company/risorseartificiali)<br/>
