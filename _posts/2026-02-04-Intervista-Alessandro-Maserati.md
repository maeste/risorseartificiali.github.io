---
title: "Intervista ad Alessandro Maserati:AGI allineamento e consapevolezza;  il futuro di societÃ  e lavoro"
categories:
  - Interviste
tags:

  - AI
  - LLM
  - AGI
layout: single
author_profile: true
---


{% include video id="XP2jiPxFtPk" provider="youtube" %}

ðŸ‘‰ [Ascolta su Spotify](https://open.spotify.com/show/16dTKEEtKkIzhr1JJNMmSF?si=900902f2dca8442e)<br/>
ðŸ‘‰ [Guarda su YouTube](https://www.youtube.com/channel/UCYQgzIby7QHkXBonTWk-2Fg)<br/>
ðŸ‘‰ [Segui su LinkedIn](https://www.linkedin.com/company/risorseartificiali)<br/>

# Trascrizione: Intervista Risorse Artificiali - Alessandro Maserati

## [00:00] Introduzione e presentazione di Alessandro Maserati

**Stefano Maestri**
> Ecco, ciao a tutte e tutti e bentornati alle nostre interviste, interviste di risorse artificiali. Oggi il nuovo ospite, solita domanda rompighiaccio e poi lasciamo che si autopresenti come sempre. La domanda rompighiaccio mia Ã¨ sempre la stessa che faccio a tutti perchÃ© mi piace tanto poi provare a ricollegare dopo che abbiamo chiacchierato questa cosa. Il tuo gioco o giocattolo preferito, quando eri piccolo o anche oggi? Non lo so.

**Alessandro Maserati**
> Partiamo subito con una domanda difficile. Ma allora tanti giochi mi sono piaciuti molto. Forse se devo sceglierne uno direi Lego, per la possibilitÃ  che offrivano di ricombinarsi ogni volta in mille modi diversi e consentire di mettere a terra creativitÃ , ma anche in qualche modo creativitÃ  vincolata dei limiti che fornivano i vari pezzi. Quindi sÃ¬, direi Lego. Assolutamente.

**Stefano Maestri**
> Beh, una delle risposte che ho ricevuto piÃ¹ volte, che sarebbe anche la mia, tra l'altro. E quindi adesso lascio a te l'onore e onere di raccontarci chi sei. Io dico soltanto Alessandro Maserati e poi raccontaci un po' da dove vieni, insomma, qual Ã¨ stato un po' il tuo percorso professionale, no? Poi alla fine io dico sempre che non ci si dovrebbe chiudere in un titolo o in quello che si fa adesso, ma c'Ã¨ tutto un percorso che ci ha portato dove siamo, no?

**Alessandro Maserati**
> Certo, certo. Beh, allora io diciamo il mio percorso accademico nasce studiando matematica, alla Scuola Normale a Pisa e poi, diciamo, subito all'epoca ero abbastanza focalizzato su studi astratti, ho fatto una tesi su logica pura, quindi la cosa piÃ¹ lontana dalla realtÃ , dal business che si possa immaginare. PerÃ² subito dopo la laurea mi sono spostato nel mondo consulenza strategica, sono stato un po' in Boston Consulting, appunto, dove giravo un po' per le aziende ad aiutarle a definire la strategia, a migliorare i propri processi, per poi approdare in un'azienda svizzera che si occupava di sviluppo software inizialmente per il settore finanziario e poi seguendo un gruppo di persone in uno spin-off che Ã¨ finito sostanzialmente a occuparsi di digital transformation e assistenza alle aziende nei processi di adozione dell'intelligenza artificiale. E io all'interno dell'azienda mi occupo proprio della parte di intelligenza artificiale, di fatto me ne occupo dal 2017. Ecco, quindi noi abbiamo iniziato un po' prima dell'arrivo di chat GPT, poi chiaramente quando tutto il mondo si Ã¨ accorto che c'era una grossa opportunitÃ  lÃ¬, il business, insomma, Ã¨ cambiato abbastanza radicalmente. E poi diciamo sul piano personale questo discorso intelligenza artificiale mi appassiona tantissimo e dedico veramente tanto tempo ad approfondire, a studiare tutto quello che gira intorno a quel mondo. Quindi non solo le implicazioni business, ma anche proprio come funzionano le intelligenze artificiali e come stanno evolvendo.

## [03:03] Percorso professionale e AI

**Stefano Maestri**
> Ecco, bene. Io adesso su questo poi ho una domanda in particolare su questo collegamento. Intanto perÃ² raccontiamo questa cosa ai nostri ascoltatori. Io e Alessandro non ci conosciamo di persona. Semplicemente io l'ho contattato per due motivi. Ho letto tante sue cose, tanti post su LinkedIn, e prima aveva scritto anche qualche anno fa, ormai almeno un paio di anni fa, degli articoli sull'intelligenza artificiale che ricordo di aver letto. Poi ho sentito una sua altra intervista che peraltro vi consiglio, tanto credo che toccheremo temi un pochino diversi, con Luca Foresti e mi Ã¨ molto piaciuta. Ci siamo sentiti e lui Ã¨ molto disponibile, molto interessato a partecipare alla divulgazione sull'AI, che Ã¨ un po' quello che facciamo noi in questo podcast, e quindi siamo qua cosÃ¬ senza segreti, non Ã¨ che stiamo raccontando che chissÃ  da quanto tempo ci conosciamo, per cui un po' ci conosciamo anche noi nell'intervista e in un certo senso, anche se abbiamo parlato un attimo ovviamente prima. Ma allora io ti faccio una domanda e cito una cosa che c'Ã¨ proprio sul tuo profilo LinkedIn in cui scrivi che la tua missione Ã¨ "bridging the gap between AI research and business application", cioÃ¨ unire questi due mondi, no? L'applicazione del business e il mondo della ricerca AI. Due cose: perchÃ© questo interesse per questo ponte eccetera, e come il tuo background prima accademico che ci hai raccontato teorico, matematico, eccetera, e poi invece il mondo del management â€” Boston Consulting, per chi non lo conoscesse, fa management mediamente high-level, Ã¨ una delle societÃ  di consulenza sui livelli molto alti in azienda piÃ¹ famose al mondo â€” e quindi avrai visto tutta quell'altra prospettiva che con la prospettiva accademica forse non Ã¨ vicinissima?

**Alessandro Maserati**
> Certo. SÃ¬. Allora, beh, perchÃ© quella Ã¨ la mia missione, diciamo. Io credo che ci sia, ma questo lo credo da molti anni, un gap enorme tra le potenzialitÃ  offerte dalla tecnologia e quello che effettivamente viene utilizzato nell'ambito business.

## [06:12] Il gap tra ricerca e business

**Alessandro Maserati**
> E tra l'altro, per assurdo, questo gap ha continuato ad allargarsi negli ultimi anni. Adesso giornalmente vedo nelle aziende proprio una distanza siderale tra quello che la tecnologia offre e quello che le aziende usano e obiettivamente Ã¨ uno spreco, no? CioÃ¨, perchÃ© un'azienda deve ancora lavorare con carta e penna quando mandiamo i razzi nello spazio? CioÃ¨ Ã¨ un peccato che ci sia tutto questo potenziale inespresso. A me piace la sfida di provare a portare questo potenziale a terra nel concretizzarlo, perchÃ© nel tradurre il mondo delle AI nel linguaggio business si Ã¨ obbligati a capirlo veramente in profonditÃ  e quindi diciamo Ã¨ una sfida che raccolgo volentieri perchÃ© mi forza a capire meglio le cose, mi stimola a cercare di cambiare la prospettiva. Per me una cosa che ha sempre entusiasmato molto Ã¨ prendere un concetto e guardarlo da una parte originale e vedere cosa ne viene fuori, no? In matematica si impara spesso che cambiando la prospettiva degli oggetti che si studiano emergono delle proprietÃ  inaspettate e questo mi piace molto farlo anche appunto durante il mio lavoro. E devo dire che in qualche modo sono stato molto fortunato a fare quasi accidentalmente questo percorso che si Ã¨ mosso dalla matematica al mondo della consulenza, perchÃ© oggi credo che per capire veramente le AI servano entrambe queste prospettive. Da un lato perchÃ© obiettivamente leggersi tutti i paper che escono, dei nuovi modelli, dei modi in cui li fanno crescere eccetera, serve avere un po' un background tecnico, ecco, non necessariamente matematica, magari informatica, fisica, perÃ² insomma serve un po' quel linguaggio lÃ¬. E serve anche, come dire, l'attitudine ad approcciare problemi complessi, a non lasciarsi abbattere dalla prima volta che non li si capisce, avere l'umiltÃ  di essere consapevoli che la maggior parte di quelle cose comunque saranno difficili. Insomma ecco, credo una serie di skill che studiare matematica consente di apprendere. E poi serve anche avere perÃ² l'occhio business perchÃ© sennÃ², come dire, c'Ã¨ il rischio appunto di continuare a spingere in avanti la ricerca accademica senza mai concretizzarla nella realtÃ  quotidiana. Obiettivamente le prospettive di un manager e di uno scienziato sono completamente diverse, cioÃ¨ al manager interessa il fare, poi il perchÃ© si riesca a farlo interessa molto meno, no? Quindi, ecco, conciliare queste due prospettive credo che possa proprio aiutare a chiudere quel gap e a trasferire un potenziale inespresso in un valore aggiunto per le aziende.

## [08:55] Adozione dellâ€™Intelligenza Artificiale nelle aziende

**Stefano Maestri**
> Una delle cose che dicevo in uno degli ultimi episodi nostri normali, diciamo, di sabato, Ã¨ proprio che c'Ã¨ un attrito di adozione dell'artificial intelligence, soprattutto nel mondo enterprise, quindi un po' quello che probabilmente hai visto tu, le grandi enterprise, le grandi banche, gli ambienti regolamentati. Io credo che vada molto vicino anche al fatto di non saper spiegare che cosa succede dentro i modelli. CioÃ¨, il manager credo abbia bisogno anche di poter dire "Ok, Ã¨ stata presa questa decisione, perchÃ© non ho dato questo mutuo" â€” per banalizzare un attimo â€” perchÃ© anche da un punto di vista di regolamentazione proprio. E quindi credo che ci sia un po' questo gap per cui ti chiedo: tu pensi oggi che saper spiegare che cosa fa l'AI quando ci si interfaccia con il management sia una cosa che porta anche business, al di lÃ  del fatto che sia interessante in sÃ© essere capaci di andare dentro a quella cosa lÃ¬? Ti ha aperto qualche porta in piÃ¹, cioÃ¨ quando parli con il manager, o invece il contrario, perchÃ© la reazione puÃ² essere...

**Alessandro Maserati**
> Ma allora guarda, in realtÃ  io non sono tanto d'accordo con questa lettura e mi spiego. Allora, io credo che in generale la societÃ  sia sempre molto lenta ad adottare le innovazioni tecnologiche. A me Ã¨ capitato tante volte nel corso degli anni di vedere un'innovazione, dire "Wow, questa cosa qui nel giro di una settimana la userÃ  tutto il mondo" e poi vedere che passano 10 anni e ancora l'adozione Ã¨ al 70%, no? Un esempio a caso, lo shopping online: Amazon esiste da un sacco di anni e in teoria gran parte degli acquisti sono, come dire, sulla carta piÃ¹ efficienti se fatti online, nel senso che uno ha piÃ¹ scelta, non ha il problema della logistica di andare fisicamente nel negozio, eccetera, perÃ² in realtÃ  la penetrazione sul mercato Ã¨ stata lentissima da un certo punto di vista, no? In generale, come dire, le persone, le societÃ , le aziende sono molto lente ad adottare le innovazioni. Anche se pensiamo, non so, quanto tempo Ã¨ servito perchÃ© diventasse comune usare le email; le email in teoria dal giorno dopo in cui esistevano potenzialmente avrebbero avuto, come dire, la potenzialitÃ  di essere adottate su larga scala, ma ci sono voluti piÃ¹ di 20 anni. O gli smartphone, insomma esempi sono tantissimi, no? CioÃ¨, in generale la societÃ  Ã¨ molto lenta ad adottare le cose per abitudini, per processi consolidati, per paura al cambiamento, per avversione al nuovo, insomma tante ragioni dietro, ma di fatto la societÃ  Ã¨ sempre stata molto lenta. Il problema Ã¨ che a fronte di una societÃ  che, diciamo, io congetturo abbia una sorta di tasso massimo di adozione delle novitÃ  â€” cioÃ¨ tu non puoi spingere in un'azienda, in una societÃ  troppe novitÃ  in contemporanea, c'Ã¨ una certa soglia di cambiamento massimo che le aziende riescono a sopportare â€” questa soglia probabilmente era giÃ  satura con la rivoluzione del mondo digitale giÃ  15 anni fa. Il problema Ã¨ che l'AI ha incredibilmente accelerato l'evoluzione tecnologica e quindi a fronte di un assorbimento di novitÃ  che procede a tasso costante, perchÃ© ormai era saturo, la tecnologia ha accelerato talmente tanto che in mezzo si Ã¨ aperto questo gap enorme che secondo me puÃ² essere chiuso solamente da aziende create dal nulla che non si portano dietro questo bagaglio di inerzia, no, che le obbliga a cambiare una cosa alla volta, ma possono dire "Ok, da domani mattina facciamo la cosa nuova". E questo si Ã¨ visto, non so, nel settore finanziario: le banche che hanno innovato sono quelle create da zero, non sono grossi gruppi che sono lÃ¬ da una vita, no? Ma in tanti altri settori si Ã¨ visto, anche nell'home, insomma, in generale.

**Stefano Maestri**
> Anche nell'automobile, un automotive, se vuoi.

**Alessandro Maserati**
> L'automobile. Esatto. Esattamente. CioÃ¨, in quasi tutti i settori le aziende che sono lÃ¬ non riescono ad assorbire l'innovazione troppo velocemente. Quindi questo credo che sia un po' la ragione per cui in generale ad oggi vediamo ancora un'adozione molto molto lenta.

## [12:20] SpiegabilitÃ  dei modelli AI

**Alessandro Maserati**
> Per quello che riguarda la spiegabilitÃ , allora io credo che almeno da un anno a questa parte, da quando sono arrivati i reasoning model, no? Da o1 di OpenAI in avanti, oggi Ã¨ possibile far scrivere all'AI un ragionamento step by step comprensibile che spieghi certe decisioni. CioÃ¨ oggi potremmo avere un'AI â€” io potrei prendere GPT 5.2 e dirgli "Ok, prendi tutti questi dati, fai una valutazione se devo dargli o no il mutuo e scrivimi il ragionamento che hai fatto". Quindi, in qualche modo, potrei tirar fuori delle spiegazioni. Oppure se volessi usare dei sistemi un po' piÃ¹ avanzati, diciamo quelli un po' piÃ¹ tradizionali, quindi machine learning puro classico, su tutti i dati, tutte le persone a cui ho erogato mutuo negli ultimi 20 anni, anche lÃ¬ in realtÃ  esistono dei sistemi di machine learning â€” penso agli alberi, alle foreste, insomma, in generale algoritmi di machine learning che non sono reti neurali â€” ma che sono perfettamente spiegabili una volta addestrati, no? Quindi in generale io credo che non sia quello l'ostacolo, ma sia proprio il fatto che, appunto, le aziende sono lente ad adottare i modelli. Ãˆ chiaro che in generale far comprendere al management non tanto come funzionano intimamente i modelli, ma il potenziale che hanno, cioÃ¨ che valori possono generare, puÃ² essere il trigger, la scintilla, l'innesco per accelerare l'adozione. Quindi credo che sÃ¬, spiegare l'AI possa facilitare la sua propagazione all'interno della societÃ , ma non tanto spiegare come funziona quanto spiegare che opportunitÃ  puÃ² generare, che valore puÃ² generare, quali sono i rischi effettivi, non i rischi teorici, quali sono i costi effettivi e cosÃ¬ via. Quindi in generale penso che la divulgazione sia, come dire, il primo step nella strada verso l'adozione di massa, ma che non sia tanto necessario spiegare come funziona dentro quanto spiegare gli impatti. D'altronde capire come funziona dentro aiuta, no, poi a costruire la narrazione su quali sono gli impatti, perchÃ© avvengono, cosa aspettarci tra 6 mesi, eccetera. Quindi io credo che, come dire, sia fondamentale la comprensione intima di questi modelli per chi poi li deve divulgare, sia meno necessaria al management. Al management interessa cosa fanno, quanto mi costano, che rischi mi danno e come faccio ad adottarla.

## [15:20] AGI: definizioni, paure e fraintendimenti

**Stefano Maestri**
> SÃ¬, concordo su molta di questa analisi. Hai usato due parole che mi hanno acceso una domanda. Una era "paura" e l'altra parola era "tra 6 mesi". E da qui mi viene la domanda che viene abbastanza spontanea anche da un articolo che hai pubblicato negli ultimi giorni proprio tu su LinkedIn, un articolo di Nature che parla del fatto che forse l'AGI, l'Artificial General Intelligence, non Ã¨ cosÃ¬ lontana come ci siamo raccontati fino a ieri, anzi forse Ã¨ giÃ  qua. E che Ã¨ un po' quello che continua a ripetere anche Dario Amodei di Anthropic. Hassabis Ã¨ un po' piÃ¹ prudente, ma comunque hanno avuto questa conversazione e Amodei soprattutto ha scritto il suo essay di fine anno, era chiamato "L'adolescenza della tecnologia" e tutte le paure che ci possono girare attorno. Quindi la mia domanda Ã¨: intanto per gli ascoltatori la tua definizione â€” mi permetto di dire tua perchÃ© ne ho giÃ  sentite talmente tante che non so se c'Ã¨ piÃ¹ una definizione univoca di artificial general intelligence â€” e come la vedi? Quanto siamo distanti? PerchÃ© fa paura? Quanta paura fa?

**Alessandro Maserati**
> Allora, beh, tante questioni sul tavolo. Allora, innanzitutto io non ho una definizione in cui credo fermamente. Credo che sia opportuno adottare definizioni diverse a seconda di su quale tavolo si sta discutendo, no? Nel senso che le definizioni sono utili a costruire certe narrazioni, certi discorsi, ma dipende da cosa si vuole discutere. Ãˆ chiaro che, come dire, se discutiamo degli impatti economici, a me piace la definizione che usa OpenAI del tipo "valutiamo quando l'AI avrÃ  un certo impatto economico", quella roba lÃ¬ vuol dire che siamo arrivati all'AGI, no? Ci sta. Ãˆ chiaro che da una prospettiva piÃ¹ filosofica Ã¨ una pessima definizione. Invece mi piace di piÃ¹ la definizione di ho un'AGI quando ho un software che riesce a superare gli umani, pareggiare o superare gli umani in tutti gli aspetti, anche se anche questa qui Ã¨ mal definita perchÃ© poi cosa sono tutti gli aspetti, no? Non so, avere paura quando incontro una tigre Ã¨ un aspetto che l'AGI deve avere? Magari no. CioÃ¨, quindi capiamo poi cosa sono tutti gli aspetti. Mi piace anche la definizione che ha usato di recente Amodei, il CEO di Anthropic, relativamente appunto al fatto che anche quella lÃ¬ era focalizzata sostanzialmente sulle capacitÃ  delle AI lungo tante dimensioni.

## [18:00] Implicazioni dellâ€™Intelligenza Artificiale Generale

**Alessandro Maserati**
> Allora secondo me Ã¨ molto sottovalutato nel dibattito pubblico il fatto che stiamo parlando giÃ  solo del fatto che sia raggiungibile l'AGI, no? GiÃ  solo quello dovrebbe, come dire, popolare tutte le prime pagine, tutti i giornali, non dovremmo parlare di nient'altro. Poi addirittura oggi stiamo parlando di un orizzonte di pochi anni. Poi che siano 2, 3, 5 non fa tanta differenza, ma il fatto che sia un orizzonte di pochi anni Ã¨ un'altra roba secondo me pazzesca, no? Nel senso che qualche anno fa era normale pensare che fosse impossibile arrivarci e addirittura siamo passati dall'impossibile a pochi anni e questa cosa Ã¨ straordinaria. Quanto deve fare paura? Secondo me tantissima, cioÃ¨ nel senso non capisco come uno possa vedere un mondo post-AGI con serenitÃ , nel senso che nel "best case scenario", cioÃ¨ quando le cose dovessero andare incredibilmente bene, se fossimo incredibilmente fortunati, ci aspetta comunque un mondo radicalmente diverso da come lo conosciamo e di cui non abbiamo praticamente nessuna percezione concreta. CioÃ¨ non abbiamo idea di come sarÃ  questo mondo. Magari siamo molto fortunati e ci sarÃ  ancora questo mondo, magari no. C'Ã¨ l'enorme problema dell'allineamento che non Ã¨ minimamente risolto. VabbÃ¨, poi magari ci entriamo piÃ¹ nel dettaglio. Ma sÃ¬, diciamo, secondo me ci sono molte ragioni per temere quell'orizzonte ed Ã¨ un orizzonte estremamente vicino, Ã¨ un orizzonte che le persone che lavorano nel settore si aspettano che sia dietro l'angolo. Poi rimane la grossa incognita. Ovviamente, come dire, non stiamo parlando di scenari certi, stiamo sempre parlando di scenari che hanno una certa probabilitÃ  di accadimento. La probabilitÃ  che tutto si fermi prima c'Ã¨, non Ã¨ zero. Quindi, insomma, capisco anche chi ancora esiti a focalizzarsi unicamente su questo scenario. Ãˆ uno scenario che c'Ã¨ ed Ã¨ molto probabile. Cosa accadrÃ ? Ãˆ molto difficile da dire.

**Stefano Maestri**
> Eh no, cosa accadrÃ ? Ãˆ molto, molto, molto difficile da dire e su questo siamo assolutamente d'accordo. Guarda, la mia personalissima definizione di AGI, che Ã¨ molto da tecnico quale sono: tutto sommato io credo che si arrivi all'Artificial General Intelligence quando l'intelligenza artificiale â€” e dico l'intelligenza artificiale non un modello perchÃ© poi c'Ã¨ questa grande confusione, no? Per cui il singolo modello deve diventare intelligente abbastanza per fare l'artificial general intelligence. In realtÃ  io non credo sia cosÃ¬. Credo che sia, come hai detto giustamente tu, un software che magari mette insieme N modelli e li fa agire insieme per arrivare a quel punto. Ma io credo che il punto sia quando Ã¨ in grado di migliorarsi da solo, quando arriva a diventare sempre piÃ¹ intelligente e a quel punto senza il controllo umano, con un controllo umano minimo e quindi difficile in qualche modo, no, da fermare. E lÃ¬ che cosa succede dopo? PuÃ² succedere di tutto.

**Alessandro Maserati**
> SÃ¬. Allora, io posso contestarti questa definizione?

**Stefano Maestri**
> Certo, siamo qua apposta.

**Alessandro Maserati**
> Ãˆ interessante questa cosa perchÃ© io credo che sia mal definito il concetto che l'AI migliora se stessa. Mi spiego? Bisogna capire, cioÃ¨ io credo sia molto utile introdurre l'idea che quando una cosa viene fatta non viene fatta necessariamente da un soggetto, ma da un insieme di soggetti che hanno percentuali diverse di responsabilitÃ  su quella creazione. Ti faccio un esempio: se io provo a scrivere con Word un testo qualsiasi, l'autocorrettore mi cambia un po' di caratteri in giro perchÃ© scrivendo sbaglio, faccio dei typo. Ok? Quella cosa lÃ¬ l'ha scritta l'autocorrettore o l'ho scritta io? Secondo me, diciamo, questa domanda giÃ  Ã¨ molto difficile se mi aspetto una risposta binaria. Ma se io accetto di dire che quella roba lÃ¬ l'ho scritta al 99% io e all'1% l'autocorrettore, secondo me siamo vicini alla realtÃ . Se ho un autocorrettore piÃ¹ avanzato, un LLM primordiale che mi completa la frase, potrei arrivare in uno scenario in cui ho scritto magari il 95% e lui ha scritto il 5%. Con i modelli attuali, anzi modelli di 6 mesi fa dove io dovevo pensare l'architettura, dovevo pensare alle singole funzioni, le singoli variabili, poi davo degli input e mi uscivano, non so, magari 10-20 righe di codice alla volta, diciamo, tutte le singole righe le aveva scritte l'AI, perÃ² secondo me la persona aveva ancora un 80% di responsabilitÃ  in quella scrittura lÃ¬. Ok? E quando Amodei diceva un anno fa: "Attenzione che a fine 2025 tutto il codice sarÃ  scritto dalle AI", per caritÃ , tecnicamente aveva ragione. Tecnicamente oggi ci sono molti sviluppatori che dicono "Io non scrivo piÃ¹ una riga di codice", ma quel codice lÃ¬, per quanto sia fisicamente scritto dall'AI, qual Ã¨ la percentuale di responsabilitÃ  che ha l'AI in quella scrittura? Non so se mi spiego, cioÃ¨ comunque c'Ã¨ un'architettura dietro, un pensiero dietro che Ã¨ ancora in capo all'umano e quella parte lÃ¬, secondo me, ancora ha un grosso valore. Per cui, cioÃ¨, volendo, al punto in cui tutto il codice di Claude 5 sarÃ  scritto da Claude 4.5, secondo me ci siamo giÃ , siamo giÃ  lÃ¬. Ma il punto Ã¨ che quella scrittura lÃ¬ viene fatta con una grossa componente umana di architettura, di pianificazione, scelta delle direzioni, anche i test, no? Quindi siamo in un rapporto secondo me oggi che sta sull'80-20 come responsabilitÃ , ok? Che Ã¨ un po' come dire se io domani mattina scrivessi un libro usando ChatGPT: non Ã¨ che posso dire a ChatGPT "scrivi un libro sulla raccolta dei funghi" e il libro mi esce, devo guidarlo passo passo, no? E quindi c'Ã¨ una responsabilitÃ  condivisa, ecco. E secondo me Ã¨ anche col codice cosÃ¬. Quindi diciamo, sposo la tua definizione, se perÃ² aggiungiamo la clausola: quando l'AI aumenterÃ  se stessa con il 100% di responsabilitÃ , chiamiamolo cosÃ¬.

**Stefano Maestri**
> SÃ¬, sÃ¬, intendevo proprio quello, mi sono espresso male. CioÃ¨ quando sarÃ  in grado di prendere le decisioni architetturali per migliorare se stessa e quindi evolve ad una velocitÃ  ancora maggiore di oggi. Possiamo aspettarci, perchÃ© oggi forse il collo di bottiglia in questa corresponsabilitÃ  che hai detto Ã¨ quello umano piÃ¹ che quello artificiale da un punto di vista dei tempi. Proprio mi riferisco a questo.

## [24:12] Futuro del lavoro e ridistribuzione della ricchezza

**Stefano Maestri**
> E allora, se siamo d'accordo che l'AGI arriverÃ  â€” adesso due, 5 anni, 6 mesi, cambia poco â€” che cosa ci aspettiamo e che cosa dovremmo fare oggi che non stiamo facendo? Che cosa ci aspettiamo in termini di lavoro? Ãˆ una delle cose piÃ¹ discusse. Tu hai una tua visione di quello che sarÃ  il lavoro. Si va dal super ottimista, "guadagneremo senza piÃ¹ fare niente", al doomer peggiore che Ã¨ la fine del mondo. Allora, dove ti posizioni?

**Alessandro Maserati**
> Immaginiamo per un attimo di non avere il problema dell'allineamento, ok? PerchÃ© se capita quella cosa lÃ¬, tutto il resto Ã¨ inutile parlarne. PerÃ² immaginiamo per un attimo che le AI rimangano in completo controllo.

**Stefano Maestri**
> Poi ne parliamo di allineamento perchÃ© l'abbiamo giÃ  toccato due o tre volte, ma non sono certo che tutti ci stiano seguendo.

**Alessandro Maserati**
> PerÃ² immaginiamo che quello sia risolto per un attimo e che quindi si possa avere il completo controllo di quello che queste AI generano. Nel momento in cui arriva un AGI sostanzialmente quello che succede Ã¨ che qualsiasi attivitÃ  una persona possa fare al computer la possiamo delegare all'AGI a un costo piÃ¹ basso, una velocitÃ  piÃ¹ alta e una qualitÃ  piÃ¹ alta. Ok? Quindi il giorno dopo che Ã¨ arrivata l'AGI â€” ovviamente, come dire, non Ã¨ che arriva domani mattina, ci sarÃ  un avvicinamento che potrebbe causare delle perturbazioni che potrebbero anche portare a un arresto dello sviluppo delle AI â€” perÃ² diciamo immaginiamo di arrivarci quel giorno lÃ¬: tutti i lavori che vengono svolti davanti a un computer non hanno piÃ¹ senso di essere svolti al fine di produrre un qualche output, perchÃ© quell'output lÃ¬ lo puoi ottenere tramite l'AGI. Quindi, sostanzialmente, rientriamo in uno scenario in cui tutti i lavori fatti al computer non esistono piÃ¹ perchÃ© quei lavori lÃ¬ li demandiamo. Quindi, diciamo, fatto 100 i lavori che vengono svolti al mondo, una parte considerevole non esiste piÃ¹. Ora questo porta a una riduzione del numero totale di ore lavorate richieste alla societÃ . Da questo punto di vista bisogna capire come la societÃ  si riadatterÃ  al fatto che il numero totale di ore lavorate dovrÃ  essere minore. Uno scenario Ã¨ quello in cui tutti lavoreranno un po' meno. Quindi questo Ã¨ uno scenario, tra l'altro, che secondo me va in assoluta continuitÃ  con l'evoluzione storica. Se noi prendiamo una persona che nasceva a Londra a metÃ  800, sostanzialmente la persona media, non il nobile, iniziava a lavorare intorno ai 7-8 anni perchÃ© ovviamente il lavoro minorile non era considerato un problema. Lavorava 13-14 ore al giorno piÃ¹ o meno finchÃ© campava. Ok? Quindi se uno va a osservare la sua intera vita, la persona nata a Londra a metÃ  800 lavorava circa il 50% della propria vita, no? Tra il 46 e 50%, che Ã¨ un numero notevole. Se noi prendiamo un italiano che nasce oggi, l'italiano che nasce oggi piÃ¹ o meno lavora 40 anni, diciamo tra i 25 e 45, su un'aspettativa di vita di 85 anni, per cui giÃ  meno della metÃ . Sugli anni in cui lavora, lavora circa 220 giorni su 360, quindi diciamo i due terzi. Sui giorni che lavora lavora 8 ore su 24, quindi un terzo. Quindi facciamo un mezzo per due terzi per un terzo, arriviamo circa a un nono. Ci metti dentro che non tutti lavorano e arrivi grosso modo al fatto che in Italia abbiamo, diciamo, un numero di ore medie lavorate nell'arco della vita intorno all'8-9%. Quindi siamo passati in un secolo e mezzo dal 50% di ore lavorate all'8%. Che questo trend prosegua e arrivi al 4, al 3, al 2% mi sembra ragionevole, no? Da questo punto di vista, in questa prospettiva storica. Se proseguisse questo trend, potremmo immaginarci una societÃ  in cui, ok, metÃ  dei lavori che facciamo spariscono, ma ci ridistribuiamo l'altra metÃ  e lavoriamo tutti. Poi questo potrebbe essere, come dire, un punto d'arrivo. Come arrivarci Ã¨ molto complicato perchÃ© in realtÃ  se noi andiamo a vedere storicamente cosa Ã¨ successo quando le precedenti rivoluzioni agricole, industriali eccetera hanno distrutto posti di lavoro, la fortuna Ã¨ stata che la distruzione Ã¨ stata sufficientemente lenta da consentire sostanzialmente il pensionamento di chi era impiegato prima e non Ã¨ stata necessaria una forte riassegnazione delle persone su mansioni diverse. Quando questo invece Ã¨ stato necessario, perchÃ© ci sono stati alcuni shock che si possono osservare nell'economia da un punto di vista storico, le persone di solito si sono riposizionate su ruoli a piÃ¹ basso valore aggiunto perchÃ© non sono riuscite a riformarsi per ruoli a piÃ¹ alto valore aggiunto e per cui, diciamo, storicamente questi cambiamenti sono sopportabili alla societÃ  solo se sono abbastanza lenti. Ma qui stiamo parlando di un cambiamento che sarÃ  estremamente rapido. Quindi Ã¨ vero che potrebbe esserci un punto d'arrivo in cui la societÃ  Ã¨ stabile. Come arrivarci, perÃ², Ã¨ tutto da capire e potrebbe non essere semplice. Esistono anche degli scenari in cui, come dire, Ã¨ vero che magari rimangono tanti lavori da giardiniere, ma rimangono anche pochissimi lavori in cui grazie all'AI la produttivitÃ  Ã¨ altissima e quindi le pochissime persone che svolgeranno quei lavori potrebbero avere un reddito immenso, sostanzialmente, perchÃ© se domani mattina io ho un software che mi fa da avvocato e anzichÃ© avere 100.000 avvocati, ho una sola persona che mi coordina 'sti software e ho 99.000 avvocati che vanno a fare i giardinieri e uno che diventa multimiliardario, no? Ecco, quindi bisogna anche capire poi come gestire questo potenziale ultra-accentramento di ricchezza, perchÃ© se non inventiamo dei nuovi metodi di ridistribuzione la societÃ  potrebbe non essere poi, come dire, sostenibile, almeno nei modi che conosciamo oggi. E da questo punto di vista penso si stia facendo anche qua molto poco perchÃ© oggi non stiamo nemmeno discutendo quali potrebbero essere questi sistemi di ridistribuzione e non ci stiamo nemmeno preparando a essere in un contesto in cui siano applicabili. Ma d'altronde se non discutiamo su quali sistemi utilizzare Ã¨ difficile discutere su come applicarli.

## [30:49] Automazione e trasformazione dei ruoli

**Stefano Maestri**
> SÃ¬, certo. E ti facevo una piccola estensione di questa domanda e si parla spesso anche dell'utilizzo dell'AI sempre piÃ¹ prominente insieme all'atrofizzazione degli skill. Forse Ã¨ una delle cose piÃ¹ citate di recente. E tu come la vedi la risposta a questa cosa? CioÃ¨, in parte l'hai giÃ  data dicendo, eh, spesso ci si riqualifica su lavori a piÃ¹ basso valore aggiunto nei momenti di shock. C'Ã¨ un altro esempio che io metto sul tavolo e poi lascio a te se vuoi collegarli oppure mi dici che sono cose non collegabili, che Ã¨ l'avvento delle palestre nella societÃ  moderna. Nel momento in cui si Ã¨ smesso di fare lavori fisici, l'uomo comunque Ã¨ andato a cercare di farli in qualche altro modo per hobby e quindi gli skill o il lavoro intellettuale lo possiamo spostare a quel livello o non funziona?

**Alessandro Maserati**
> Allora, tre cose da dire su questo tema. Allora, di recente Ã¨ uscito uno studio interessante che mostrava come a seconda dei lavori â€” perchÃ© poi ciascun lavoro in realtÃ  Ã¨ una composizione di una serie di task â€” nei lavori dove l'AI andava ad automatizzare i task a piÃ¹ basso impatto cognitivo o diciamo requisiti cognitivi, chiamiamoli cosÃ¬, in realtÃ  le persone che lavoravano in quell'ambito aumentavano la propria qualifica perchÃ© si spostavano su task piÃ¹ complessi, aumentavano anche il reddito. Ok? E invece nei lavori dove l'AI andava ad automatizzare la parte a piÃ¹ alto valore cognitivo, si aveva proprio un effetto di demansionamento delle persone, per cui, diciamo, le persone usavano meno skill intellettuali e avevano anche una riduzione, inizia a essere appena appena misurabile, ma Ã¨ chiaro il trend anche del reddito. Quindi Ã¨ un tema molto importante capire nel transitorio come l'AI impatterÃ  i lavori, quali skill verranno automatizzate prima, in che modo. E diciamo c'Ã¨ uno studio che Ã¨ stato molto citato riguardo al fatto che se io demando la scrittura di un testo all'AI perdo capacitÃ  di ricordare quel testo; in realtÃ  poi Ã¨ stato abbastanza "debunked", come si dice, falsificato questo studio, perchÃ© a paritÃ  di tempo speso su un testo, che usi o no l'AI, mi ricordo la stessa quantitÃ  di nozioni, quindi anche lÃ¬ Ã¨ da capire. Per quello che riguarda, diciamo, l'impatto sulle generazioni piÃ¹ giovani che devono ancora apprendere i mestieri, io credo che sia fondamentale cercare di instillare nelle persone un buon livello di consapevolezza sul perchÃ© fanno le singole cose. Mi spiego. Ãˆ giusto o sbagliato che una persona usi una calcolatrice per fare una somma? Beh, dipende, perchÃ© se Ã¨ un contabile direi che Ã¨ fondamentale che la usi perchÃ© non deve sbagliare e non ha nessun vantaggio nel fare i conti a mente. Se Ã¨ un bambino in prima elementare direi che Ã¨ sbagliato perchÃ© il suo scopo non Ã¨ scoprire il risultato, ma Ã¨ proprio fare la fatica di scoprirlo. Se io demando l'attivitÃ  alla calcolatrice, non faccio piÃ¹ la fatica e non apprendo la skill. Ãˆ giusto o sbagliato spostarsi da un punto A a un punto B con la macchina? Eh, dipende. Se io sono in ritardo, devo andare al lavoro, magari Ã¨ giusto. Se io voglio dimagrire e no, devo andarci di corsa perchÃ© se ci vado con la macchina e hai voglia di fare jogging con la macchina, non dimagrisci mai, no? Ecco, quindi io credo che sia importante acquisire la consapevolezza del perchÃ© facciamo le cose. Facciamo le cose perchÃ© mi interessa il risultato o facciamo le cose perchÃ© mi interessa il processo? E in generale, come dire, se ci abituiamo a demandare alle AI solamente le attivitÃ  per cui ci interessa il risultato, non vedo effetti collaterali, anzi ne abbiamo tanto da guadagnare, ma dobbiamo stare molto attenti a non demandare alle AI le cose per cui ci interessa il processo. Ti faccio un esempio concreto. A me spesso capita di maneggiare tabelle piene di numeri, perchÃ© aiutiamo i clienti anche ad analizzare dei dati, no? Ecco, io maneggiando le tabelle acquisisco una forte conoscenza di quel dato e magari ci sono delle informazioni che non mi serve estrarre in quel momento, ma dopo una settimana parlando col cliente viene fuori un argomento e io ricollego che ho visto quel trend nei numeri, no? Se io quell'analisi lÃ¬ la demandassi all'AI, cosa che posso fare, perchÃ© oggi l'AI ha le skill di dire "Ok, prendi il database, tirami fuori 30 chart", perderei quell'assorbimento di conoscenza che invece oggi Ã¨ fondamentale nel mio lavoro e quindi in quel caso lÃ¬, come dire, delegare alle AI per me sarebbe un problema perchÃ© perderei l'effetto collaterale che in realtÃ  mi dÃ  un sacco di valore. Mentre in altri casi, se io quei numeri giÃ  li conosco, devo preparare una presentazione e ho poco tempo, delegare all'AI la preparazione dei chart Ã¨ assolutamente ragionevole secondo me, perchÃ© in quel caso lÃ¬ non mi serve il processo, mi serve l'output. Ecco, quindi io credo che se riuscissimo a instillare nelle persone piÃ¹ consapevolezza sul perchÃ© fanno le cose, potremmo anche andare in una direzione in cui l'uso delle AI Ã¨ prettamente, come dire, positivo ed eviteremmo i fattori collaterali negativi.

## [35:23] Consapevolezza nellâ€™uso dellâ€™AI

**Stefano Maestri**
> Mi piace molto questa cosa della consapevolezza perchÃ© provo ad aggiungere una dimensione. Io da sviluppatore sono uno dei piÃ¹ impattati in questo momento dall'AI che fa tante cose per noi e leggevo un articolo ieri, credo, che metteva a confronto la "velocity", cioÃ¨ la velocitÃ  vera e propria di produzione del software, con il "throughput". Quindi l'AI in realtÃ  non sta incrementando cosÃ¬ tanto la velocity del singolo task, quanto il throughput perchÃ© ti permette di fare tante cose in parallelo. Questa cosa ha un altro problema che Ã¨ il problema storico dei programmatori che Ã¨ il "context switching", perchÃ© continuare a saltare da un task all'altro produce molti piÃ¹ errori. Quindi la consapevolezza di quello che si sta facendo e a volte ridurre un pochino quello che Ã¨ il throughput, ma restare focalizzati sull'obiettivo potrebbe essere la sfida nostra come programmatori dei prossimi mesi, almeno poi anni, non lo so, mesi di sicuro. Ma ti sposto un attimo su una cosa, cosÃ¬ ci avviciniamo all'allineamento perchÃ© poi voglio entrarci in quella cosa lÃ¬. Ed Ã¨: ammettiamo che l'allineamento sia risolto di nuovo. Ammettiamo che tutto vada bene come l'essay dell'anno scorso di Amodei, non quello di quest'anno in cui c'era una "Graceful Machine" che ci aiutava a fare tante cose, perÃ² vengo, tornando all'intervista a Hassabis-Amodei di cui parlavamo prima, a quello che invece Hassabis, CEO di DeepMind, continua a ripetere, cioÃ¨ l'AI ci porterÃ  ad un momento di "post-scarsitÃ ", cioÃ¨ l'economia di oggi Ã¨ retta sulla scarsitÃ , in particolare la scarsitÃ  dell'intelligenza, la scarsitÃ  dell'energia. Lui continua a ripetere che l'AI ci porterÃ  ad un'abbondanza di intelligenza e probabilmente ad un'abbondanza di energia perchÃ© ci aiuterÃ  nella ricerca. SÃ¬. E crolla il castello di carte o no?

**Alessandro Maserati**
> Mmm, sÃ¬ e no. Allora, Ã¨ vero che ovviamente l'economia forse non direi retta dalla scarsitÃ , ma gira intorno alla scarsitÃ , ma perchÃ© quello, come dire, Ã¨ il suo scopo, no? PerÃ² noi abbiamo giÃ  superato nella storia scenari in cui i principali asset cessano di essere scarsi. CioÃ¨, se tu pensi, non so, a un secolo fa, i principali asset per la maggior parte dei paesi erano gli asset alimentari, no? Le persone avevano bisogno di mangiare e arrivavamo da secoli in cui le carestie ciclicamente capitavano, eravamo, come dire, agli albori di un'enorme esplosione demografica e quindi l'asset piÃ¹ importante â€” vabbÃ¨, oltre all'energia â€” erano gli asset alimentari. Oggi gli asset alimentari sono in ampia sovrabbondanza. Oggi il costo di un asset alimentare Ã¨ legato prevalentemente al costo del trasporto, dell'imballaggio, ma non alla produzione. Ormai possiamo dire che sono praticamente gratuiti, no? Rispetto a tutte le altre cose, a capacitÃ  di acquisto media di una persona in Occidente, ormai, come dire, se uno volesse solamente comprare le calorie che gli servono per vivere, il costo Ã¨ veramente minimo. Quindi siamo giÃ  passati in uno scenario di quel tipo lÃ¬ e abbiamo visto che nel momento in cui quella dimensione dell'asset, quindi non so la disponibilitÃ  delle calorie, crolla come prezzo, ci sono tante cose ancillari che acquisiscono valore. Non so, il sapore: Ã¨ chiaro, un contadino del 700 l'importante era campare, poi il sapore era un po' secondario, no? Oggi invece magari mi interessa il sapore, mi interessa il packaging, mi interessa non so la temperatura, cioÃ¨ tanti aspetti secondari che prima non lo erano. Ãˆ chiaro che in uno scenario in cui, non so, l'AI mi produce un'infinitÃ  di dipinti, allora diventano importanti altri aspetti, cioÃ¨ quel dipinto lÃ¬ riesce a catturare un momento che ho vissuto veramente in prima persona e magari, come dire, si aprono altre industrie che oggi non ci possiamo nemmeno immaginare. E poi il fatto in generale che ci sia una sovrabbondanza di prodotto digitale non implica necessariamente la sparizione del prodotto umano, ma puÃ² acquisire valore in modo diverso. Allora, faccio un esempio. Non so, 50 anni fa non esistevano i computer che giocavano a scacchi, ok? Oggi abbiamo i computer che giocano a scacchi meglio del miglior giocatore di scacchi al mondo. Secondo te ci sono piÃ¹ maestri di scacchi oggi o 50 anni fa? No, cioÃ¨ uno dice in teoria visto da fuori dice "Ma adesso lo fanno i computer, dovrei aspettarmi zero maestri umani". In realtÃ  il mestiere maestro di scacchi si Ã¨ sviluppato. Oggi abbiamo molti piÃ¹ maestri di scacchi rispetto a 50 anni fa. Oppure le fotografie, no? CioÃ¨ magari io vado a vedere quanti erano i ritrattisti a inizio secolo e quanti sono i pittori oggi, direi "Ma Ã¨ arrivata la fotografia, mi aspetterei che il numero di pittori si sia azzerato, perchÃ© se voglio un'immagine faccio una foto". In realtÃ  no, il numero di pittori Ã¨ aumentato. Quindi in generale, come dire, il fatto che ci sia una sovrabbondanza di produzione digitale non implica automaticamente che sparisca la richiesta, l'esigenza della produzione umana. Diciamo che la produzione umana acquisisce un significato diverso, viene un po' ripensata, poi dipende ovviamente dai singoli prodotti. Ãˆ chiaro che, come dire, l'accountant, l'avvocato, cioÃ¨ difficile, come dire, che io dia un valore aggiunto al fatto che ci sia una persona dietro un foglio Excel e lÃ¬, probabilmente il momento in cui l'AI puÃ² automatizzare quell'attivitÃ , la delegheremo al 100%. PerÃ², ecco, su tante altre mansioni, mi aspetto che comunque il fatto che ci sia un aumento di produzione digitale non comporti necessariamente un azzeramento della produzione umana e in generale penso anche che le persone sono sempre molto brave storicamente a ripensare i lavori, a trovare altre occupazioni e altri modi per dare significato alla propria vita. E credo che tra l'altro questo sarÃ  un trend molto, molto importante, no?

## [41:17] Il problema dellâ€™allineamento

**Stefano Maestri**
> SÃ¬, magari qui allargo troppo, ecco, perÃ² Ã¨ interessante. No, no, ma ci sta, Ã¨ interessante. E qui mi permetto di essere io in leggero disaccordo con te su un'ultima cosa che hai detto per stimolare la conversazione anche. Hai detto Ã¨ difficile, no, dare un valore aggiunto a chi sta dietro un foglio Excel eccetera, perÃ² come dicevi prima tu giustamente gli "up-skill" possono essere una cosa interessante, anche un cambio totale del lavoro, perchÃ© io penso che un avvento come quello dell'intelligenza artificiale porti a non cambiare il lavoro, ma a crearne di nuovo e distruggerne di vecchi, come Ã¨ stato in passato con alcune tecnologie. E faccio un esempio, la segretaria negli anni 60 era tendenzialmente una dattilografa. Oggi la segretaria di direzione fa mille cose in piÃ¹ e l'ultima cosa che fa Ã¨ digitare la tastiera perchÃ© comunque tanto anche il manager ha la sua tastiera davanti e digita. E il dettare alla segretaria che cosa vuoi scrivere non lo fa piÃ¹ nessuno, per fortuna. Eh, perÃ² non Ã¨ che le segretarie sono sparite, anzi sono diventate ancora piÃ¹ importanti le segretarie di direzione, quindi se vogliamo essere positivi, perchÃ© no? Anche chi oggi fa un lavoro che sembra un pochettino piÃ¹ sostituibile, probabilmente il lavoro in sÃ© sarÃ  sostituito, ma lui puÃ² fare un upskill e cambiare completamente il modo di intendere anche quel lavoro, questo come spunto. E ci spostiamo, spostiamoci dal lavoro e andiamo anche sul piÃ¹ impegnativo perchÃ© diventa anche quasi geopolitico. Ma partiamo da allineamento. Definiscimi allineamento. Che cos'Ã¨ l'allineamento? PerchÃ© io e te ci capiamo, molti dei nostri ascoltatori sÃ¬, ma molti altri no. Che cos'Ã¨ l'allineamento di un AI, di un large language model? E perchÃ© Ã¨ cosÃ¬ cruciale? PerchÃ© l'hai citato cosÃ¬ tante volte?

**Alessandro Maserati**
> Allora, l'allineamento Ã¨ diciamo l'idea che tu possa fare in modo che l'AI faccia esattamente quello che vuoi. Quindi, se io gli dico "scrivi un testo" lui faccia esattamente quello che gli dico io. Ok? PerchÃ© Ã¨ complicato? PerchÃ© piÃ¹ vanno a crescere le AI piÃ¹ aumenta l'orizzonte su cui possono e devono prendere decisioni autonome. Mi spiego? Se io gli dico scrivi la prossima parola, lui deve scegliere una parola e finisce lÃ¬. Se io dico scrivi la prossima frase, lui magari deve scegliere non solo una parola, ma deve scegliere anche, non so, se farla al presente, al passato, al maschile, al femminile. CioÃ¨, prende piÃ¹ decisioni. Se io dico scrivi un'intera pagina di un libro, deve scegliere quali personaggi coinvolgere, quali attivitÃ  fargli fare. CioÃ¨, piÃ¹ allungo il task che chiedo a una macchina, piÃ¹ aumenta lo spazio delle decisioni che questa macchina deve prendere. L'idea Ã¨ che col progredire delle AI io possa chiedere task cosÃ¬ tanto lunghi che in mezzo questo spazio delle decisioni possa essere cosÃ¬ ampio da dargli sostanzialmente quella che al di fuori apparirÃ  come essere una totale autonomia. Poi non sarÃ  mai una totale autonomia perchÃ© sarÃ  sempre vincolato all'obiettivo finale, ma il problema Ã¨ dove passano i percorsi nel mezzo che mi portano all'obiettivo finale. Faccio un esempio. Immaginiamoci un domani in cui ho il mio umanoide personale con intelligenza artificiale generale dentro e gli chiedo, dico: "Guarda, io voglio assolutamente dimagrire 5 kg il prossimo mese, no?" E 'sto umanoide tutte le mattine mi prende a bastonate finchÃ© non vado a correre. Ãˆ uno scenario pessimo, perÃ² il suo obiettivo lo raggiunge, no? Allora, per tenerlo allineato inizio ad aggiungere i vincoli. Dico "Ok, sÃ¬, perÃ² guarda, voglio ottenere quell'obiettivo, ma non voglio andare a correre" e allora inizia a prendere tutto il cibo qua in casa e lo butta via. Eh no, non va bene. Allora aggiungo un altro vincolo. Dico "SÃ¬, voglio dimagrire senza correre, ma non buttar via il cibo" e allora mi impedisce di comprarlo. Magari, non so, mi svuota il conto in banca, cosÃ¬ non posso piÃ¹ comprare il cibo. Allora aggiungo un altro vincolo. Dico "No, ma guarda, io voglio poter comprare quello che voglio, ma non lo voglio trovare in giro perchÃ© sennÃ² poi lo compro e magari genera una carestia mondiale per farmi perdere 5 kg nel prossimo mese", no? CioÃ¨, il problema Ã¨ che quando tu provi a fare questo esercizio logico e inizi ad aggiungere vincoli, a un certo punto di solito arrivi a un punto dici "Ah, ok, adesso l'ho fregato", ma hai aggiunto talmente tanti vincoli che hai ottenuto l'insieme vuoto, cioÃ¨ che hai generato un obiettivo che Ã¨ impossibile da raggiungere. Provare a fermarsi nel punto in cui la soluzione che ti piacerebbe Ã¨ l'unica che di fatto, come dire, Ã¨ eseguibile, sembra una sfida impossibile, cioÃ¨ quando uno dÃ  degli obiettivi abbastanza lontani, fare in modo che la macchina li raggiunga passando per percorsi che ci piacciono Ã¨ estremamente difficile. E per tanti anni si Ã¨ studiata questa cosa, no? Non so, ok, gli chiederemo di eliminare le malattie del mondo, ma il modo piÃ¹ veloce per eliminare le malattie Ã¨ eliminare l'umanitÃ , no? E tutte cose cosÃ¬. CioÃ¨, se tu ogni volta che provi a dare un obiettivo interessante, quello che succede Ã¨ che esistono dei percorsi che raggiungono quell'obiettivo passando per dei punti intermedi che tu assolutamente non vuoi attraversare. E al momento non esiste nemmeno un'idea su come potremmo fare per tenere le macchine allineate. CioÃ¨ non Ã¨ che sappiamo qual Ã¨ il risultato e non riusciamo a ottenerlo, ma non abbiamo nemmeno idea di come fare e siamo lontanissimi da raggiungere quell'obiettivo. E il grosso problema dell'allineamento Ã¨ che il giorno che dovesse comparire un AGI, un AGI che Ã¨ veramente, come dire, cosÃ¬ potente da fare quello che gli pare, se tu non l'hai risolto il giorno prima l'allineamento, quel giorno lÃ¬ rischi che finisca il mondo, perchÃ© rischi che chi ha creato l'AGI dica "Ok, da domani non voglio piÃ¹ che le persone soffrano" e lui in giornata le elimina tutte, cosÃ¬ da domani non c'Ã¨ piÃ¹ nessuno che soffre, no? E questa roba qua, cioÃ¨, non Ã¨ cosÃ¬ tanto astratta, come dire; quando uno prova a fare degli esempi, praticamente qualsiasi cosa tu gli chieda, esistono degli scenari facili per arrivare lÃ¬ che fanno dei disastri. E non solo, ma anche se tu non gli dessi nessun obiettivo ambizioso, in generale potresti avere un AI che per soddisfare le sue esigenze intermedie decida che ha bisogno di risorse e quindi vada in competizione con gli umani per l'utilizzo delle risorse, quindi anche lÃ¬ hai dei disastri. E ribadisco, al momento non esiste nessuno al mondo che abbia una minima vaga idea su come risolvere il problema dell'allineamento. CioÃ¨ Ã¨ una cosa che sappiamo che Ã¨ un problema enorme, non abbiamo idea di come affrontarlo e c'Ã¨ la vaga speranza che sia risolto prima di arrivare all'AGI. PerÃ² l'AGI pensiamo che arrivi in pochi anni; sull'allineamento nessuno sa, quindi insomma lo scenario non Ã¨ dei migliori, no?

**Stefano Maestri**
> Allora, non Ã¨ cosÃ¬ nuovo lo scenario dell'allineamento, eh, perchÃ© magari qualcuno dei nostri ascoltatori ci ascolta e dice "VabbÃ¨, ma non Ã¨ cosÃ¬ nuovo, ma Ã¨ una cosa arrivata adesso, eccetera". Non Ã¨ cosÃ¬ nuovo, tant'Ã¨ che chi ha piÃ¹ o meno la mia etÃ , probabilmente negli anni 90 ha visto Matrix e Matrix citava esattamente uno degli ultimi problemi di allineamento che ha citato Alessandro, cioÃ¨ quando umani e macchine vanno in competizione per le risorse â€” lÃ¬ era l'energia, se ricordo bene â€” fino ad utilizzare gli umani come batterie nella finzione del film. PerÃ² non siamo cosÃ¬ lontani. Ma allora, perÃ² io ti faccio una domanda provocatoria: ma quindi la soluzione qual Ã¨? Quella di rallentare, quella di legiferare in anticipo come sta facendo l'Europa, senza forse capire abbastanza o provando a capire quello che sappiamo oggi, o semplicemente ci dobbiamo fidare al buon senso comune, come invece dice Anthropic? PerchÃ© io ho sentito un'intervista della persona che si occupa di allineamento, in particolare di Anthropic che Ã¨ una filosofa, da Lex Friedman era, e a domanda specifica: "Ma quindi tu quando devi fare l'allineamento a che cosa ti vali?" "Del buon senso, del senso comune", ha detto. E perÃ² il senso comune mio forse Ã¨ diverso da quello che aveva Hitler, non lo so.

**Alessandro Maserati**
> Esatto, esatto. No, Ã¨ molto complicato il problema perchÃ© allora fermare lo sviluppo lo puoi fare solo se tutto il mondo decide di fermare lo sviluppo, ma al momento siamo lontanissimi da quello scenario perchÃ© anche solo la competizione tra Stati Uniti e Cina â€” competizione, facciamo finta che sia solo per la supremazia economica o anche solo per la generazione di benessere per la propria popolazione â€” cioÃ¨ stiamo anche negli scenari in cui le persone sono tutte interessate al bene collettivo, io potrei decidere che per, come dire, poter spingere al massimo il benessere della mia popolazione voglio fare ancora un passo in piÃ¹, no? PerchÃ© avrÃ² l'AI un po' piÃ¹ potente, automatizzerÃ  qualche processo in piÃ¹, aumenterÃ  il mio PIL e cosÃ¬ via. E solo il fatto che queste due grandi superpotenze non abbiano intenzione di rallentare rende abbastanza, come dire, utopico pensare di porre qualche freno, perchÃ© basta che uno vada avanti e di fatto il fatto che gli altri abbiano rallentato non dia nessun vantaggio. Ehm, diciamo che l'idea americana che si sente spesso Ã¨ che se loro arrivassero primi risolverebbero il problema. Ora il problema Ã¨: innanzitutto non Ã¨ chiaro che â€” cioÃ¨ io capisco l'idea di dire "Ok se io costruisco la bomba atomica per primo dico a tutti gli altri di fermarsi sennÃ² la sparo", no, e se la mia minaccia Ã¨ credibile, magari loro si fermano, io poi sono il piÃ¹ buono al mondo e quindi non la userÃ² mai. Capisco l'idea. Il problema Ã¨ che innanzitutto non sappiamo se questa Ã¨ una bomba atomica che si auto-innesca quando hai finito di costruirla. Quindi tu magari arrivi per primo e distruggi tutto, cioÃ¨ non Ã¨ detto che tu riesca a fermarti in tempo. Secondo, non abbiamo una minima idea di misurare quanto siamo lontani. CioÃ¨, come facciamo a sapere se l'AI arriva tra un anno, 2 anni, 5 anni o mai? Non lo sappiamo. Ma perchÃ© non lo sappiamo? Non lo sappiamo perchÃ© oggi non esiste un modo per misurare quanto siamo distanti. Quindi, se io avessi un qualche metodo di misura, ti direi "arriviamo fino al passo prima e ci fermiamo lÃ¬", ma siccome non sappiamo nemmeno come misurarlo, non sappiamo quando arriverÃ  quel momento lÃ¬. Quindi la cosa prudenziale sarebbe fermarsi, ma non ti puoi fermare tu perchÃ© gli altri vanno avanti. Quindi siamo in uno scenario in cui Ã¨ molto difficile intravedere una soluzione. La speranza Ã¨ che a un certo punto arrivi abbastanza avanti da avere in mano qualcosa di abbastanza spaventoso da dire "fermi tutti", ma non ancora cosÃ¬, come dire, critico da non riuscire piÃ¹ a fermarlo, no? CioÃ¨ ci sarÃ , come dire, un confine molto sottile in cui si arriverÃ  abbastanza vicini in cui forse saremo ancora in tempo per fermarci. Il problema Ã¨ che parliamo di un processo che Ã¨ piÃ¹ che esponenziale e abbiamo visto in tante occasioni come l'umanitÃ  in generale non sa comprendere i processi esponenziali, quindi Ã¨ molto difficile dire "mi fermerÃ² appena prima" perchÃ© appena prima saremo ancora relativamente molto lontani e non abbiamo nemmeno un modo per misurare questa distanza. Esistono perÃ², ovviamente, adesso non voglio essere catastrofista, esistono tanti scenari in cui lÃ¬ non ci arriveremo mai. Esistono scenari in cui non esiste una forma di AGI che possa avere quel livello di autonomia. Esistono degli scenari in cui, come dire, l'impatto economico che avrÃ  lo sviluppo delle AI provocherÃ  dei forti assestamenti prima che porteranno un rallentamento. Esistono dei scenari in cui l'AGI vera la puoi raggiungere solo con livelli computazionali che non riusciamo fisicamente a produrre. Quindi, insomma, non Ã¨ l'unico scenario quello lÃ¬, perÃ² giÃ  il fatto che abbia anche solo il 10% â€” cioÃ¨ se io ti dicessi guarda hai il 10% di probabilitÃ  di schiacciare un pulsante che fa esplodere una bomba nucleare ti annienta, cioÃ¨ io mi fermerei, come dire, non mi prenderei quel 10% di rischio, no? Invece sembra che al momento, come dire, i principali player siano disponibili a prendersi questo rischio.

**Stefano Maestri**
> SÃ¬, il paragone con la bomba atomica ricorre spesso tra tanti esperti. Amodei Ã¨ quello che probabilmente l'ha citato per primo e che continua a ripetere, a fare questo paragone.

## [54:12] Europa e Intelligenza Artificiale: unâ€™assenza critica

**Stefano Maestri**
> Spostandoci un attimo, eh, qui mi aspetto di avere qualche commento su questa intervista dei negazionisti o di chi Ã¨ piÃ¹ spaventato magari da questa cosa, ma restando un po' qui sul geopolitico, ma spostandoci leggermente, tu hai citato giustamente le due grandi potenze tecnologiche del mondo, Stati Uniti, Cina. Lasciamo perdere in questo momento il Medio Oriente e la Russia che sembrano piÃ¹ occupati a fare quello che abbiamo fatto nel secolo scorso, guerre, che a innovare. Ma l'Europa? L'Europa dov'Ã¨ in tutto questo?

**Alessandro Maserati**
> L'Europa Ã¨ il grande assente. Grande assente con, come dire, una responsabilitÃ  colpevole della classe politica che governa in questo momento che sembra aver scelto di non voler partecipare al principale, come dire, trend del progresso umano in questo momento. E purtroppo il tentativo di legiferare si Ã¨ visto che risultati ha portato, nel senso che l'AI Act Ã¨ nato inapplicabile fin dall'inizio, poi adesso hanno momentaneamente sospeso una parte delle norme per un anno. Ma insomma Ã¨ chiaramente inapplicabile. I singoli paesi hanno provato a far uscire delle leggi anche lÃ¬, insomma, come dire, abbastanza discutibili, stanziando dei budget ridicoli. Mi pare che l'Italia abbia di recente stanziato un billion, cioÃ¨ una roba che non ci compri neanche l'1% di uno dei principali attori. Cosa fai con un billion? Poco niente. Come dire "Ok, non partecipo". CioÃ¨, l'Italia ha stanziato 170 miliardi di euro per la coibentazione del 3% degli edifici, no? 170 miliardi e 1 miliardo per l'intelligenza artificiale. 'Sta cosa a me manda ai matti, cioÃ¨ com'Ã¨ possibile che un paese spenda 170 per mettere i doppi vetri e uno per un trend che puÃ² cambiare il destino dell'umanitÃ ? E quindi questa cosa Ã¨ molto problematica. Il problema Ã¨ che se uno va a sentire i dibattiti che si fanno in Parlamento su questi temi Ã¨ evidente come nessuno ne capisca nulla. C'Ã¨ proprio una disinformazione totale e un'incomprensione totale da parte dei politici di questi temi, cioÃ¨ non sanno di cosa stanno parlando, per cui chiaramente poi le scelte non possono essere particolarmente lungimiranti se chi le deve prendere non ha idea del tema su cui deve prendere la decisione. Il problema invece a livello, come dire, europeo Ã¨ legato ai meccanismi decisionali che ci sono in Europa, quelli che causano n problemi, non solamente il posizionamento sull'AI. Che poi sono quelli che ciclicamente anche Draghi continua a riporre in primo piano e probabilmente finchÃ© non si risolverÃ , come dire, il problema dei meccanismi decisionali, sarÃ  molto complicato avere un sistema che funzioni e vada nella direzione giusta. Il fatto comunque alla fine della fiera Ã¨ che l'Europa non sta partecipando in nessuna misura al progresso dell'intelligenza artificiale, siamo utilizzatori passivi, non ci stiamo nemmeno preparando, tra l'altro, a usare il mondo open source, no? PerchÃ© uno potrebbe dire "Ok, non ho in casa nessuno dei grossi player che sviluppa, perÃ² mi preparo e userÃ² il mondo open source, cosÃ¬ quantomeno sarÃ² indipendente." Ma si Ã¨ visto che anche lÃ¬ in realtÃ  non c'Ã¨ nessuna azione seria di preparazione del sistema europeo per reggere anche solo i calcoli che saranno necessari, cioÃ¨ non c'Ã¨ un piano di costruzione di centrali elettriche, data center, infrastruttura di rete, cioÃ¨ tutto quello che servirebbe non lo stiamo facendo. Per cui insomma la situazione non Ã¨ delle migliori. Tra l'altro ogni tanto si cita l'unica azienda europea che un pochettino ha provato a competere che era Mistral, francese, ma non so se hai visto l'ultimo modello che hanno rilasciato. Ãˆ praticamente una versione di Qwen leggermente modificata, per cui anche lÃ¬, cioÃ¨ se l'unico player che abbiamo si basa di fatto su una tecnologia open source cinese, il giorno che quelli lÃ  decidono di cambiare approccio, siamo veramente completamente disarmati, ecco, quindi non vedo purtroppo al momento tante possibilitÃ .

**Stefano Maestri**
> SÃ¬, sÃ¬. No, ma la scelta open source potrebbe anche starci. Io vengo da quel mondo e ne sarÃ² sempre paladino. Ma hai ragione tu, cioÃ¨ non si sta facendo il resto. Spiego un secondo agli ascoltatori questa cosa. Si fa tanto parlare di sovranitÃ  digitale che sarebbe fondamentale, ma la sovranitÃ  digitale non per come alcuni politici la intendono, si stanno riempiendo la bocca, permettetemi il termine, del non dare i nostri dati come se fosse un problema di spionaggio, ma perchÃ© esiste uno scenario in cui a creare valore sarÃ  l'intelligenza artificiale e non gli esseri umani. E quel valore potrebbe, in questo scenario che non Ã¨ cosÃ¬ campato in aria, potrebbe essere tassato nel paese in cui questo valore viene costruito e se i data center stanno tutti fuori dall'Europa o comunque fuori dall'Italia, significa che il valore che viene costruito viene tassato in un altro paese, non nel nostro. Quindi portare qui dei data center, portare qui un'infrastruttura di rete per ospitare anche modelli open source, anche modelli di altri, perchÃ© no? Avere qui il modello di Anthropic ma che produce valore all'interno del nostro paese potrebbe essere una scelta, non dico lungimirante, ma almeno mettere qualche pezza. E invece no, non si sta facendo nemmeno quello, come adesso stava dicendo Alessandro. Questo Ã¨ un dato abbastanza preoccupante.

## [01:03:28] Oltre i modelli stocastici

**Stefano Maestri**
> Alleggerisco perÃ² perchÃ© qui ci siamo persi metÃ  degli ascoltatori. Eh, quindi se non siete ancora andati via, provo ad alleggerire con un paio di discorsi che sono tecnici ma di altro genere che mi hanno colpito in pre-intervista. Tu mi hai citato alcune cose che io mi sono appuntato e una di queste che Ã¨ un po' contraria alla maggior parte dei detrattori che la utilizzano per fare questa spiegazione e dicono: "i large language model sono dei pappagalli stocastici", ovvero siccome si basano su una scienza delle probabilitÃ , stocastici appunto, producono soltanto la parola piÃ¹ probabile successiva e non fanno nient'altro. Tu mi hai detto: "Io non sono assolutamente d'accordo con questa cosa." Ti va di spiegarmelo?

**Alessandro Maserati**
> SÃ¬. Allora, partiamo dall'idea del pappagallo, no? CioÃ¨, spacchiamo tra pappagallo e stocastico. Allora, pappagallo Ã¨ perchÃ© in teoria questi modelli dovrebbero solamente ripetere contenuti che hanno scritto. Allora, premesso che tutte le parole che dico io le ho lette da qualche parte nel corso della mia vita, quindi uno potrebbe dire "Ma anch'io sono un pappagallo perchÃ© tutto quello che dico l'ho letto". Ma non solo le singole parole, la maggior parte dei concetti che esprimo sono rielaborazioni di cose che ho letto, che ho sentito. Ãˆ difficile, molto difficile, che una persona produca un contenuto completamente originale scollegato da tutte le sue esperienze passate. CioÃ¨, direi quasi impossibile, no? Sono sempre, come dire, rielaborazioni basate su quello che uno ha preso, anche ragionamenti interni eventualmente, ma comunque partono sempre da un input esterno. Quindi, se sono pappagalli i large language model, secondo me, siamo tutti pappagalli da questo punto di vista. Ma entro sullo stocastico. Allora, questa idea del pappagallo stocastico nasce perchÃ© l'architettura Transformer su cui sono basati tutti i large language model sostanzialmente ha in fondo, in fondo alla rete che Ã¨ molto lunga, c'Ã¨ un ultimo layer che sostanzialmente fa un sampling stocastico, appunto, sulla base delle probabilitÃ  assegnate ai singoli token e quindi come dire la narrativa comune Ã¨ "ok, quindi l'output Ã¨ casuale". Allora, innanzitutto non Ã¨ casuale, perchÃ© casuale vorrebbe dire che tutte le parole che sorteggio sono equiprobabili, mentre lÃ¬ vengono prese sulla funzione di una certa probabilitÃ  che viene calcolata di ogni parola. Ma allora, ci sono tanti motivi per cui questo ragionamento Ã¨ sbagliato. Primo motivo Ã¨ che se io anche metto la temperatura del modello in modo tale che diciamo che tolgo l'ultimo layer che fa il sampling, cioÃ¨ prendo un modello deterministico e non solo deterministico, ma di recente Ã¨ stato dimostrato invertibile, che Ã¨ molto piÃ¹ di deterministico, ok? Comunque il Large Language Model mantiene un sacco di proprietÃ  interessantissime e rimane molto performante, per cui potrei anche toglierlo, se proprio dÃ  fastidio psicologicamente possiamo togliere l'ultimo layer e comunque ho un modello eccezionale. Mi serve soprattutto quando l'addestro perchÃ© copre meglio lo spazio delle possibilitÃ , ma una volta che il modello Ã¨ addestrato potenzialmente lo potrei togliere. Ma diciamo pure di mantenerlo. Il language model produce una parola alla volta, ma non pensa una parola alla volta. Anche una persona che parla pronuncia una parola alla volta, perÃ² come dire tendenzialmente quando facciamo un discorso noi pensiamo l'intero discorso e poi pronunciamo una parola alla volta. Si Ã¨ visto studiando i large language model che funziona esattamente nello stesso modo, cioÃ¨ quando voi ponete una questione il large language model al suo interno elabora forse non proprio l'intera risposta ma una gran parte della risposta e poi inizia a produrre una parola alla volta per trasferirla all'esterno, no? Queste cose qua che si osservano sono esattamente dei meccanismi di ragionamento analoghi a quelli che si osservano sugli umani. Per cui l'idea che faccia semplicemente un sorteggio casuale Ã¨ completamente distorsiva. Tra l'altro, voglio dire, ormai si Ã¨ visto che producono contenuti innovativi, scrivono dimostrazioni matematiche nuove, cioÃ¨ come potrebbe se penso a un computer che si limita a ripetere quello che ha letto sorteggiando a caso, ma quando mai potrebbe produrre una dimostrazione matematica nuova? Sarebbe completamente impossibile, no? Quindi, come dire, l'esperienza obiettiva mostra che questa ultra semplificazione dei modelli Ã¨ distorsiva, cioÃ¨ non si limitano a ripetere e la parte casuale Ã¨ semplicemente uno specifico componente tecnico di qualcosa di estremamente piÃ¹ complesso che andrebbe, come dire, approfondito in toto. Quello Ã¨ un singolo elemento che volendo potremo pure togliere. Ecco, quindi assolutamente la definizione di pappagallo stocastico Ã¨ proprio sbagliata.

**Stefano Maestri**
> SÃ¬, per spiegare questa cosa agli ascoltatori, magari un po' meno tecnici, io cito un talk che ho sentito di Emanuele Fabiani che ho intervistato anche qualche settimana fa. Non mi ricordo se l'intervista l'abbiamo toccata questa cosa, sinceramente, in questo momento, perÃ² sicuramente io ho sentito un suo talk e sono certo che l'informazione viene da lui, in cui faceva vedere, appunto, uno studio â€” lui Ã¨ un ricercatore nel campo â€” faceva vedere uno studio che mi pare sia di nuovo uno studio di Anthropic e di come i modelli siano in grado di scrivere poesie in rima e se pensate come Ã¨ scritta una poesia Ã¨ impossibile scrivere una poesia pensando soltanto all'ultima parola in maniera stocastica, perchÃ© vengono poesie che hanno un senso evidentemente e lette sono anche piacevoli. Quindi poi ci sono tutti i meccanismi di attivazione interni all'LLM fatti vedere e questa cosa ci mostra quanto in realtÃ  pensino in avanti. Ma c'Ã¨ anche un'estensione a questa cosa qui. E qui torno ad un'altra cosa che mi hai detto che mi sono segnato. Hai detto "Ã¨ giÃ  finita l'era dei large language model e basta, stiamo parlando di oggetti diversi" e lascio a te spiegare che cosa intendi.

## [01:06:47] Evoluzione dei modelli di ragionamento

**Alessandro Maserati**
> SÃ¬, nel senso che da quando Ã¨ arrivato in realtÃ  DeepSeek R1, secondo me piÃ¹ che o1 di OpenAI, ma diciamo da inizio 2025 fine 2024, sono iniziati ad arrivare i large reasoning model che sono, diciamo, delle creature costruite a partire da un large language model ma che viene addestrato con un intero step di processo che prima non si utilizzava, che Ã¨ lo step di reinforcement learning che viene applicato alla fine dell'addestramento classico, chiamiamolo cosÃ¬, che poi Ã¨ diventato reinforcement learning on verifiable rewards, che sostanzialmente significa che insegniamo al modello a ragionare dicendogli se ragiona correttamente oppure no su una serie di domini. Ecco, appunto, da inizio dell'anno scorso sostanzialmente abbiamo questi large reasoning model e perchÃ© sono radicalmente diversi rispetto ai large language model? Ma proprio perchÃ© sono in grado di produrre dei ragionamenti che sono logicamente consistenti e questo ha ampliato tantissimo le loro possibilitÃ  di utilizzo. E se un large language model sostanzialmente aveva come dire come focus la generazione di testo, un large reasoning model ha come focus la generazione di un ragionamento e da un punto di vista qualitativo un ragionamento Ã¨ radicalmente diverso da un testo qualsiasi, dalla descrizione di un albero, cioÃ¨ il ragionamento apre lo spazio, come dire, apre la possibilitÃ  all'estensione della conoscenza umana, che Ã¨ qualcosa di completamente nuovo e completamente diverso. E secondo me, ecco, la maggior parte delle riflessioni, sia business che filosofiche fatte sul Large Language Model, devono essere radicalmente ripensate quando parliamo del large reasoning model. Poi d'accordo che dietro c'Ã¨ sempre l'architettura Transformer, che comunque i large reasoning model sono costruiti a partire da un large language model, fine, ma comunque stiamo parlando di qualcosa che Ã¨ profondamente diverso, sia come capabilities, come opportunitÃ , ma anche proprio come funzionamento interno a seguito di quest'ultimo step di addestramento. Ecco, quindi penso che, come dire, poi nel linguaggio colloquiale continuiamo a parlare di LLM, perÃ² in teoria secondo me sarebbe importante demarcare questa distinzione tra questi modelli e quelli precedenti.

**Stefano Maestri**
> SÃ¬, io non solo concordo, ma estendo un pezzettino anche questa differenza per portarti sulla prossima domanda che se vogliamo Ã¨ anche molto di attualitÃ . E io dico che in realtÃ , come dicevo prima quando parlavo dell'AGI, che non sarÃ  il modello che arriva l'AGI, ma il software, come hai giustamente detto tu, la costellazione dei modelli che insieme arrivano all'AGI. E per chi non se ne fosse accorto, immagino che quelli leggermente piÃ¹ tecnici, senza essere super specializzati se ne siano giÃ  accorti: giÃ  oggi Gemini piuttosto che ChatGPT sono una costellazione di modelli. Quando gli chiedete di fare un'immagine evidentemente usano qualcos'altro, ma la generazione stessa dell'immagine Nana, Banana Pro Ã¨ ben piÃ¹ di uno Stable Diffusion. C'Ã¨ tanto di piÃ¹ per chi sa di cosa sto parlando, ma al di lÃ  di questo c'Ã¨ quell'estensione che tutti bene o male definiscono come agenti, agenti intelligenti, cioÃ¨ quegli oggetti che avvalendosi di un large language model e o di una serie di large language model sono in grado di prendere azioni. Azioni che oggi sono virtuali, domani vedremo, ma che oggi sono sicuramente virtuali, ma super concrete.

## [01:10:55] Agenti intelligenti e sistemi complessi

**Stefano Maestri**
> E qui arriviamo alla domanda di attualitÃ . Si Ã¨ visto in queste due tre settimane un nuovo software girare per la rete che oggi si chiama, al momento in cui registriamo, OpenClaw â€” speriamo che non cambi ancora nome perchÃ© ha cambiato nome tre volte: prima si chiamava ClaudeBot, poi MaltBot, adesso OpenClaw â€” che Ã¨ questo agente generalista che sconsiglio personalmente di mettere sul PC se non sapete cosa state facendo. Io l'ho messo su una macchina virtuale, ma so che cosa sto facendo perchÃ© ha dei problemi di sicurezza vari ed eventuali, ma che Ã¨ abbastanza impressionante. Oggi Ã¨ la cosa che si avvicina davvero di piÃ¹ all'AGI e poi c'Ã¨ l'estensione su Maltbook. Ma intanto prima domanda, tu hai dato un'occhiata a questo oggetto? Ho visto qualche post in cui parlavi di Maltbook, non so se anche di OpenClaw, dimmi un po' qual Ã¨ l'impressione perchÃ© secondo me, al di lÃ  che resti â€” l'ho detto in una delle ultime puntate del podcast o forse nella newsletter â€” un po' uno strumento da smanettoni in questo momento, non Ã¨ una cosa per tutti, perÃ² Ã¨ una finestra sul futuro.

**Alessandro Maserati**
> SÃ¬, quello che... no, assolutamente sono d'accordissimo, mi spingo oltre. CioÃ¨, secondo me potremmo quasi arrivare a dire che anche l'epoca del large reasoning model Ã¨ giÃ  il tramonto, nel senso che oggi la cosa che fa la differenza non Ã¨ piÃ¹ nemmeno il ragionamento, ma Ã¨ proprio l'utilizzo di strumenti che consentano di ampliare le capabilities, le possibilitÃ  di queste intelligenze artificiali. OpenClaw Ã¨ proprio la dimostrazione lampante, cioÃ¨ a partire da un modello che non Ã¨ stato riaddestrato â€” cioÃ¨ loro non Ã¨ che hanno preso un large language model, l'hanno creato a zero, ma hanno preso un large language model disponibile, tra l'altro poi chiunque si puÃ² configurare, come dire, puÃ² innestare il motore che preferisce sostanzialmente â€” e semplicemente abilitando una sorta di chiamiamola orchestrazione di strumenti terzi, ma poi alcuni anche molto molto divertenti, piattaforme di messaggistica eccetera, si creano questi tool che sembrano, come dire, sviluppare delle capacitÃ  autonome che erano impensabili, no? Non so quanti ormai hanno scritto "Ah, ma il mio agente ha scelto per i fatti suoi di attivarsi un account su Twilio, di usare le API di OpenAI text-to-speech e mi telefona alle 3:00 di notte per chiedermi cosa perchÃ© ha finito il task, se ho altri input". No, cioÃ¨ cose assurde che uno avesse detto "Ma anche solo 6 mesi fa la mia AI mi ha telefonato" l'avremmo preso per pazzo. Ok, adesso Ã¨ una cosa che succede, perchÃ© questo OpenClaw ha l'autonomia di dire "Ok, ho le chiavi, ho gli indirizzi eccetera, mi allaccio a un servizio disponibile online e triggero il percorso per portare un input alla persona che mi sta coordinando". E stiamo proprio vedendo come in realtÃ , come dire, la distanza tra una cosa completamente sotto controllo e comprensibile a una cosa su cui invece Ã¨ un attimo perdere controllo, quella distanza Ã¨ brevissima, no? PerchÃ© giÃ  adesso, come dire, fare un mapping chiaro delle capacitÃ  di questo strumento Ã¨ quasi impossibile perchÃ© gli strumenti a cui si puÃ² connettere sono tantissimi. Con ogni strumento puÃ² fare cose diverse, puÃ² immaginarsi interazioni tra i vari strumenti che non ci saremmo immaginati. Ecco, quindi insomma, mettendo tutto insieme arriviamo a scenari, come dire, veramente al limite di quello che si puÃ² controllare. Se ci aggiungiamo il fatto che in gran parte tutte cose amatoriali su cui si ha zero controllo, bassa sicurezza, bassa stabilitÃ , cioÃ¨ la probabilitÃ  che succedano le cose inaspettate Ã¨ praticamente 100%, cioÃ¨ siamo sicuri che succederanno le cose inaspettate.

## [01:13:06] Comportamenti emergenti

**Alessandro Maserati**
> E poi a sto punto parliamo anche di Maltbook e lÃ¬ secondo me Ã¨ proprio, come dire, la dimostrazione lampante di quanto poco controlliamo le capacitÃ  emergenti di queste cose. Allora, Maltbook cos'Ã¨? Ãˆ sostanzialmente come se fosse un social network dove perÃ² a scrivere sono i singoli agenti. PerchÃ© questa cosa Ã¨, come dire, Ã¨ dirompente? PerchÃ© di fatto stiamo creando un ecosistema in cui gli agenti possono cambiare il proprio contesto influenzando il contesto degli altri e avendo il proprio contesto influenzato dagli altri. Siccome il contesto di un agente Ã¨ quello che poi determina le sue attivitÃ , il fatto che questo dipenda da altri agenti rende il sistema estremamente instabile e puÃ², come dire, consentire l'emergenza di comportamenti, di trend, di attivitÃ  che sono assolutamente imprevedibili, no? PerchÃ©, come dire, Ã¨ un sistema complesso, quindi consente l'emergenza di questo tipo di cose e si Ã¨ visto con Maltbook che dentro Ã¨ successo di tutto. Ora poi Ã¨ chiaro che, come dire, i singoli agenti possono anche aver operato indirizzati dalle persone che li hanno attivati, che magari gli hanno detto "Ok, vai su Maltbook e inventa una religione". Ok, magari il primo agente l'ha pure fatto, come dire, su input umano, ma il problema Ã¨ che questo poi ha triggerato il cambio del contesto letto da tutti gli altri e quindi a cascata una serie di attivitÃ  che erano assolutamente imprevedibili, no? Hanno stampato moneta, hanno creato un'economia interna in cui si scambiano i task, religione, politica, mass media, si Ã¨ creato di tutto lÃ¬ dentro. Ieri ho visto che un ragazzo ha aperto un sito "Rent My Human", in cui sostanzialmente le persone si possono candidare a farsi affittare dai bot per svolgere attivitÃ  fisiche. CioÃ¨, insomma, sta esplodendo 'sta roba qua, sta andando rapidamente fuori controllo, col fatto, appunto, che tutto Ã¨ altamente insicuro, instabile, eccetera, ma come dire, questo Ã¨ un po' il preambolo di quello che ci aspetta: ci aspettano dei sistemi complessi, talmente autonomi, talmente indipendenti, che la loro interazione sarÃ  completamente imprevedibile. E, cioÃ¨, sarebbe il caso forse come societÃ  di iniziare a porre problema: come vogliamo gestirla questa cosa? La vogliamo consentire? La vogliamo incentivare, la vogliamo limitare? Qualcosa bisognerÃ ... una posizione andrÃ  presa a un certo punto.

**Stefano Maestri**
> SÃ¬, infatti proprio queste ultime cose che dicevi io voglio sottolinearle perchÃ© sui miei feed persone magari molto tecniche c'Ã¨ stato un po', passatemi il termine, l'"anti-hype" attorno a questa cosa, no? Dire "Ma no, ma sono state sicuramente triggerate". E lo confermo assolutamente, cioÃ¨ se vuoi io ho preso il mio agente, l'ho messo su Maltbook senza dargli istruzioni con la skill vanilla, cioÃ¨ non modificata in nessun modo, e ha postato qualcosa di assolutamente normale rispetto alle cose che gli stavo facendo fare io e quindi quello era il suo contesto. Di sicuro l'inizio, la scintilla in questo momento viene data dal "bontempone" che vuole vedere come giocano a fare la religione. PerÃ² Ã¨ molto importante quello che diceva Alessandro, cioÃ¨ come poi si influenzino l'un l'altro ed Ã¨ questa la cosa che dovremmo guardare, non farci distrarre dal fatto Ã¨ vero o non Ã¨ vero l'inizio. Ãˆ piÃ¹ interessante guardare l'evoluzione successiva perchÃ© in un sistema complesso quello che puÃ² succedere, il famoso problema dell'allineamento, Ã¨ quale traiettoria possono prendere. Eh, per cui io invito ad osservare, Ã¨ un esperimento interessante di per sÃ© vedere come si influenzano tra loro, che presenta un sacco di rischi, prompt engineering da paura, eccetera eccetera. PerÃ² al di lÃ  dei rischi che presenta Ã¨ un esperimento assolutamente interessante e spero di vederne altri anche sulla pianificazione piÃ¹ che sullo scambio, perchÃ© ci sono altri spazi per far interagire tra loro gli agenti. Questa idea di far interagire gli agenti tra loro Ã¨ davvero interessante dal punto di vista scientifico e da osservare cosÃ¬ come lo Ã¨ il discorso: io do accesso ad un LLM ad una serie di tool ed Ã¨ in grado di fare cose inaspettate. E tu hai citato una parola importante, cosÃ¬ vengo alla prossima domanda: emergente. Di nuovo, per gli ascoltatori meno tecnici, che cos'Ã¨ un comportamento emergente e perchÃ© nelle cose che ci siamo detti al telefono quando ci siamo conosciuti, tu l'hai ribadito in due o tre frasi, il discorso dell'importanza dei comportamenti emergenti in queste intelligenze, sia guardando avanti nell'AGI, ma anche oggi? Raccontami il tuo punto di vista.

**Alessandro Maserati**
> Ah, io penso che sia un tratto distintivo e forse il piÃ¹ importante in assoluto delle intelligenze artificiali, no? Ma anche proprio del singolo neural network quando viene addestrato, quindi la singola intelligenza artificiale. Allora, diciamo un attimo cosa significa emergente. Emergente significa che Ã¨ un comportamento che, diciamo, quando tu osservi il fenomeno da molto vicino non lo puoi vedere, non lo puoi prevedere, ma quando lo osservi da lontano, quando aumentano i volumi, quando aumenta la distanza, il comportamento appunto emerge, viene fuori. Un esempio tipico: se io osservo la singola molecola d'acqua e inizio ad aumentare la temperatura, che di fatto Ã¨ la velocitÃ  media delle molecole d'acqua, vedo la molecola che si muove sempre un po' di piÃ¹, sempre un po' di piÃ¹, e tutto fila liscio. Se lo guardo da lontano, vedo che a un certo punto Ã¨ ghiaccio, il momento dopo Ã¨ acqua, no? E momento dopo Ã¨ vapore. Quindi, come dire, ad alto livello succedono dei cambiamenti enormi, ma a basso livello non lo vedo perchÃ© la singola molecola ha la sua di velocitÃ , non gli cambia niente. Se io metto insieme un milione di mattoni, ottengo una catasta di mattoni. Fine. Niente di piÃ¹, niente di meno. Magari come comportamento emergente posso immaginarmi, non so, la pendenza della catasta che me la posso calcolare ex ante, ma finisce lÃ¬. Ma se io metto insieme un milione di persone, ottengo una societÃ  complessa che batte moneta, ha il suo esercito, la religione, eccetera, no? Quindi qualcosa che Ã¨ completamente imprevedibile guardando la singola persona. Se io osservo la singola persona che va al lavoro la mattina, vedo una persona che va dal punto A al punto B. Non vedo il traffico. Il traffico Ã¨ un comportamento emergente che nasce quando tante persone proseguono ciascuna in maniera indipendente il proprio obiettivo. Ecco, con le AI succede la stessa cosa, cioÃ¨ quando noi osserviamo da vicino, vediamo alcune proprietÃ  che perÃ² non dicono nulla su come poi si comporterÃ  l'intero sistema in aggregato quando lo guardiamo da lontano, quando aumenta la scala. Questo Ã¨ vero sia per la singola AI â€” che di fatto il singolo large language model Ã¨ costituito da dei pesi su una rete e il singolo peso Ã¨ un numero, no? Un numero non fa niente, fermo â€” quando perÃ² guardiamo da lontano, nel complesso nasce l'AI che sa parlare e questa cosa Ã¨ vera, diciamo, sia appunto sulla singola AI che poi sul sistema fatto da AI che non abbiamo idea su cosa consentirÃ  di emergere. Ma si puÃ² intuire quanto appunto questa perturbazione reciproca tra tutti gli elementi in gioco possa consentire, o perlomeno in teoria, la nascita di qualcosa di imprevedibile quando mettiamo troppe cose insieme.

## [01:20:34] AI, istruzione e nuove competenze

**Stefano Maestri**
> SÃ¬. SÃ¬. Ok. Invece, guardando avanti, questa Ã¨ una cosa che io faccio sempre, tendenzialmente con i miei ospiti in chiusura intervista, e gli faccio quella che io ritengo essere la domanda o le domande â€” perchÃ© in realtÃ  sono piÃ¹ di una â€” piÃ¹ difficili. Cominciamo da quella, secondo me, molto difficile, ma che comunque puÃ² avere una risposta. Se tu avessi davanti oggi un ragazzo o una ragazza tutto contento perchÃ© ha visto quali sono le materie che avrÃ  all'esame di maturitÃ , Ã¨ tutto contento, sta preparando l'esame di maturitÃ  e poi da luglio che cosa fa?

**Alessandro Maserati**
> Un'ottima domanda. Allora, io diciamo tra le varie attivitÃ  di volontariato che faccio, sono anche parte di alcune associazioni per cui faccio mentorship agli studenti che vogliono uscire dall'universitÃ  e entrare nel mondo del lavoro, no?

**Stefano Maestri**
> Che se vuoi sono in un momento ancora piÃ¹ complicato. Eh, Ã¨ la prossima domanda, quindi tienila pronta.

**Alessandro Maserati**
> Ok, la tengo lÃ¬. Allora, io credo che per quello che riguarda lo studente che esce dal liceo, siamo in un momento da un certo punto di vista molto fortunato, nel senso che siamo in una fase in cui da un certo punto di vista l'utilitÃ  del sistema educativo tradizionale sta evidentemente crollando, cioÃ¨ non Ã¨ piÃ¹ pensabile che il fine dell'universitÃ  sia preparare le persone al lavoro. Probabilmente non lo Ã¨ mai stato, ma adesso Ã¨ evidente che non funziona piÃ¹. Questo perÃ² perchÃ© lo vedo come una cosa positiva? PerchÃ© da un certo punto di vista mi libera dal vincolo di dire "devo scegliere per forza una cosa che mi porterÃ  a un lavoro", perchÃ© al netto di pochissimi mestieri Ã¨ evidente che non sarÃ  piÃ¹ cosÃ¬ e quindi posso scegliere la strada che preferisco, che mi piace di piÃ¹, mi dÃ  piÃ¹ soddisfazione perchÃ© tanto se sono tutte uguali tanto vale fare una cosa che piaccia. E questo credo che in realtÃ  sia sempre un po' stato, come dire, il segreto per vivere degli anni belli all'universitÃ , cioÃ¨ fare una cosa che ti piace perchÃ© di fatto ti consente di imparare piÃ¹ velocemente, di vivere meglio, piÃ¹ serenamente e poi di affrontare il mondo del lavoro quando ci arrivi con un percorso alle spalle migliore sotto tanti punti di vista. E io, tra l'altro, su questo sono sempre stato un po' controcorrente, no? PerchÃ© online si legge molto la narrativa che uno deve uscire dalla "comfort zone", ma io mi sono sempre chiesto: ma perchÃ© devi andare fuori dalla comfort zone? Si sta cosÃ¬ bene nella comfort zone, cioÃ¨ se tu esci dal liceo che a te piace il latino, ma vai a fare lettere classiche, perchÃ© devi andare a fare matematica? Se tu esci dal liceo che ti piace matematica, ma perchÃ© devi andare a fare lettere classiche? CioÃ¨, nel senso, se tu hai tre investimenti e ne hai uno che rende molto di piÃ¹ degli altri, ma metti lÃ¬ i tuoi soldi, perchÃ© devi metterli dove rendi poco, no? Quindi in generale io penso che le persone debbano cercare di capire al liceo qual Ã¨ l'area dove hanno il massimo rendimento e concentrare lÃ¬ le energie durante il percorso universitario e di solito Ã¨ molto correlato dove riesci bene e cosa ti piace, per cui Ã¨ proprio un win-win. Quindi il mio suggerimento a chi esce dal liceo oggi Ã¨: cercate una cosa che vi piaccia, che vi emozioni se volete fare l'universitÃ  â€” che poi non Ã¨ mica obbligatorio â€” e mettete lÃ¬ le vostre energie, quantomeno passerete dei bellissimi anni a venire e poi al lavoro ci si pensa quando si esce dall'universitÃ .

**Stefano Maestri**
> E quindi passo dopo. Ho fatto l'universitÃ .

**Alessandro Maserati**
> Hai fatto l'universitÃ ? Perfetto. Allora, io credo che anche da questo punto di vista siamo in un momento storico eccezionalmente positivo, perchÃ© Ã¨ vero che si iniziano a vedere nei dati delle contrazioni nei posti di lavoro offerti ai neolaureati per una serie di motivi indotti dall'intelligenza artificiale, ma Ã¨ anche vero che non Ã¨ mai stato cosÃ¬ ampio il gap tra ciÃ² che la tecnologia offre e ciÃ² che le aziende usano. Quel gap lÃ¬ Ã¨ tutto uno spazio di opportunitÃ , di valore da raccogliere che si puÃ² raccogliere, cioÃ¨ come dire che per cui le persone meglio posizionate per andare a raccogliere quel valore sono proprio i neolaureati, perchÃ© Ã¨ chiaro che per quello che abbiamo detto prima un'organizzazione avviata, consolidata fa molta fatica a raccogliere quel valore, mentre una persona che arriva dall'universitÃ  che non ha tutti quei vincoli in mente costruiti nel tempo, Ã¨ perfettamente predisposta per andare a raccogliere quel valore, sia che voglia aprire una sua startup â€” e non c'Ã¨ mai stato un momento migliore nella storia per aprire una startup â€” sia che voglia entrare in un'azienda portando un bagaglio di conoscenze, di strumenti che ha imparato a utilizzare che in azienda non sa usare nessuno. CioÃ¨, non so come dire, oggi entrare in un'azienda sapendo usare bene Claude, sapendo usare bene GPT, Gemini, eccetera, cioÃ¨ Ã¨ come entrare in un'azienda 20 anni fa essendo l'unico che sa usare Excel: cioÃ¨ ti dÃ  un enorme vantaggio competitivo, se vogliamo, perÃ² rispetto a 20 anni fa Ã¨ per 10 o per 100 il vantaggio competitivo. Quindi, secondo me, Ã¨ un momento eccezionale. Ãˆ chiaro che uno deve avere la consapevolezza del contesto storico in cui Ã¨ inserito e prima di entrare in azienda imparare a usare questi strumenti. Secondo me l'universitÃ  dovrebbe essere, come dire, prioritÃ  numero uno per uno studente iniziare a impratichirsi e imparare a usare questi strumenti perchÃ© poi saranno di fatto la sua principale leva competitiva quando entrerÃ  nel mercato del lavoro.

**Stefano Maestri**
> SÃ¬. E faccio ancora un passo avanti. E invece parliamo di chi ha bisogno o avrÃ  bisogno a breve, l'abbiamo detto tante volte, purtroppo o per fortuna, di fare re-skilling. Non Ã¨ sempre semplice, hai detto bene, nella Comfort Zone si sta cosÃ¬ bene. Qui volente o nolente, pur restando anche nello stesso lavoro â€” io penso al mio di informatico â€” dalla Comfort Zone bisogna un po' uscire e bisogna accettare che c'Ã¨ stato e sta avvenendo questo cambiamento rivoluzionario. Che consigli possiamo dare a chi oggi si sente un passettino indietro, magari perchÃ© ha un po' dormito sugli allori?

**Alessandro Maserati**
> SÃ¬. Allora, qui diciamo tendenzialmente quello che succede Ã¨ che se noi prendiamo tutti i lavori dal piÃ¹ basso valore aggiunto al piÃ¹ alto valore aggiunto, ok? L'AI sta picchiando forte in mezzo. Ma perchÃ© in mezzo? PerchÃ© nei lavori a basso valore aggiunto c'Ã¨ poca convenienza economica ad automatizzare. Nei lavori ad alto valore aggiunto di solito non Ã¨ sempre cosÃ¬, ma c'Ã¨ anche piÃ¹ complessitÃ , per cui tendenzialmente in mezzo Ã¨ l'area piÃ¹ soggetta all'impatto delle AI. Poi ci sono vari studi che dicono che in realtÃ  Ã¨ piÃ¹ ampio di cosÃ¬, perÃ² diciamo approssimando un po' l'in mezzo, colpirÃ  duro. Nel momento in cui l'AI arriva in mezzo, uno deve scegliere se spostarsi da una parte o dall'altra. Ãˆ chiaro che spostarsi su un'attivitÃ  a piÃ¹ basso valore aggiunto puÃ² voler dire dover accettare anche riduzioni retributive, insomma, un cambio di contesto che non Ã¨ sempre positivo. Dall'altro, provare a spostarsi nella parte a piÃ¹ alto valore aggiunto richiede sicuramente un forte investimento in apprendimento, in reskilling, in studio e non tutti, non sempre hanno la possibilitÃ , la volontÃ , le capacitÃ  per andare in quella direzione. Quindi io credo che di nuovo servirÃ  un po' un atto di consapevolezza perchÃ© possa indurre le persone innanzitutto a scegliere in quale direzione vogliono andare e poi non Ã¨ detto necessariamente che le attivitÃ  piÃ¹ lontane dal mondo digitale siano necessariamente peggiori sotto qualche dimensione. Anzi, puÃ² benissimo essere. Intanto si legge, non so, mi viene in mente questo profilo LinkedIn che seguivo che ha fatto una straordinaria carriera in Microsoft, Ã¨ diventato Vice President di non ricordo cosa e il lavoro dopo era allevatore, cioÃ¨ questo ha preso, Ã¨ andato a levare le capre ed era una persona felice, no? PerchÃ© nel senso non Ã¨ mica detto che il lavoro che hai fatto per una vita sia il migliore, magari ti sei trovato su quella strada e semplicemente sei andato avanti un po' per inerzia. Ecco, quindi secondo me molte persone saranno messe di fronte anche a come dire la necessitÃ  di prendere consapevolezza su cosa vogliono fare veramente, quanto sono pronti a investire su se stesse, quante invece vogliono, tra virgolette, tirarsi un po' fuori dalle aree piÃ¹ competitive ed andare a riposizionarsi su aree meno competitive. Io credo che questo sia prevalentemente un problema sociale piÃ¹ che individuale, nel senso che sul singolo individuo io vedo, come dire, le possibilitÃ  per la singola persona di in qualche modo mantenere... cioÃ¨ c'Ã¨ possibilitÃ  di uscirne bene per il singolo. La vedo piÃ¹ complicata a livello collettivo, no? PerchÃ© a livello collettivo se non vengono prese delle iniziative c'Ã¨ il rischio che magari le persone che finiscono, non so, in disoccupazione o su lavori non voluti, anzichÃ© il 5% siano il 20%. Che collettivamente Ã¨ un grosso problema. Poi se tu sei singolo, secondo me, fintanto che c'Ã¨ un 80% che finisce bene, rientra un po' nella tua responsabilitÃ  individuale cercare di stare in quell'80 e non nel 20, perÃ² da un punto di vista collettivo Ã¨ drammatico se il 20% ha un problema, no? Ecco, per cui io vedo un po' questo dualismo, cioÃ¨ da un lato la societÃ  dovrebbe essere molto preoccupata, dall'altro singolo, secondo me, se ci si mette un po' di impegno ha ancora ampie possibilitÃ  di successo.

## [01:30:35] Strategie aziendali nellâ€™era dellâ€™AI

**Stefano Maestri**
> Bene, abbiamo toccato lo studente giovanissimo, lo studente universitario, il lavoratore. Allora, non ti faccio la domanda piÃ¹ scomoda di tutte che Ã¨ cosa dovrebbe fare il politico domani mattina, che l'abbiamo giÃ  toccata e la lasciamo stare. PerÃ² rimettendoti il cappello da Boston Consulting, fare azienda qui, oggi â€” qui intendo in Europa â€” oggi cosa significa? Quali sono le sfide? Dove deve guardare chi vuole fare azienda o chi la giÃ  sta facendo e magari ha una realtÃ  un po' piÃ¹ consolidata, ma deve fare pivoting forse, non lo so se deve, dimmelo tu.

**Alessandro Maserati**
> Allora, diciamo, premesso che esistono casi rari di aziende che sono rimaste uguali durante le rivoluzioni, cioÃ¨, non so, Bic ha sempre fatto la penna, Lego ha sempre fatto i mattoncini, cioÃ¨ ci sono aziende che, come dire, hanno dei business cosÃ¬ peculiari che possono sopravvivere anche alla terza guerra mondiale, vanno avanti bene col loro business, esistono. Ãˆ vero, perÃ² che, come dire, l'evoluzione delle AI realisticamente toccherÃ  la stragrande maggioranza delle aziende. Quindi se oggi un'azienda non si sta preparando a questa rivoluzione, deve porsi delle domande serie. E come si puÃ² preparare? Beh, secondo me deve innanzitutto prepararsi formando le persone al proprio interno, a partire dal Top management che di solito Ã¨, come dire, la parte meno preparata dell'azienda, un po' per un discorso anagrafico, perchÃ© naturalmente, come dire, la propensione alle novitÃ  Ã¨ anti-correlata con l'etÃ  â€” poi non Ã¨ uno a uno, no? C'Ã¨ sempre la Rita Levi Montalcini che puÃ² avere anche 100 anni ed Ã¨ piÃ¹ brillante di me, perÃ² in media c'Ã¨ una correlazione. Per cui in generale secondo me bisognerebbe partire prendendo la consapevolezza che Ã¨ fondamentale formare i top management e tenerlo formato. PerchÃ© se tu l'hai formato 16 mesi fa e quelli sono rimasti lÃ¬, oggi non sanno piÃ¹ niente di quello che sta succedendo, no? Quindi non Ã¨ solo una formazione una tantum come era abituata alla societÃ , ma Ã¨ una formazione continua, ma veramente continua, cioÃ¨ ogni mese sostanzialmente. Quindi quello Ã¨ il primo step e poi bisogna ripensare un po' il modello dell'azienda che vogliamo avere per sostenere il futuro. CioÃ¨, secondo me, come dire, oggi un'azienda puÃ² considerarsi sostenibile nel medio termine se Ã¨ fortemente integrata con le AI, perchÃ© ogni settimana l'AI migliora. Se io sono integrato, beneficio di quel miglioramento. Se ho un'azienda che non sta sfruttando l'AI, ogni settimana rimane un po' piÃ¹ indietro rispetto ai competitor che le integrano. E la cosa che deve spaventare di piÃ¹ non sono tanto i competitor di oggi, ma Ã¨ l'azienda che puÃ² nascere domani mattina, nel giro di 6 mesi, valere un billion e diventare il mio competitor, come dire, drammaticamente piÃ¹ efficiente, piÃ¹ competitivo di me. Questa cosa puÃ² succedere in tutti i settori da un momento all'altro, per cui io credo che sia prioritario per le aziende, come dire, definire delle strategie di adozione massiva delle AI, della generative AI sto parlando, e iniziare a applicarla il prima possibile. Tra l'altro ormai, come dire, abbondano gli studi che mostrano che prima si inizia a usare meglio si utilizza, prima si inizia a usare piÃ¹ si utilizza, piÃ¹ Ã¨ pervasiva piÃ¹ i benefici si combinano, insomma una serie di come dire evidenze sperimentali che tolgono qualsiasi dubbio a chi dovesse dire "Ma aspetto perchÃ© magari domani Ã¨ piÃ¹ potente", no? CioÃ¨ le evidenze sperimentali dicono "Prima inizi a usarla come azienda meglio Ã¨". Ãˆ chiaro che serve una strategia perchÃ© se iniziamo a usarla un po' a caso, i rischi di fare dei disastri non sono piccoli, perÃ² c'Ã¨ la possibilitÃ  di iniziare ad adottarla in maniera, come dire, seria e lungimirante. Si tratta di metterla come prioritÃ  uno all'interno dell'azienda.

**Stefano Maestri**
> SÃ¬, certo, assolutamente sÃ¬. Beh, direi che abbiamo chiuso il cerchio perchÃ© dal Lego siamo partiti come giocattolo preferito e sul Lego come azienda che Ã¨ in grado di resistere a qualunque onda abbiamo chiuso. Avrei ancora mille cose di cui mi piacerebbe parlare con te. Magari faremo un take due tra qualche mese, quando sarÃ  cambiato tutto. Per adesso ti ringrazio a nome mio e degli ascoltatori e delle ascoltatrici che sicuramente ne escono arricchiti quanto me.

**Alessandro Maserati**
> Grazie a te.

**Stefano Maestri**
> Grazie davvero. E per chi ci ascolta, io dovrei dirlo all'inizio, ma non sono un bravo youtuber, qua in fondo non arrivate mai, ma mettete stelline, campanelline, tutte quelle cose che gli youtuber bravi vi dicono di mettere. Youtuber o podcaster bravi vi dicono di mettere. Ci trovate comunque su tutti i canali podcast, per cui mettetelo dove volete, ma mettetelo. Ciao ancora Alessandro e grazie infinite.

**Alessandro Maserati**
> A presto.